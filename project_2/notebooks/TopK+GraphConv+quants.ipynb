{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 001, Loss: 1.61522, Train Acc: 0.20000, Test Acc: 0.26969\n",
      "Epoch: 002, Loss: 1.59979, Train Acc: 0.21000, Test Acc: 0.27134\n",
      "Epoch: 003, Loss: 1.58407, Train Acc: 0.27000, Test Acc: 0.30845\n",
      "Epoch: 004, Loss: 1.56930, Train Acc: 0.35000, Test Acc: 0.41278\n",
      "Epoch: 005, Loss: 1.58072, Train Acc: 0.41000, Test Acc: 0.45691\n",
      "Epoch: 006, Loss: 1.57049, Train Acc: 0.26000, Test Acc: 0.34680\n",
      "Epoch: 007, Loss: 1.54216, Train Acc: 0.26000, Test Acc: 0.27216\n",
      "Epoch: 008, Loss: 1.52092, Train Acc: 0.22000, Test Acc: 0.19794\n",
      "Epoch: 009, Loss: 1.55032, Train Acc: 0.23000, Test Acc: 0.21691\n",
      "Epoch: 010, Loss: 1.48786, Train Acc: 0.28000, Test Acc: 0.30062\n",
      "Epoch: 011, Loss: 1.50580, Train Acc: 0.26000, Test Acc: 0.25031\n",
      "Epoch: 012, Loss: 1.42729, Train Acc: 0.27000, Test Acc: 0.32000\n",
      "Epoch: 013, Loss: 1.43645, Train Acc: 0.29000, Test Acc: 0.27505\n",
      "Epoch: 014, Loss: 1.50821, Train Acc: 0.26000, Test Acc: 0.24660\n",
      "Epoch: 015, Loss: 1.39396, Train Acc: 0.45000, Test Acc: 0.47670\n",
      "Epoch: 016, Loss: 1.37056, Train Acc: 0.52000, Test Acc: 0.53567\n",
      "Epoch: 017, Loss: 1.39192, Train Acc: 0.33000, Test Acc: 0.32742\n",
      "Epoch: 018, Loss: 1.37699, Train Acc: 0.36000, Test Acc: 0.34103\n",
      "Epoch: 019, Loss: 1.41515, Train Acc: 0.36000, Test Acc: 0.33608\n",
      "Epoch: 020, Loss: 1.34705, Train Acc: 0.46000, Test Acc: 0.50845\n",
      "Epoch: 021, Loss: 1.32719, Train Acc: 0.47000, Test Acc: 0.47794\n",
      "Epoch: 022, Loss: 1.29830, Train Acc: 0.48000, Test Acc: 0.47835\n",
      "Epoch: 023, Loss: 1.30088, Train Acc: 0.55000, Test Acc: 0.55505\n",
      "Epoch: 024, Loss: 1.33924, Train Acc: 0.47000, Test Acc: 0.50804\n",
      "Epoch: 025, Loss: 1.32461, Train Acc: 0.49000, Test Acc: 0.43753\n",
      "Epoch: 026, Loss: 1.29441, Train Acc: 0.37000, Test Acc: 0.37608\n",
      "Epoch: 027, Loss: 1.29255, Train Acc: 0.49000, Test Acc: 0.53856\n",
      "Epoch: 028, Loss: 1.37941, Train Acc: 0.33000, Test Acc: 0.42969\n",
      "Epoch: 029, Loss: 1.47783, Train Acc: 0.29000, Test Acc: 0.35299\n",
      "Epoch: 030, Loss: 1.45974, Train Acc: 0.41000, Test Acc: 0.43464\n",
      "Epoch: 031, Loss: 1.36198, Train Acc: 0.49000, Test Acc: 0.53938\n",
      "Epoch: 032, Loss: 1.46010, Train Acc: 0.46000, Test Acc: 0.50351\n",
      "Epoch: 033, Loss: 1.28511, Train Acc: 0.40000, Test Acc: 0.45567\n",
      "Epoch: 034, Loss: 1.32762, Train Acc: 0.54000, Test Acc: 0.52082\n",
      "Epoch: 035, Loss: 1.27514, Train Acc: 0.65000, Test Acc: 0.57649\n",
      "Epoch: 036, Loss: 1.22792, Train Acc: 0.55000, Test Acc: 0.53237\n",
      "Epoch: 037, Loss: 1.21222, Train Acc: 0.54000, Test Acc: 0.51876\n",
      "Epoch: 038, Loss: 1.20066, Train Acc: 0.53000, Test Acc: 0.52701\n",
      "Epoch: 039, Loss: 1.11251, Train Acc: 0.52000, Test Acc: 0.52454\n",
      "Epoch: 040, Loss: 1.17698, Train Acc: 0.51000, Test Acc: 0.52206\n",
      "Epoch: 041, Loss: 1.13162, Train Acc: 0.52000, Test Acc: 0.55216\n",
      "Epoch: 042, Loss: 1.15757, Train Acc: 0.49000, Test Acc: 0.53814\n",
      "Epoch: 043, Loss: 1.06738, Train Acc: 0.56000, Test Acc: 0.59381\n",
      "Epoch: 044, Loss: 1.10829, Train Acc: 0.62000, Test Acc: 0.62433\n",
      "Epoch: 045, Loss: 1.14475, Train Acc: 0.64000, Test Acc: 0.62144\n",
      "Epoch: 046, Loss: 1.08682, Train Acc: 0.63000, Test Acc: 0.58268\n",
      "Epoch: 047, Loss: 1.13969, Train Acc: 0.38000, Test Acc: 0.48990\n",
      "Epoch: 048, Loss: 1.26695, Train Acc: 0.62000, Test Acc: 0.59546\n",
      "Epoch: 049, Loss: 1.05235, Train Acc: 0.52000, Test Acc: 0.54928\n",
      "Epoch: 050, Loss: 1.08592, Train Acc: 0.59000, Test Acc: 0.57773\n",
      "Epoch: 051, Loss: 1.18001, Train Acc: 0.60000, Test Acc: 0.60701\n",
      "Epoch: 052, Loss: 0.95575, Train Acc: 0.55000, Test Acc: 0.51093\n",
      "Epoch: 053, Loss: 1.29403, Train Acc: 0.61000, Test Acc: 0.58021\n",
      "Epoch: 054, Loss: 1.00676, Train Acc: 0.57000, Test Acc: 0.52948\n",
      "Epoch: 055, Loss: 1.08306, Train Acc: 0.67000, Test Acc: 0.57361\n",
      "Epoch: 056, Loss: 1.04177, Train Acc: 0.63000, Test Acc: 0.58515\n",
      "Epoch: 057, Loss: 1.08205, Train Acc: 0.68000, Test Acc: 0.64124\n",
      "Epoch: 058, Loss: 1.06478, Train Acc: 0.59000, Test Acc: 0.55052\n",
      "Epoch: 059, Loss: 1.07296, Train Acc: 0.59000, Test Acc: 0.57732\n",
      "Epoch: 060, Loss: 1.03065, Train Acc: 0.67000, Test Acc: 0.61691\n",
      "Epoch: 061, Loss: 0.95105, Train Acc: 0.67000, Test Acc: 0.65773\n",
      "Epoch: 062, Loss: 0.95916, Train Acc: 0.65000, Test Acc: 0.63052\n",
      "Epoch: 063, Loss: 0.91308, Train Acc: 0.67000, Test Acc: 0.62680\n",
      "Epoch: 064, Loss: 0.89546, Train Acc: 0.64000, Test Acc: 0.61856\n",
      "Epoch: 065, Loss: 1.01595, Train Acc: 0.60000, Test Acc: 0.56289\n",
      "Epoch: 066, Loss: 0.92770, Train Acc: 0.68000, Test Acc: 0.61979\n",
      "Epoch: 067, Loss: 0.81883, Train Acc: 0.65000, Test Acc: 0.63175\n",
      "Epoch: 068, Loss: 0.92140, Train Acc: 0.68000, Test Acc: 0.64206\n",
      "Epoch: 069, Loss: 0.95179, Train Acc: 0.69000, Test Acc: 0.61237\n",
      "Epoch: 070, Loss: 0.85036, Train Acc: 0.69000, Test Acc: 0.63381\n",
      "Epoch: 071, Loss: 0.90225, Train Acc: 0.68000, Test Acc: 0.62969\n",
      "Epoch: 072, Loss: 0.82508, Train Acc: 0.66000, Test Acc: 0.62062\n",
      "Epoch: 073, Loss: 0.79855, Train Acc: 0.64000, Test Acc: 0.60866\n",
      "Epoch: 074, Loss: 0.86943, Train Acc: 0.70000, Test Acc: 0.64948\n",
      "Epoch: 075, Loss: 0.84037, Train Acc: 0.72000, Test Acc: 0.65155\n",
      "Epoch: 076, Loss: 0.80285, Train Acc: 0.65000, Test Acc: 0.62763\n",
      "Epoch: 077, Loss: 1.03006, Train Acc: 0.58000, Test Acc: 0.61732\n",
      "Epoch: 078, Loss: 0.92659, Train Acc: 0.68000, Test Acc: 0.63381\n",
      "Epoch: 079, Loss: 0.84808, Train Acc: 0.64000, Test Acc: 0.61361\n",
      "Epoch: 080, Loss: 0.87339, Train Acc: 0.67000, Test Acc: 0.61113\n",
      "Epoch: 081, Loss: 0.82051, Train Acc: 0.71000, Test Acc: 0.64866\n",
      "Epoch: 082, Loss: 0.77039, Train Acc: 0.68000, Test Acc: 0.64247\n",
      "Epoch: 083, Loss: 0.81996, Train Acc: 0.70000, Test Acc: 0.63876\n",
      "Epoch: 084, Loss: 0.83397, Train Acc: 0.67000, Test Acc: 0.62103\n",
      "Epoch: 085, Loss: 0.82630, Train Acc: 0.71000, Test Acc: 0.64041\n",
      "Epoch: 086, Loss: 0.76176, Train Acc: 0.66000, Test Acc: 0.63464\n",
      "Epoch: 087, Loss: 0.79596, Train Acc: 0.70000, Test Acc: 0.64454\n",
      "Epoch: 088, Loss: 0.74405, Train Acc: 0.62000, Test Acc: 0.60041\n",
      "Epoch: 089, Loss: 0.80255, Train Acc: 0.70000, Test Acc: 0.64495\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GraphConv, TopKPooling\n",
    "from torch_geometric.nn import global_max_pool as gmp\n",
    "from torch_geometric.nn import global_mean_pool as gap\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import utils\n",
    "\n",
    "train_dataset = utils.GraphDataset(\n",
    "    \"../data/\", \"MixedShapesSmallTrain_TRAIN\", True, n_quantiles=100\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = utils.GraphDataset(\n",
    "    \"../data/\", \"MixedShapesSmallTrain_TEST\", True, n_quantiles=100\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = GraphConv(1, 128)\n",
    "        self.pool1 = TopKPooling(128, ratio=0.8)\n",
    "        self.conv2 = GraphConv(128, 128)\n",
    "        self.pool2 = TopKPooling(128, ratio=0.8)\n",
    "        self.conv3 = GraphConv(128, 128)\n",
    "        self.pool3 = TopKPooling(128, ratio=0.8)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(256, 128)\n",
    "        self.lin2 = torch.nn.Linear(128, 64)\n",
    "        self.lin3 = torch.nn.Linear(64, 5)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool3(x, edge_index, None, batch)\n",
    "        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = x1 + x2 + x3\n",
    "\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.log_softmax(self.lin3(x), dim=-1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, data.y)\n",
    "        loss.backward()\n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data).max(dim=1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train(epoch)\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(\n",
    "        f\"Epoch: {epoch:03d}, Loss: {loss:.5f}, Train Acc: {train_acc:.5f}, \"\n",
    "        f\"Test Acc: {test_acc:.5f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
