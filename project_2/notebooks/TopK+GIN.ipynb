{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 000, Loss: 1.66425, Train Acc: 0.20000, Test Acc: 0.27134\n",
      "Epoch: 001, Loss: 1.63038, Train Acc: 0.21000, Test Acc: 0.28866\n",
      "Epoch: 002, Loss: 1.61610, Train Acc: 0.27000, Test Acc: 0.39340\n",
      "Epoch: 003, Loss: 1.60226, Train Acc: 0.27000, Test Acc: 0.42062\n",
      "Epoch: 004, Loss: 1.60519, Train Acc: 0.23000, Test Acc: 0.32536\n",
      "Epoch: 005, Loss: 1.57502, Train Acc: 0.26000, Test Acc: 0.39216\n",
      "Epoch: 006, Loss: 1.58937, Train Acc: 0.36000, Test Acc: 0.46639\n",
      "Epoch: 007, Loss: 1.56956, Train Acc: 0.38000, Test Acc: 0.46598\n",
      "Epoch: 008, Loss: 1.56812, Train Acc: 0.38000, Test Acc: 0.47381\n",
      "Epoch: 009, Loss: 1.58385, Train Acc: 0.26000, Test Acc: 0.34515\n",
      "Epoch: 010, Loss: 1.53109, Train Acc: 0.28000, Test Acc: 0.33443\n",
      "Epoch: 011, Loss: 1.51700, Train Acc: 0.39000, Test Acc: 0.45567\n",
      "Epoch: 012, Loss: 1.52111, Train Acc: 0.34000, Test Acc: 0.37320\n",
      "Epoch: 013, Loss: 1.52980, Train Acc: 0.34000, Test Acc: 0.44165\n",
      "Epoch: 014, Loss: 1.50593, Train Acc: 0.37000, Test Acc: 0.45897\n",
      "Epoch: 015, Loss: 1.49014, Train Acc: 0.32000, Test Acc: 0.41856\n",
      "Epoch: 016, Loss: 1.44762, Train Acc: 0.35000, Test Acc: 0.43010\n",
      "Epoch: 017, Loss: 1.39264, Train Acc: 0.44000, Test Acc: 0.47670\n",
      "Epoch: 018, Loss: 1.42491, Train Acc: 0.44000, Test Acc: 0.50103\n",
      "Epoch: 019, Loss: 1.43560, Train Acc: 0.45000, Test Acc: 0.48742\n",
      "Epoch: 020, Loss: 1.38425, Train Acc: 0.43000, Test Acc: 0.50103\n",
      "Epoch: 021, Loss: 1.40195, Train Acc: 0.53000, Test Acc: 0.53897\n",
      "Epoch: 022, Loss: 1.38775, Train Acc: 0.48000, Test Acc: 0.48866\n",
      "Epoch: 023, Loss: 1.41797, Train Acc: 0.52000, Test Acc: 0.53732\n",
      "Epoch 00025: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch: 024, Loss: 1.44578, Train Acc: 0.42000, Test Acc: 0.47711\n",
      "Epoch: 025, Loss: 1.35483, Train Acc: 0.40000, Test Acc: 0.46598\n",
      "Epoch: 026, Loss: 1.39666, Train Acc: 0.41000, Test Acc: 0.44619\n",
      "Epoch: 027, Loss: 1.40706, Train Acc: 0.42000, Test Acc: 0.44165\n",
      "Epoch: 028, Loss: 1.39604, Train Acc: 0.41000, Test Acc: 0.44990\n",
      "Epoch 00030: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch: 029, Loss: 1.38908, Train Acc: 0.41000, Test Acc: 0.46309\n",
      "Epoch: 030, Loss: 1.34722, Train Acc: 0.41000, Test Acc: 0.46515\n",
      "Epoch: 031, Loss: 1.35886, Train Acc: 0.42000, Test Acc: 0.48206\n",
      "Epoch: 032, Loss: 1.34956, Train Acc: 0.51000, Test Acc: 0.49485\n",
      "Epoch: 033, Loss: 1.35775, Train Acc: 0.52000, Test Acc: 0.51670\n",
      "Epoch 00035: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch: 034, Loss: 1.34926, Train Acc: 0.48000, Test Acc: 0.53608\n",
      "Epoch: 035, Loss: 1.35804, Train Acc: 0.51000, Test Acc: 0.53938\n",
      "Epoch: 036, Loss: 1.30533, Train Acc: 0.51000, Test Acc: 0.54103\n",
      "Epoch: 037, Loss: 1.29932, Train Acc: 0.51000, Test Acc: 0.54227\n",
      "Epoch: 038, Loss: 1.36341, Train Acc: 0.52000, Test Acc: 0.54598\n",
      "Epoch: 039, Loss: 1.36133, Train Acc: 0.53000, Test Acc: 0.54557\n",
      "Epoch: 040, Loss: 1.33100, Train Acc: 0.53000, Test Acc: 0.54763\n",
      "Epoch 00042: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch: 041, Loss: 1.35803, Train Acc: 0.57000, Test Acc: 0.54515\n",
      "Epoch: 042, Loss: 1.34016, Train Acc: 0.56000, Test Acc: 0.54268\n",
      "Epoch: 043, Loss: 1.33642, Train Acc: 0.56000, Test Acc: 0.54351\n",
      "Epoch: 044, Loss: 1.40441, Train Acc: 0.56000, Test Acc: 0.54474\n",
      "Epoch: 045, Loss: 1.35220, Train Acc: 0.56000, Test Acc: 0.54433\n",
      "Epoch 00047: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch: 046, Loss: 1.34227, Train Acc: 0.57000, Test Acc: 0.54598\n",
      "Epoch: 047, Loss: 1.32313, Train Acc: 0.58000, Test Acc: 0.54515\n",
      "Epoch: 048, Loss: 1.32584, Train Acc: 0.57000, Test Acc: 0.54722\n",
      "Epoch: 049, Loss: 1.35094, Train Acc: 0.57000, Test Acc: 0.54845\n",
      "Epoch: 050, Loss: 1.33637, Train Acc: 0.57000, Test Acc: 0.54845\n",
      "Epoch 00052: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch: 051, Loss: 1.34100, Train Acc: 0.57000, Test Acc: 0.54763\n",
      "Epoch: 052, Loss: 1.31533, Train Acc: 0.57000, Test Acc: 0.54763\n",
      "Epoch: 053, Loss: 1.31966, Train Acc: 0.57000, Test Acc: 0.54763\n",
      "Epoch: 054, Loss: 1.32413, Train Acc: 0.57000, Test Acc: 0.54763\n",
      "Epoch: 055, Loss: 1.33635, Train Acc: 0.57000, Test Acc: 0.54845\n",
      "Epoch 00057: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch: 056, Loss: 1.38283, Train Acc: 0.57000, Test Acc: 0.54887\n",
      "Epoch: 057, Loss: 1.34382, Train Acc: 0.57000, Test Acc: 0.54928\n",
      "Epoch: 058, Loss: 1.34982, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 059, Loss: 1.31180, Train Acc: 0.57000, Test Acc: 0.54845\n",
      "Epoch: 060, Loss: 1.31540, Train Acc: 0.57000, Test Acc: 0.54887\n",
      "Epoch 00062: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch: 061, Loss: 1.31902, Train Acc: 0.57000, Test Acc: 0.54887\n",
      "Epoch: 062, Loss: 1.35575, Train Acc: 0.57000, Test Acc: 0.54928\n",
      "Epoch: 063, Loss: 1.32110, Train Acc: 0.57000, Test Acc: 0.54928\n",
      "Epoch: 064, Loss: 1.31259, Train Acc: 0.57000, Test Acc: 0.54887\n",
      "Epoch: 065, Loss: 1.36619, Train Acc: 0.57000, Test Acc: 0.54887\n",
      "Epoch 00067: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Epoch: 066, Loss: 1.32559, Train Acc: 0.57000, Test Acc: 0.54887\n",
      "Epoch: 067, Loss: 1.33358, Train Acc: 0.57000, Test Acc: 0.54887\n",
      "Epoch: 068, Loss: 1.32229, Train Acc: 0.57000, Test Acc: 0.54928\n",
      "Epoch: 069, Loss: 1.33797, Train Acc: 0.57000, Test Acc: 0.54928\n",
      "Epoch: 070, Loss: 1.32392, Train Acc: 0.57000, Test Acc: 0.54928\n",
      "Epoch 00072: reducing learning rate of group 0 to 4.8828e-07.\n",
      "Epoch: 071, Loss: 1.31589, Train Acc: 0.57000, Test Acc: 0.55010\n",
      "Epoch: 072, Loss: 1.32393, Train Acc: 0.57000, Test Acc: 0.55010\n",
      "Epoch: 073, Loss: 1.31402, Train Acc: 0.57000, Test Acc: 0.55010\n",
      "Epoch: 074, Loss: 1.32971, Train Acc: 0.57000, Test Acc: 0.55010\n",
      "Epoch: 075, Loss: 1.34543, Train Acc: 0.57000, Test Acc: 0.55010\n",
      "Epoch 00077: reducing learning rate of group 0 to 2.4414e-07.\n",
      "Epoch: 076, Loss: 1.31429, Train Acc: 0.57000, Test Acc: 0.55010\n",
      "Epoch: 077, Loss: 1.32790, Train Acc: 0.57000, Test Acc: 0.55010\n",
      "Epoch: 078, Loss: 1.31155, Train Acc: 0.57000, Test Acc: 0.55010\n",
      "Epoch: 079, Loss: 1.34144, Train Acc: 0.57000, Test Acc: 0.55010\n",
      "Epoch: 080, Loss: 1.34652, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch 00082: reducing learning rate of group 0 to 1.2207e-07.\n",
      "Epoch: 081, Loss: 1.34384, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 082, Loss: 1.36885, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 083, Loss: 1.32296, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 084, Loss: 1.35720, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 085, Loss: 1.29683, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 086, Loss: 1.34301, Train Acc: 0.57000, Test Acc: 0.55052\n",
      "Epoch: 087, Loss: 1.33303, Train Acc: 0.57000, Test Acc: 0.55052\n",
      "Epoch: 088, Loss: 1.32124, Train Acc: 0.57000, Test Acc: 0.55010\n",
      "Epoch 00090: reducing learning rate of group 0 to 6.1035e-08.\n",
      "Epoch: 089, Loss: 1.34198, Train Acc: 0.57000, Test Acc: 0.55010\n",
      "Epoch: 090, Loss: 1.32329, Train Acc: 0.57000, Test Acc: 0.55010\n",
      "Epoch: 091, Loss: 1.33880, Train Acc: 0.57000, Test Acc: 0.55010\n",
      "Epoch: 092, Loss: 1.31151, Train Acc: 0.57000, Test Acc: 0.55010\n",
      "Epoch: 093, Loss: 1.35276, Train Acc: 0.57000, Test Acc: 0.55010\n",
      "Epoch 00095: reducing learning rate of group 0 to 3.0518e-08.\n",
      "Epoch: 094, Loss: 1.33417, Train Acc: 0.57000, Test Acc: 0.55010\n",
      "Epoch: 095, Loss: 1.32728, Train Acc: 0.57000, Test Acc: 0.55010\n",
      "Epoch: 096, Loss: 1.35780, Train Acc: 0.57000, Test Acc: 0.55010\n",
      "Epoch: 097, Loss: 1.33987, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 098, Loss: 1.30939, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 099, Loss: 1.28973, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 100, Loss: 1.32980, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 101, Loss: 1.33673, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 102, Loss: 1.34059, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch 00104: reducing learning rate of group 0 to 1.5259e-08.\n",
      "Epoch: 103, Loss: 1.34279, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 104, Loss: 1.30209, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 105, Loss: 1.34160, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 106, Loss: 1.29989, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 107, Loss: 1.31559, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 108, Loss: 1.33370, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 109, Loss: 1.34424, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 110, Loss: 1.34860, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 111, Loss: 1.33413, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 112, Loss: 1.32064, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 113, Loss: 1.32195, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 114, Loss: 1.30960, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 115, Loss: 1.26853, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 116, Loss: 1.38111, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 117, Loss: 1.35205, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 118, Loss: 1.36800, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 119, Loss: 1.36640, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 120, Loss: 1.34597, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 121, Loss: 1.29041, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 122, Loss: 1.34612, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 123, Loss: 1.35439, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 124, Loss: 1.32432, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 125, Loss: 1.35226, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 126, Loss: 1.34248, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 127, Loss: 1.34052, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 128, Loss: 1.31784, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 129, Loss: 1.31523, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 130, Loss: 1.36506, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 131, Loss: 1.32888, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 132, Loss: 1.34092, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 133, Loss: 1.33276, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 134, Loss: 1.31594, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 135, Loss: 1.36739, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 136, Loss: 1.35257, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 137, Loss: 1.36241, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 138, Loss: 1.29763, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 139, Loss: 1.32237, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 140, Loss: 1.33084, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 141, Loss: 1.35132, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 142, Loss: 1.33043, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 143, Loss: 1.31741, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 144, Loss: 1.32381, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 145, Loss: 1.33920, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 146, Loss: 1.31374, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 147, Loss: 1.31797, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 148, Loss: 1.34308, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 149, Loss: 1.33173, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 150, Loss: 1.29121, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 151, Loss: 1.29456, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 152, Loss: 1.36156, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 153, Loss: 1.30462, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 154, Loss: 1.32577, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 155, Loss: 1.33699, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 156, Loss: 1.35035, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 157, Loss: 1.33953, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 158, Loss: 1.32412, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 159, Loss: 1.32875, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 160, Loss: 1.29122, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 161, Loss: 1.35646, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 162, Loss: 1.31467, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 163, Loss: 1.34458, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 164, Loss: 1.34779, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 165, Loss: 1.33127, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 166, Loss: 1.32126, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 167, Loss: 1.29228, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 168, Loss: 1.29182, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 169, Loss: 1.32435, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 170, Loss: 1.31002, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 171, Loss: 1.31451, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 172, Loss: 1.32645, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 173, Loss: 1.29219, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 174, Loss: 1.35929, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 175, Loss: 1.32796, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 176, Loss: 1.31508, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 177, Loss: 1.35508, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 178, Loss: 1.35420, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 179, Loss: 1.31559, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 180, Loss: 1.31174, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 181, Loss: 1.30448, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 182, Loss: 1.33069, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 183, Loss: 1.32289, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 184, Loss: 1.28147, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 185, Loss: 1.36292, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 186, Loss: 1.34640, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 187, Loss: 1.34515, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 188, Loss: 1.32253, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 189, Loss: 1.31480, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 190, Loss: 1.28743, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 191, Loss: 1.34594, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 192, Loss: 1.29801, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 193, Loss: 1.31947, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 194, Loss: 1.35507, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 195, Loss: 1.33548, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 196, Loss: 1.32759, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 197, Loss: 1.36172, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 198, Loss: 1.33081, Train Acc: 0.57000, Test Acc: 0.54969\n",
      "Epoch: 199, Loss: 1.34371, Train Acc: 0.57000, Test Acc: 0.54969\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric as tg\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GraphConv, TopKPooling\n",
    "from torch_geometric.nn import global_max_pool as gmp\n",
    "from torch_geometric.nn import global_mean_pool as gap\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import utils\n",
    "\n",
    "train_dataset = utils.GraphDataset(\n",
    "    \"../data/\", \"MixedShapesSmallTrain_TRAIN\", True, n_quantiles=100\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = utils.GraphDataset(\n",
    "    \"../data/\", \"MixedShapesSmallTrain_TEST\", True, n_quantiles=100\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        nn = tg.nn.MLP([1, 64])\n",
    "        self.conv1 = tg.nn.GINConv(nn)\n",
    "        self.pool1 = TopKPooling(64, ratio=0.8)\n",
    "        nn = tg.nn.MLP([64, 64])\n",
    "        self.conv2 = tg.nn.GINConv(nn)\n",
    "        self.pool2 = TopKPooling(64, ratio=0.8)\n",
    "        nn = tg.nn.MLP([64, 64])\n",
    "        self.conv3 = tg.nn.GINConv(nn)\n",
    "        self.pool3 = TopKPooling(64, ratio=0.8)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(128, 64)\n",
    "        self.lin2 = torch.nn.Linear(64, 16)\n",
    "        self.lin3 = torch.nn.Linear(16, 5)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool3(x, edge_index, None, batch)\n",
    "        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)\n",
    "\n",
    "        x = x1 + x2 + x3\n",
    "\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.log_softmax(self.lin3(x), dim=-1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=3, mode=\"min\", cooldown=1, factor=0.5, verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "\n",
    "    loss_all = 0\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, data.y)\n",
    "        loss.backward()\n",
    "        loss_all += data.num_graphs * loss.item()\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred = model(data).max(dim=1)[1]\n",
    "\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "for epoch in range(200):\n",
    "    loss = train(epoch)\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    scheduler.step(loss)\n",
    "    print(\n",
    "        f\"Epoch: {epoch:03d}, Loss: {loss:.5f}, Train Acc: {train_acc:.5f}, \"\n",
    "        f\"Test Acc: {test_acc:.5f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
