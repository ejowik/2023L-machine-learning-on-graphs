{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import DenseGraphConv, GCNConv, dense_mincut_pool\n",
    "from torch_geometric.utils import to_dense_adj, to_dense_batch\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import utils\n",
    "import torch_geometric as tg\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stop_thresh = 25\n",
    "best_macro_f1 = -1\n",
    "\n",
    "\n",
    "train_dataset = utils.GraphDataset(\n",
    "    \"../data/\", \"MixedShapesSmallTrain_TRAIN\", True, n_quantiles=100\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "test_dataset = utils.GraphDataset(\n",
    "    \"../data/\", \"MixedShapesSmallTrain_TEST\", True, n_quantiles=100\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, graph_sizes, n_edge_layers=2, hidden_channels=8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.edge_layers = torch.nn.ModuleList()\n",
    "        for it in range(n_edge_layers):\n",
    "            nn = tg.nn.MLP([in_channels, hidden_channels])\n",
    "            self.edge_layers.append(tg.nn.GINEConv(nn, edge_dim=1))\n",
    "            in_channels = hidden_channels\n",
    "        self.gcn_layers = torch.nn.ModuleList()\n",
    "        for n_nodes in graph_sizes:\n",
    "            self.gcn_layers.append(Linear(hidden_channels, n_nodes))\n",
    "            nn = tg.nn.MLP([hidden_channels, hidden_channels])\n",
    "            self.gcn_layers.append(tg.nn.DenseGINConv(nn))\n",
    "            nn = tg.nn.MLP([hidden_channels, hidden_channels])\n",
    "            self.gcn_layers.append(tg.nn.DenseGINConv(nn))\n",
    "        nn = tg.nn.MLP([hidden_channels, hidden_channels])\n",
    "        self.gcn_layers.append(tg.nn.DenseGINConv(nn))\n",
    "\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = (\n",
    "            data.x,\n",
    "            data.edge_index,\n",
    "            data.edge_attr,\n",
    "            data.batch,\n",
    "        )\n",
    "        for layer in self.edge_layers:\n",
    "            x = layer(x, edge_index, edge_attr).relu()\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        adj = to_dense_adj(edge_index, batch)\n",
    "\n",
    "        mincut_loss = torch.zeros(1)\n",
    "        ortho_loss = torch.zeros(1)\n",
    "        for layer in self.gcn_layers:\n",
    "            if layer._get_name() == \"Linear\":\n",
    "                s = layer(x)\n",
    "                x, adj, mc, o = dense_mincut_pool(x, adj, s, mask)\n",
    "                mincut_loss += mc\n",
    "                ortho_loss += o\n",
    "                mask = None\n",
    "            else:\n",
    "                x = layer(x, adj).relu()\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1), mincut_loss, ortho_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net(1, 5, [50, 25], hidden_channels=32, n_edge_layers=1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=5, mode=\"min\", cooldown=2, factor=0.5, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_out, mc_loss, o_loss = model(data)\n",
    "        loss = F.nll_loss(y_out, data.y) + mc_loss + o_loss\n",
    "        # print(F.nll_loss(y_out, data.y),\"and\",mc_loss+o_loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    loss = 0\n",
    "    for data in loader:\n",
    "        y_out, mc_loss, o_loss = model(data)\n",
    "        y_pred.append(y_out.argmax(dim=-1))\n",
    "        y_true.append(data.y - 1)\n",
    "        loss += float(F.nll_loss(y_out, data.y) * data.num_graphs + mc_loss + o_loss)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    y_true = np.concatenate(y_true)\n",
    "    return (\n",
    "        f1_score(y_true=y_true, y_pred=y_pred, average=\"macro\"),\n",
    "        accuracy_score(y_true=y_true, y_pred=y_pred),\n",
    "        loss / len(loader.dataset),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train_Loss: 26.3546,Test_Loss: 28.0147,Train_f1: 0.0667,Test_f1: 0.0773,Train_acc: 0.2000,Test_acc: 0.2396\n",
      "Epoch: 002, Train_Loss: 21.5654,Test_Loss: 23.0461,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 003, Train_Loss: 23.6773,Test_Loss: 25.3701,Train_f1: 0.0000,Test_f1: 0.0000,Train_acc: 0.0000,Test_acc: 0.0000\n",
      "Epoch: 004, Train_Loss: 13.1858,Test_Loss: 13.9380,Train_f1: 0.0000,Test_f1: 0.0000,Train_acc: 0.0000,Test_acc: 0.0000\n",
      "Epoch: 005, Train_Loss: 6.3383,Test_Loss: 6.4604,Train_f1: 0.0000,Test_f1: 0.0000,Train_acc: 0.0000,Test_acc: 0.0000\n",
      "Epoch: 006, Train_Loss: 6.4619,Test_Loss: 5.9951,Train_f1: 0.0667,Test_f1: 0.0773,Train_acc: 0.2000,Test_acc: 0.2396\n",
      "Epoch: 007, Train_Loss: 6.5756,Test_Loss: 5.6607,Train_f1: 0.0667,Test_f1: 0.0588,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 008, Train_Loss: 6.1597,Test_Loss: 5.1931,Train_f1: 0.0667,Test_f1: 0.0588,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 009, Train_Loss: 4.7403,Test_Loss: 3.9867,Train_f1: 0.0667,Test_f1: 0.0588,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 010, Train_Loss: 2.9932,Test_Loss: 2.5743,Train_f1: 0.0667,Test_f1: 0.0588,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 011, Train_Loss: 2.0696,Test_Loss: 1.8882,Train_f1: 0.0667,Test_f1: 0.0773,Train_acc: 0.2000,Test_acc: 0.2396\n",
      "Epoch: 012, Train_Loss: 1.8452,Test_Loss: 1.8004,Train_f1: 0.0667,Test_f1: 0.0588,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 013, Train_Loss: 1.7090,Test_Loss: 1.7122,Train_f1: 0.0667,Test_f1: 0.0773,Train_acc: 0.2000,Test_acc: 0.2396\n",
      "Epoch: 014, Train_Loss: 1.7463,Test_Loss: 1.8481,Train_f1: 0.0000,Test_f1: 0.0000,Train_acc: 0.0000,Test_acc: 0.0000\n",
      "Epoch: 015, Train_Loss: 1.8034,Test_Loss: 1.9275,Train_f1: 0.0667,Test_f1: 0.0850,Train_acc: 0.2000,Test_acc: 0.2697\n",
      "Epoch: 016, Train_Loss: 1.7758,Test_Loss: 1.8830,Train_f1: 0.0667,Test_f1: 0.0850,Train_acc: 0.2000,Test_acc: 0.2697\n",
      "Epoch: 017, Train_Loss: 1.7389,Test_Loss: 1.7959,Train_f1: 0.0667,Test_f1: 0.0850,Train_acc: 0.2000,Test_acc: 0.2697\n",
      "Epoch: 018, Train_Loss: 1.7189,Test_Loss: 1.7240,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 019, Train_Loss: 1.6408,Test_Loss: 1.6165,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 020, Train_Loss: 1.6321,Test_Loss: 1.6169,Train_f1: 0.0667,Test_f1: 0.0588,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 021, Train_Loss: 1.6611,Test_Loss: 1.6907,Train_f1: 0.0000,Test_f1: 0.0000,Train_acc: 0.0000,Test_acc: 0.0000\n",
      "Epoch: 022, Train_Loss: 1.6244,Test_Loss: 1.6353,Train_f1: 0.0667,Test_f1: 0.0773,Train_acc: 0.2000,Test_acc: 0.2396\n",
      "Epoch: 023, Train_Loss: 1.6275,Test_Loss: 1.6287,Train_f1: 0.0667,Test_f1: 0.0773,Train_acc: 0.2000,Test_acc: 0.2396\n",
      "Epoch: 024, Train_Loss: 1.6340,Test_Loss: 1.6241,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 025, Train_Loss: 1.6193,Test_Loss: 1.6069,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 026, Train_Loss: 1.6405,Test_Loss: 1.6235,Train_f1: 0.0667,Test_f1: 0.0859,Train_acc: 0.2000,Test_acc: 0.2701\n",
      "Epoch: 027, Train_Loss: 1.6241,Test_Loss: 1.6406,Train_f1: 0.0000,Test_f1: 0.0000,Train_acc: 0.0000,Test_acc: 0.0000\n",
      "Epoch: 028, Train_Loss: 1.6221,Test_Loss: 1.6443,Train_f1: 0.0000,Test_f1: 0.0000,Train_acc: 0.0000,Test_acc: 0.0000\n",
      "Epoch: 029, Train_Loss: 1.6195,Test_Loss: 1.6303,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 030, Train_Loss: 1.6191,Test_Loss: 1.6162,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 031, Train_Loss: 1.6177,Test_Loss: 1.6103,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 032, Train_Loss: 1.6169,Test_Loss: 1.6153,Train_f1: 0.0667,Test_f1: 0.0773,Train_acc: 0.2000,Test_acc: 0.2396\n",
      "Epoch: 033, Train_Loss: 1.6191,Test_Loss: 1.6263,Train_f1: 0.0667,Test_f1: 0.0773,Train_acc: 0.2000,Test_acc: 0.2396\n",
      "Epoch: 034, Train_Loss: 1.6194,Test_Loss: 1.6297,Train_f1: 0.0667,Test_f1: 0.0773,Train_acc: 0.2000,Test_acc: 0.2396\n",
      "Epoch: 035, Train_Loss: 1.6166,Test_Loss: 1.6225,Train_f1: 0.0667,Test_f1: 0.0850,Train_acc: 0.2000,Test_acc: 0.2697\n",
      "Epoch: 036, Train_Loss: 1.6151,Test_Loss: 1.6139,Train_f1: 0.0667,Test_f1: 0.0588,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 037, Train_Loss: 1.6162,Test_Loss: 1.6109,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 038, Train_Loss: 1.6177,Test_Loss: 1.6139,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 039, Train_Loss: 1.6172,Test_Loss: 1.6188,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 040, Train_Loss: 1.6155,Test_Loss: 1.6232,Train_f1: 0.0623,Test_f1: 0.0404,Train_acc: 0.2000,Test_acc: 0.1287\n",
      "Epoch: 041, Train_Loss: 1.6152,Test_Loss: 1.6267,Train_f1: 0.0000,Test_f1: 0.0000,Train_acc: 0.0000,Test_acc: 0.0000\n",
      "Epoch 00042: reducing learning rate of group 0 to 2.5000e-03.\n",
      "Epoch: 042, Train_Loss: 1.6156,Test_Loss: 1.6272,Train_f1: 0.0000,Test_f1: 0.0000,Train_acc: 0.0000,Test_acc: 0.0000\n",
      "Epoch: 043, Train_Loss: 1.6153,Test_Loss: 1.6252,Train_f1: 0.0000,Test_f1: 0.0000,Train_acc: 0.0000,Test_acc: 0.0000\n",
      "Epoch: 044, Train_Loss: 1.6149,Test_Loss: 1.6218,Train_f1: 0.0667,Test_f1: 0.0773,Train_acc: 0.2000,Test_acc: 0.2396\n",
      "Epoch: 045, Train_Loss: 1.6148,Test_Loss: 1.6179,Train_f1: 0.0667,Test_f1: 0.0773,Train_acc: 0.2000,Test_acc: 0.2396\n",
      "Epoch: 046, Train_Loss: 1.6150,Test_Loss: 1.6143,Train_f1: 0.0667,Test_f1: 0.0773,Train_acc: 0.2000,Test_acc: 0.2396\n",
      "Epoch: 047, Train_Loss: 1.6151,Test_Loss: 1.6116,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 048, Train_Loss: 1.6151,Test_Loss: 1.6102,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 049, Train_Loss: 1.6148,Test_Loss: 1.6107,Train_f1: 0.0672,Test_f1: 0.0462,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 050, Train_Loss: 1.6144,Test_Loss: 1.6130,Train_f1: 0.0667,Test_f1: 0.0577,Train_acc: 0.2000,Test_acc: 0.1682\n",
      "Epoch: 051, Train_Loss: 1.6143,Test_Loss: 1.6165,Train_f1: 0.0667,Test_f1: 0.0850,Train_acc: 0.2000,Test_acc: 0.2697\n",
      "Epoch: 052, Train_Loss: 1.6145,Test_Loss: 1.6203,Train_f1: 0.0667,Test_f1: 0.0850,Train_acc: 0.2000,Test_acc: 0.2697\n",
      "Epoch: 053, Train_Loss: 1.6148,Test_Loss: 1.6230,Train_f1: 0.0667,Test_f1: 0.0850,Train_acc: 0.2000,Test_acc: 0.2697\n",
      "Epoch: 054, Train_Loss: 1.6148,Test_Loss: 1.6236,Train_f1: 0.0667,Test_f1: 0.0850,Train_acc: 0.2000,Test_acc: 0.2697\n",
      "Epoch: 055, Train_Loss: 1.6145,Test_Loss: 1.6222,Train_f1: 0.0586,Test_f1: 0.0774,Train_acc: 0.1900,Test_acc: 0.2689\n",
      "Epoch 00056: reducing learning rate of group 0 to 1.2500e-03.\n",
      "Epoch: 056, Train_Loss: 1.6143,Test_Loss: 1.6197,Train_f1: 0.0000,Test_f1: 0.0000,Train_acc: 0.0000,Test_acc: 0.0000\n",
      "Epoch: 057, Train_Loss: 1.6143,Test_Loss: 1.6184,Train_f1: 0.0494,Test_f1: 0.0365,Train_acc: 0.1200,Test_acc: 0.0870\n",
      "Epoch: 058, Train_Loss: 1.6143,Test_Loss: 1.6173,Train_f1: 0.0672,Test_f1: 0.0461,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 059, Train_Loss: 1.6143,Test_Loss: 1.6165,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 060, Train_Loss: 1.6143,Test_Loss: 1.6160,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 061, Train_Loss: 1.6143,Test_Loss: 1.6159,Train_f1: 0.0696,Test_f1: 0.0468,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 062, Train_Loss: 1.6143,Test_Loss: 1.6161,Train_f1: 0.1258,Test_f1: 0.1315,Train_acc: 0.2200,Test_acc: 0.2272\n",
      "Epoch: 063, Train_Loss: 1.6142,Test_Loss: 1.6165,Train_f1: 0.0667,Test_f1: 0.0855,Train_acc: 0.2000,Test_acc: 0.2697\n",
      "Epoch: 064, Train_Loss: 1.6142,Test_Loss: 1.6169,Train_f1: 0.0667,Test_f1: 0.0850,Train_acc: 0.2000,Test_acc: 0.2697\n",
      "Epoch: 065, Train_Loss: 1.6142,Test_Loss: 1.6173,Train_f1: 0.0667,Test_f1: 0.0850,Train_acc: 0.2000,Test_acc: 0.2697\n",
      "Epoch: 066, Train_Loss: 1.6143,Test_Loss: 1.6176,Train_f1: 0.0667,Test_f1: 0.0879,Train_acc: 0.2000,Test_acc: 0.2709\n",
      "Epoch: 067, Train_Loss: 1.6143,Test_Loss: 1.6177,Train_f1: 0.0667,Test_f1: 0.0965,Train_acc: 0.2000,Test_acc: 0.2746\n",
      "Epoch 00068: reducing learning rate of group 0 to 6.2500e-04.\n",
      "Epoch: 068, Train_Loss: 1.6142,Test_Loss: 1.6177,Train_f1: 0.0672,Test_f1: 0.1001,Train_acc: 0.2000,Test_acc: 0.2763\n",
      "Epoch: 069, Train_Loss: 1.6142,Test_Loss: 1.6177,Train_f1: 0.0667,Test_f1: 0.0965,Train_acc: 0.2000,Test_acc: 0.2746\n",
      "Epoch: 070, Train_Loss: 1.6142,Test_Loss: 1.6175,Train_f1: 0.0667,Test_f1: 0.0879,Train_acc: 0.2000,Test_acc: 0.2709\n",
      "Epoch: 071, Train_Loss: 1.6142,Test_Loss: 1.6174,Train_f1: 0.0667,Test_f1: 0.0850,Train_acc: 0.2000,Test_acc: 0.2697\n",
      "Epoch: 072, Train_Loss: 1.6142,Test_Loss: 1.6173,Train_f1: 0.0667,Test_f1: 0.0850,Train_acc: 0.2000,Test_acc: 0.2697\n",
      "Epoch: 073, Train_Loss: 1.6142,Test_Loss: 1.6171,Train_f1: 0.0667,Test_f1: 0.0850,Train_acc: 0.2000,Test_acc: 0.2697\n",
      "Epoch: 074, Train_Loss: 1.6142,Test_Loss: 1.6170,Train_f1: 0.0667,Test_f1: 0.0850,Train_acc: 0.2000,Test_acc: 0.2697\n",
      "Epoch: 075, Train_Loss: 1.6142,Test_Loss: 1.6169,Train_f1: 0.0667,Test_f1: 0.0851,Train_acc: 0.2000,Test_acc: 0.2697\n",
      "Epoch 00076: reducing learning rate of group 0 to 3.1250e-04.\n",
      "Epoch: 076, Train_Loss: 1.6142,Test_Loss: 1.6169,Train_f1: 0.0672,Test_f1: 0.0870,Train_acc: 0.2000,Test_acc: 0.2701\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m train()\n\u001b[1;32m      3\u001b[0m train_macro_f1,train_acc, train_loss \u001b[39m=\u001b[39m test(train_loader)\n\u001b[0;32m----> 4\u001b[0m test_macro_f1,test_acc, test_loss \u001b[39m=\u001b[39m test(test_loader)\n\u001b[1;32m      5\u001b[0m scheduler\u001b[39m.\u001b[39mstep(train_loss)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[1;32m      7\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train_Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m02.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m,Test_Loss: \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m:\u001b[39;00m\u001b[39m02.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m,Train_f1: \u001b[39m\u001b[39m{\u001b[39;00mtrain_macro_f1\u001b[39m:\u001b[39;00m\u001b[39m01.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m,Test_f1: \u001b[39m\u001b[39m{\u001b[39;00mtest_macro_f1\u001b[39m:\u001b[39;00m\u001b[39m01.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m,Train_acc: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m01.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m,Test_acc: \u001b[39m\u001b[39m{\u001b[39;00mtest_acc\u001b[39m:\u001b[39;00m\u001b[39m01.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[35], line 22\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m     20\u001b[0m y_true \u001b[39m=\u001b[39m []\n\u001b[1;32m     21\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m loader:\n\u001b[1;32m     23\u001b[0m     y_out,mc_loss, o_loss \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     24\u001b[0m     y_pred\u001b[39m.\u001b[39mappend(y_out\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Projects/2023L-machine-learning-on-graphs/project_2/utils.py:141\u001b[0m, in \u001b[0;36mGraphDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    138\u001b[0m data\u001b[39m.\u001b[39my \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels[idx] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    139\u001b[0m data\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m    140\u001b[0m data\u001b[39m.\u001b[39medge_attr \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 141\u001b[0m     (data\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m) \u001b[39m*\u001b[39;49m \u001b[39m100\u001b[39;49m)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m    142\u001b[0m     \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39mweight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m )\n\u001b[1;32m    145\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 501):\n",
    "    train()\n",
    "    train_macro_f1, train_acc, train_loss = test(train_loader)\n",
    "    test_macro_f1, test_acc, test_loss = test(test_loader)\n",
    "    scheduler.step(train_loss)\n",
    "    print(\n",
    "        f\"Epoch: {epoch:03d}, Train_Loss: {train_loss:02.4f},Test_Loss: {test_loss:02.4f},Train_f1: {train_macro_f1:01.4f},Test_f1: {test_macro_f1:01.4f},Train_acc: {train_acc:01.4f},Test_acc: {test_acc:01.4f}\"\n",
    "    )\n",
    "    if test_macro_f1 > best_macro_f1:\n",
    "        best_accuracy = test_macro_f1\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), \"../data/best_model.pth\")\n",
    "    # elif epoch - best_epoch > early_stop_thresh:\n",
    "    #     print(\n",
    "    #         f\"Early stopped training at epoch {epoch} best macro F1 {best_macro_f1} in {best_epoch}\"\n",
    "    #     )\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
