{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import utils\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from os import path\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import MLP, GINConv, global_add_pool\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dirpath,\n",
    "        dataset,\n",
    "        quantile: bool = True,\n",
    "        n_quantiles: int = 25,\n",
    "        cache: bool = True,\n",
    "    ):\n",
    "        X_ts, labels = GraphDataset.readucr(\n",
    "            path.join(dirpath, dataset, f\"{dataset}.txt\")\n",
    "        )\n",
    "        self.X_ts = pd.DataFrame(X_ts.T)\n",
    "        self.labels = torch.tensor(labels == 1, dtype=int)\n",
    "\n",
    "        subdirname = \"quantile_\" + str(n_quantiles) if quantile else \"visibility\"\n",
    "        self.path = path.join(dirpath, dataset, subdirname)\n",
    "        if path.exists(self.path):\n",
    "            return\n",
    "        os.mkdir(self.path)\n",
    "        for idx, col in tqdm(\n",
    "            enumerate(self.X_ts.columns), total=len(self.X_ts.columns)\n",
    "        ):\n",
    "            if quantile:\n",
    "                torch.save(\n",
    "                    from_networkx(\n",
    "                        utils.df_to_quantile_graph(\n",
    "                            self.X_ts, y_col=col, n_quantiles=n_quantiles\n",
    "                        )\n",
    "                    ),\n",
    "                    path.join(self.path, f\"{idx}.pt\"),\n",
    "                )\n",
    "            else:\n",
    "                torch.save(\n",
    "                    from_networkx(utils.df_to_visibility_graph(self.X_ts, y_col=col)),\n",
    "                    path.join(self.path, f\"{idx}.pt\"),\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.load(path.join(self.path, f\"{idx}.pt\"))\n",
    "        data.y = self.labels[idx]\n",
    "        data.x = data.x.unsqueeze(1)\n",
    "        return data\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(path.join(self.path, f\"{idx}.pt\"))\n",
    "        data.y = self.labels[idx] == 1\n",
    "        data.x = data.x.unsqueeze(1)\n",
    "        return data\n",
    "\n",
    "    def readucr(filename):\n",
    "        data = np.loadtxt(filename)\n",
    "        y = data[:, 0]\n",
    "        x = data[:, 1:]\n",
    "        return x, y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.5663, Train: 0.7170, Test: 0.7045\n",
      "Epoch: 002, Loss: 0.6079, Train: 0.5193, Test: 0.5220\n",
      "Epoch: 003, Loss: 0.5662, Train: 0.6637, Test: 0.6462\n",
      "Epoch: 004, Loss: 0.5251, Train: 0.7392, Test: 0.7197\n",
      "Epoch: 005, Loss: 0.5536, Train: 0.7756, Test: 0.7765\n",
      "Epoch: 006, Loss: 0.4798, Train: 0.8312, Test: 0.8326\n",
      "Epoch: 007, Loss: 0.4331, Train: 0.8467, Test: 0.8447\n",
      "Epoch: 008, Loss: 0.4331, Train: 0.8525, Test: 0.8591\n",
      "Epoch: 009, Loss: 0.4008, Train: 0.8686, Test: 0.8765\n",
      "Epoch: 010, Loss: 0.3950, Train: 0.7631, Test: 0.7424\n",
      "Epoch: 011, Loss: 0.3969, Train: 0.8692, Test: 0.8705\n",
      "Epoch: 012, Loss: 0.3925, Train: 0.7987, Test: 0.7939\n",
      "Epoch: 013, Loss: 0.3728, Train: 0.8684, Test: 0.8629\n",
      "Epoch: 014, Loss: 0.3561, Train: 0.7809, Test: 0.7621\n",
      "Epoch: 015, Loss: 0.3503, Train: 0.8834, Test: 0.8750\n",
      "Epoch: 016, Loss: 0.3676, Train: 0.8523, Test: 0.8462\n",
      "Epoch: 017, Loss: 0.3581, Train: 0.8267, Test: 0.8303\n",
      "Epoch: 018, Loss: 0.3725, Train: 0.7981, Test: 0.7932\n",
      "Epoch: 019, Loss: 0.3794, Train: 0.8101, Test: 0.7902\n",
      "Epoch: 020, Loss: 0.3655, Train: 0.8250, Test: 0.8258\n",
      "Epoch: 021, Loss: 0.3410, Train: 0.8381, Test: 0.8311\n",
      "Epoch: 022, Loss: 0.3388, Train: 0.8970, Test: 0.8902\n",
      "Epoch: 023, Loss: 0.3650, Train: 0.8875, Test: 0.8811\n",
      "Epoch: 024, Loss: 0.3125, Train: 0.8706, Test: 0.8705\n",
      "Epoch: 025, Loss: 0.3244, Train: 0.8984, Test: 0.8902\n",
      "Epoch: 026, Loss: 0.3586, Train: 0.8884, Test: 0.8841\n",
      "Epoch: 027, Loss: 0.3405, Train: 0.9039, Test: 0.9008\n",
      "Epoch: 028, Loss: 0.3592, Train: 0.9034, Test: 0.8992\n",
      "Epoch: 029, Loss: 0.3143, Train: 0.8403, Test: 0.8386\n",
      "Epoch: 030, Loss: 0.3748, Train: 0.8889, Test: 0.8894\n",
      "Epoch: 031, Loss: 0.3335, Train: 0.8509, Test: 0.8568\n",
      "Epoch: 032, Loss: 0.3212, Train: 0.6076, Test: 0.5977\n",
      "Epoch: 033, Loss: 0.3061, Train: 0.9020, Test: 0.8917\n",
      "Epoch: 034, Loss: 0.3354, Train: 0.8778, Test: 0.8780\n",
      "Epoch: 035, Loss: 0.3098, Train: 0.9034, Test: 0.9053\n",
      "Epoch: 036, Loss: 0.3179, Train: 0.9092, Test: 0.9098\n",
      "Epoch: 037, Loss: 0.3270, Train: 0.8984, Test: 0.8939\n",
      "Epoch: 038, Loss: 0.3165, Train: 0.9009, Test: 0.9068\n",
      "Epoch: 039, Loss: 0.2936, Train: 0.9084, Test: 0.9076\n",
      "Epoch: 040, Loss: 0.2885, Train: 0.8978, Test: 0.8955\n",
      "Epoch: 041, Loss: 0.3501, Train: 0.8498, Test: 0.8538\n",
      "Epoch: 042, Loss: 0.3014, Train: 0.7770, Test: 0.7871\n",
      "Epoch: 043, Loss: 0.3073, Train: 0.8606, Test: 0.8636\n",
      "Epoch: 044, Loss: 0.2893, Train: 0.9056, Test: 0.9015\n",
      "Epoch: 045, Loss: 0.3159, Train: 0.8920, Test: 0.8848\n",
      "Epoch: 046, Loss: 0.2987, Train: 0.8378, Test: 0.8318\n",
      "Epoch: 047, Loss: 0.3228, Train: 0.8873, Test: 0.8848\n",
      "Epoch: 048, Loss: 0.3140, Train: 0.8789, Test: 0.8689\n",
      "Epoch: 049, Loss: 0.3240, Train: 0.8875, Test: 0.8939\n",
      "Epoch: 050, Loss: 0.2795, Train: 0.8953, Test: 0.8939\n",
      "Epoch: 051, Loss: 0.2793, Train: 0.9011, Test: 0.8902\n",
      "Epoch: 052, Loss: 0.2920, Train: 0.9106, Test: 0.9098\n",
      "Epoch: 053, Loss: 0.2800, Train: 0.9036, Test: 0.8992\n",
      "Epoch: 054, Loss: 0.3041, Train: 0.8895, Test: 0.8939\n"
     ]
    }
   ],
   "source": [
    "### Zerżnięte z: https://github.com/pyg-team/pytorch_geometric/blob/master/examples/compile/gin.py\n",
    "### znalezione przez: https://pytorch-geometric.readthedocs.io/en/latest/tutorial/compile.html?highlight=compile#basic-usage\n",
    "\n",
    "train_dataset = GraphDataset(\"../data/\", \"FordA_TRAIN\", False, n_quantiles=5)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = GraphDataset(\"../data/\", \"FordA_TEST\", False, n_quantiles=5)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(5):\n",
    "            mlp = MLP([in_channels, 32, 32])\n",
    "            self.convs.append(GINConv(mlp, train_eps=False))\n",
    "            in_channels = 32\n",
    "\n",
    "        self.mlp = MLP([32, 32, out_channels], norm=None, dropout=0.5)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index).relu()\n",
    "        x = global_add_pool(x, batch)\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "model = GIN(1, 2)\n",
    "\n",
    "# Compile the model into an optimized version:\n",
    "# Note that `compile(model, dynamic=True)` does not work yet in PyTorch 2.0, so\n",
    "# we use `transforms.Pad` and static compilation as a current workaround.\n",
    "# See: https://github.com/pytorch/pytorch/issues/94640\n",
    "\n",
    "# to coś mi nie działa >:(\n",
    "# model = torch.compile(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = F.cross_entropy(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_correct = 0\n",
    "    for data in loader:\n",
    "        pred = model(data.x, data.edge_index, data.batch).argmax(dim=-1)\n",
    "        total_correct += int((pred == data.y).sum())\n",
    "    return total_correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(\n",
    "        f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, \"\n",
    "        f\"Test: {test_acc:.4f}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
