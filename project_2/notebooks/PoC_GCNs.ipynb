{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import utils\n",
    "from gcn_models import GIN, GATC, SAGE\n",
    "\n",
    "from os import path\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stop_thresh = 25\n",
    "best_macro_f1 = -1\n",
    "\n",
    "\n",
    "train_dataset = utils.GraphDataset(\n",
    "    \"../data/\", \"MixedShapesSmallTrain_TRAIN\", True, n_quantiles=100\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = utils.GraphDataset(\n",
    "    \"../data/\", \"MixedShapesSmallTrain_TEST\", True, n_quantiles=100\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "\n",
    "model = GIN(1, [8, 16, 16, 8], pool_method=\"avg\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=5, mode=\"min\", cooldown=2, factor=0.5, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        loss = F.cross_entropy(out, data.y - 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader, return_loss=True):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    loss = 0\n",
    "    for data in loader:\n",
    "        y_out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        y_pred.append(y_out.argmax(dim=-1))\n",
    "        y_true.append(data.y - 1)\n",
    "        if return_loss:\n",
    "            loss += float(F.cross_entropy(y_out, data.y - 1)) * data.num_graphs\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    y_true = np.concatenate(y_true)\n",
    "    if return_loss:\n",
    "        return (\n",
    "            f1_score(y_true=y_true, y_pred=y_pred, average=\"macro\"),\n",
    "            accuracy_score(y_true=y_true, y_pred=y_pred),\n",
    "            loss / len(loader.dataset),\n",
    "        )\n",
    "    return f1_score(y_true=y_true, y_pred=y_pred, average=\"macro\"), accuracy_score(\n",
    "        y_true=y_true, y_pred=y_pred\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train_Loss: 2.2282,Test_Loss: 2.1861,Train_f1: 0.1498,Test_f1: 0.1746,Train_acc: 0.2500,Test_acc: 0.3043\n",
      "Epoch: 002, Train_Loss: 1.9125,Test_Loss: 1.9089,Train_f1: 0.1634,Test_f1: 0.1580,Train_acc: 0.2600,Test_acc: 0.2816\n",
      "Epoch: 003, Train_Loss: 1.6815,Test_Loss: 1.7197,Train_f1: 0.0741,Test_f1: 0.0552,Train_acc: 0.1400,Test_acc: 0.0940\n",
      "Epoch: 004, Train_Loss: 1.6349,Test_Loss: 1.6964,Train_f1: 0.0606,Test_f1: 0.0555,Train_acc: 0.1500,Test_acc: 0.1142\n",
      "Epoch: 005, Train_Loss: 1.6648,Test_Loss: 1.7233,Train_f1: 0.0667,Test_f1: 0.0720,Train_acc: 0.2000,Test_acc: 0.1905\n",
      "Epoch: 006, Train_Loss: 1.7006,Test_Loss: 1.7430,Train_f1: 0.0667,Test_f1: 0.0635,Train_acc: 0.2000,Test_acc: 0.1889\n",
      "Epoch: 007, Train_Loss: 1.6934,Test_Loss: 1.7369,Train_f1: 0.0667,Test_f1: 0.0635,Train_acc: 0.2000,Test_acc: 0.1889\n",
      "Epoch: 008, Train_Loss: 1.6799,Test_Loss: 1.7143,Train_f1: 0.0644,Test_f1: 0.0649,Train_acc: 0.1900,Test_acc: 0.1893\n",
      "Epoch: 009, Train_Loss: 1.7018,Test_Loss: 1.7140,Train_f1: 0.0672,Test_f1: 0.0660,Train_acc: 0.2000,Test_acc: 0.1885\n",
      "Epoch 00010: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch: 010, Train_Loss: 1.7470,Test_Loss: 1.7498,Train_f1: 0.1208,Test_f1: 0.0748,Train_acc: 0.2200,Test_acc: 0.1728\n",
      "Epoch: 011, Train_Loss: 1.7587,Test_Loss: 1.7525,Train_f1: 0.0402,Test_f1: 0.0297,Train_acc: 0.0700,Test_acc: 0.0520\n",
      "Epoch: 012, Train_Loss: 1.8025,Test_Loss: 1.7940,Train_f1: 0.0946,Test_f1: 0.1027,Train_acc: 0.1900,Test_acc: 0.2309\n",
      "Epoch: 013, Train_Loss: 1.8369,Test_Loss: 1.8295,Train_f1: 0.0696,Test_f1: 0.0898,Train_acc: 0.2000,Test_acc: 0.2660\n",
      "Epoch: 014, Train_Loss: 1.8965,Test_Loss: 1.8808,Train_f1: 0.0667,Test_f1: 0.0850,Train_acc: 0.2000,Test_acc: 0.2697\n",
      "Epoch: 015, Train_Loss: 1.9823,Test_Loss: 1.9512,Train_f1: 0.0667,Test_f1: 0.0850,Train_acc: 0.2000,Test_acc: 0.2697\n",
      "Epoch: 016, Train_Loss: 1.8793,Test_Loss: 1.8501,Train_f1: 0.0667,Test_f1: 0.0850,Train_acc: 0.2000,Test_acc: 0.2697\n",
      "Epoch: 017, Train_Loss: 1.7760,Test_Loss: 1.7676,Train_f1: 0.0696,Test_f1: 0.0874,Train_acc: 0.2000,Test_acc: 0.2652\n",
      "Epoch 00018: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch: 018, Train_Loss: 1.7112,Test_Loss: 1.7282,Train_f1: 0.0925,Test_f1: 0.0830,Train_acc: 0.2000,Test_acc: 0.1889\n",
      "Epoch: 019, Train_Loss: 1.6293,Test_Loss: 1.6562,Train_f1: 0.1266,Test_f1: 0.1348,Train_acc: 0.2300,Test_acc: 0.2474\n",
      "Epoch: 020, Train_Loss: 1.5570,Test_Loss: 1.5977,Train_f1: 0.1836,Test_f1: 0.1271,Train_acc: 0.2800,Test_acc: 0.1934\n",
      "Epoch: 021, Train_Loss: 1.5915,Test_Loss: 1.6245,Train_f1: 0.0860,Test_f1: 0.0832,Train_acc: 0.2100,Test_acc: 0.1946\n",
      "Epoch: 022, Train_Loss: 1.6331,Test_Loss: 1.6527,Train_f1: 0.0672,Test_f1: 0.0713,Train_acc: 0.2000,Test_acc: 0.1909\n",
      "Epoch: 023, Train_Loss: 1.6026,Test_Loss: 1.6242,Train_f1: 0.0863,Test_f1: 0.0733,Train_acc: 0.2100,Test_acc: 0.1909\n",
      "Epoch: 024, Train_Loss: 1.5767,Test_Loss: 1.6080,Train_f1: 0.1032,Test_f1: 0.0852,Train_acc: 0.2200,Test_acc: 0.1951\n",
      "Epoch: 025, Train_Loss: 1.5652,Test_Loss: 1.5964,Train_f1: 0.1032,Test_f1: 0.0956,Train_acc: 0.2200,Test_acc: 0.1992\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch: 026, Train_Loss: 1.5602,Test_Loss: 1.5833,Train_f1: 0.1032,Test_f1: 0.0937,Train_acc: 0.2200,Test_acc: 0.1984\n",
      "Epoch: 027, Train_Loss: 1.5586,Test_Loss: 1.5761,Train_f1: 0.1023,Test_f1: 0.0862,Train_acc: 0.2200,Test_acc: 0.1946\n",
      "Epoch: 028, Train_Loss: 1.5575,Test_Loss: 1.5734,Train_f1: 0.1023,Test_f1: 0.0849,Train_acc: 0.2200,Test_acc: 0.1951\n",
      "Epoch: 029, Train_Loss: 1.5463,Test_Loss: 1.5679,Train_f1: 0.1023,Test_f1: 0.0891,Train_acc: 0.2200,Test_acc: 0.1971\n",
      "Epoch: 030, Train_Loss: 1.5381,Test_Loss: 1.5676,Train_f1: 0.1336,Test_f1: 0.0965,Train_acc: 0.2400,Test_acc: 0.1992\n",
      "Epoch: 031, Train_Loss: 1.5388,Test_Loss: 1.5697,Train_f1: 0.1571,Test_f1: 0.1212,Train_acc: 0.2600,Test_acc: 0.2115\n",
      "Epoch: 032, Train_Loss: 1.5403,Test_Loss: 1.5679,Train_f1: 0.1693,Test_f1: 0.1273,Train_acc: 0.2700,Test_acc: 0.2140\n",
      "Epoch: 033, Train_Loss: 1.5287,Test_Loss: 1.5545,Train_f1: 0.1596,Test_f1: 0.1323,Train_acc: 0.2700,Test_acc: 0.2198\n",
      "Epoch: 034, Train_Loss: 1.5288,Test_Loss: 1.5545,Train_f1: 0.2081,Test_f1: 0.1389,Train_acc: 0.3200,Test_acc: 0.2285\n",
      "Epoch: 035, Train_Loss: 1.5298,Test_Loss: 1.5543,Train_f1: 0.2021,Test_f1: 0.1375,Train_acc: 0.3100,Test_acc: 0.2268\n",
      "Epoch: 036, Train_Loss: 1.5340,Test_Loss: 1.5557,Train_f1: 0.1397,Test_f1: 0.1171,Train_acc: 0.2500,Test_acc: 0.2091\n",
      "Epoch: 037, Train_Loss: 1.5330,Test_Loss: 1.5549,Train_f1: 0.1410,Test_f1: 0.1163,Train_acc: 0.2500,Test_acc: 0.2095\n",
      "Epoch: 038, Train_Loss: 1.5281,Test_Loss: 1.5519,Train_f1: 0.1410,Test_f1: 0.1180,Train_acc: 0.2500,Test_acc: 0.2099\n",
      "Epoch: 039, Train_Loss: 1.5251,Test_Loss: 1.5503,Train_f1: 0.1429,Test_f1: 0.1184,Train_acc: 0.2500,Test_acc: 0.2091\n",
      "Epoch: 040, Train_Loss: 1.5250,Test_Loss: 1.5531,Train_f1: 0.1548,Test_f1: 0.1302,Train_acc: 0.2600,Test_acc: 0.2165\n",
      "Epoch: 041, Train_Loss: 1.5359,Test_Loss: 1.5634,Train_f1: 0.1750,Test_f1: 0.1361,Train_acc: 0.2900,Test_acc: 0.2247\n",
      "Epoch: 042, Train_Loss: 1.5471,Test_Loss: 1.5715,Train_f1: 0.1742,Test_f1: 0.1355,Train_acc: 0.3000,Test_acc: 0.2268\n",
      "Epoch: 043, Train_Loss: 1.5546,Test_Loss: 1.5762,Train_f1: 0.1899,Test_f1: 0.1647,Train_acc: 0.3100,Test_acc: 0.2507\n",
      "Epoch: 044, Train_Loss: 1.5466,Test_Loss: 1.5686,Train_f1: 0.2095,Test_f1: 0.2151,Train_acc: 0.3200,Test_acc: 0.2948\n",
      "Epoch 00045: reducing learning rate of group 0 to 6.2500e-05.\n",
      "Epoch: 045, Train_Loss: 1.5289,Test_Loss: 1.5514,Train_f1: 0.2218,Test_f1: 0.2195,Train_acc: 0.3400,Test_acc: 0.2994\n",
      "Epoch: 046, Train_Loss: 1.5167,Test_Loss: 1.5404,Train_f1: 0.2362,Test_f1: 0.1746,Train_acc: 0.3300,Test_acc: 0.2503\n",
      "Epoch: 047, Train_Loss: 1.5149,Test_Loss: 1.5404,Train_f1: 0.1768,Test_f1: 0.1364,Train_acc: 0.2700,Test_acc: 0.2198\n",
      "Epoch: 048, Train_Loss: 1.5176,Test_Loss: 1.5432,Train_f1: 0.2001,Test_f1: 0.1251,Train_acc: 0.2800,Test_acc: 0.2132\n",
      "Epoch: 049, Train_Loss: 1.5222,Test_Loss: 1.5458,Train_f1: 0.1822,Test_f1: 0.1148,Train_acc: 0.2700,Test_acc: 0.2078\n",
      "Epoch: 050, Train_Loss: 1.5265,Test_Loss: 1.5518,Train_f1: 0.1471,Test_f1: 0.1089,Train_acc: 0.2500,Test_acc: 0.2049\n",
      "Epoch: 051, Train_Loss: 1.5274,Test_Loss: 1.5545,Train_f1: 0.1692,Test_f1: 0.1061,Train_acc: 0.2600,Test_acc: 0.2037\n",
      "Epoch: 052, Train_Loss: 1.5326,Test_Loss: 1.5602,Train_f1: 0.1502,Test_f1: 0.1078,Train_acc: 0.2500,Test_acc: 0.2045\n",
      "Epoch 00053: reducing learning rate of group 0 to 3.1250e-05.\n",
      "Epoch: 053, Train_Loss: 1.5291,Test_Loss: 1.5564,Train_f1: 0.1471,Test_f1: 0.1075,Train_acc: 0.2500,Test_acc: 0.2045\n",
      "Epoch: 054, Train_Loss: 1.5219,Test_Loss: 1.5497,Train_f1: 0.1597,Test_f1: 0.1136,Train_acc: 0.2600,Test_acc: 0.2074\n",
      "Epoch: 055, Train_Loss: 1.5167,Test_Loss: 1.5437,Train_f1: 0.1597,Test_f1: 0.1186,Train_acc: 0.2600,Test_acc: 0.2095\n",
      "Epoch: 056, Train_Loss: 1.5142,Test_Loss: 1.5418,Train_f1: 0.1571,Test_f1: 0.1238,Train_acc: 0.2600,Test_acc: 0.2128\n",
      "Epoch: 057, Train_Loss: 1.5073,Test_Loss: 1.5344,Train_f1: 0.1768,Test_f1: 0.1318,Train_acc: 0.2700,Test_acc: 0.2169\n",
      "Epoch: 058, Train_Loss: 1.5060,Test_Loss: 1.5325,Train_f1: 0.1724,Test_f1: 0.1475,Train_acc: 0.2700,Test_acc: 0.2276\n",
      "Epoch: 059, Train_Loss: 1.5080,Test_Loss: 1.5355,Train_f1: 0.1915,Test_f1: 0.1685,Train_acc: 0.2900,Test_acc: 0.2425\n",
      "Epoch: 060, Train_Loss: 1.5079,Test_Loss: 1.5352,Train_f1: 0.2110,Test_f1: 0.1805,Train_acc: 0.3000,Test_acc: 0.2528\n",
      "Epoch: 061, Train_Loss: 1.5075,Test_Loss: 1.5348,Train_f1: 0.2135,Test_f1: 0.1777,Train_acc: 0.3000,Test_acc: 0.2507\n",
      "Epoch: 062, Train_Loss: 1.5039,Test_Loss: 1.5309,Train_f1: 0.2135,Test_f1: 0.1851,Train_acc: 0.3000,Test_acc: 0.2548\n",
      "Epoch: 063, Train_Loss: 1.5033,Test_Loss: 1.5304,Train_f1: 0.2054,Test_f1: 0.1696,Train_acc: 0.2900,Test_acc: 0.2429\n",
      "Epoch: 064, Train_Loss: 1.5089,Test_Loss: 1.5361,Train_f1: 0.1768,Test_f1: 0.1445,Train_acc: 0.2700,Test_acc: 0.2243\n",
      "Epoch: 065, Train_Loss: 1.5078,Test_Loss: 1.5346,Train_f1: 0.1856,Test_f1: 0.1618,Train_acc: 0.2800,Test_acc: 0.2359\n",
      "Epoch: 066, Train_Loss: 1.5085,Test_Loss: 1.5349,Train_f1: 0.1962,Test_f1: 0.1700,Train_acc: 0.2900,Test_acc: 0.2425\n",
      "Epoch: 067, Train_Loss: 1.5083,Test_Loss: 1.5344,Train_f1: 0.1962,Test_f1: 0.1609,Train_acc: 0.2900,Test_acc: 0.2355\n",
      "Epoch: 068, Train_Loss: 1.5064,Test_Loss: 1.5316,Train_f1: 0.1768,Test_f1: 0.1492,Train_acc: 0.2700,Test_acc: 0.2276\n",
      "Epoch 00069: reducing learning rate of group 0 to 1.5625e-05.\n",
      "Epoch: 069, Train_Loss: 1.5051,Test_Loss: 1.5290,Train_f1: 0.1768,Test_f1: 0.1453,Train_acc: 0.2700,Test_acc: 0.2260\n",
      "Epoch: 070, Train_Loss: 1.5027,Test_Loss: 1.5262,Train_f1: 0.1768,Test_f1: 0.1559,Train_acc: 0.2700,Test_acc: 0.2334\n",
      "Epoch: 071, Train_Loss: 1.5030,Test_Loss: 1.5270,Train_f1: 0.1768,Test_f1: 0.1572,Train_acc: 0.2700,Test_acc: 0.2351\n",
      "Epoch: 072, Train_Loss: 1.5012,Test_Loss: 1.5252,Train_f1: 0.1768,Test_f1: 0.1725,Train_acc: 0.2700,Test_acc: 0.2474\n",
      "Epoch: 073, Train_Loss: 1.5033,Test_Loss: 1.5292,Train_f1: 0.1745,Test_f1: 0.1635,Train_acc: 0.2700,Test_acc: 0.2392\n",
      "Epoch: 074, Train_Loss: 1.5014,Test_Loss: 1.5266,Train_f1: 0.1858,Test_f1: 0.1760,Train_acc: 0.2800,Test_acc: 0.2487\n",
      "Epoch: 075, Train_Loss: 1.5013,Test_Loss: 1.5263,Train_f1: 0.1858,Test_f1: 0.1873,Train_acc: 0.2800,Test_acc: 0.2581\n",
      "Epoch: 076, Train_Loss: 1.5012,Test_Loss: 1.5256,Train_f1: 0.2015,Test_f1: 0.1864,Train_acc: 0.2900,Test_acc: 0.2577\n",
      "Epoch: 077, Train_Loss: 1.5009,Test_Loss: 1.5254,Train_f1: 0.1858,Test_f1: 0.1810,Train_acc: 0.2800,Test_acc: 0.2536\n",
      "Epoch: 078, Train_Loss: 1.5030,Test_Loss: 1.5289,Train_f1: 0.1858,Test_f1: 0.1685,Train_acc: 0.2800,Test_acc: 0.2421\n",
      "Epoch: 079, Train_Loss: 1.5020,Test_Loss: 1.5282,Train_f1: 0.1858,Test_f1: 0.1767,Train_acc: 0.2800,Test_acc: 0.2487\n",
      "Epoch: 080, Train_Loss: 1.5013,Test_Loss: 1.5272,Train_f1: 0.1858,Test_f1: 0.1863,Train_acc: 0.2800,Test_acc: 0.2577\n",
      "Epoch: 081, Train_Loss: 1.5031,Test_Loss: 1.5293,Train_f1: 0.1834,Test_f1: 0.1816,Train_acc: 0.2800,Test_acc: 0.2532\n",
      "Epoch: 082, Train_Loss: 1.5031,Test_Loss: 1.5295,Train_f1: 0.1938,Test_f1: 0.1812,Train_acc: 0.2900,Test_acc: 0.2528\n",
      "Epoch 00083: reducing learning rate of group 0 to 7.8125e-06.\n",
      "Epoch: 083, Train_Loss: 1.5045,Test_Loss: 1.5313,Train_f1: 0.1963,Test_f1: 0.1708,Train_acc: 0.2900,Test_acc: 0.2437\n",
      "Epoch: 084, Train_Loss: 1.5047,Test_Loss: 1.5323,Train_f1: 0.1963,Test_f1: 0.1584,Train_acc: 0.2900,Test_acc: 0.2355\n",
      "Epoch: 085, Train_Loss: 1.5029,Test_Loss: 1.5299,Train_f1: 0.1858,Test_f1: 0.1610,Train_acc: 0.2800,Test_acc: 0.2363\n",
      "Epoch: 086, Train_Loss: 1.4996,Test_Loss: 1.5251,Train_f1: 0.1768,Test_f1: 0.1632,Train_acc: 0.2700,Test_acc: 0.2388\n",
      "Epoch: 087, Train_Loss: 1.5004,Test_Loss: 1.5264,Train_f1: 0.1768,Test_f1: 0.1645,Train_acc: 0.2700,Test_acc: 0.2396\n",
      "Epoch: 088, Train_Loss: 1.5014,Test_Loss: 1.5271,Train_f1: 0.1845,Test_f1: 0.1754,Train_acc: 0.2700,Test_acc: 0.2470\n",
      "Epoch: 089, Train_Loss: 1.5009,Test_Loss: 1.5253,Train_f1: 0.1819,Test_f1: 0.1745,Train_acc: 0.2700,Test_acc: 0.2474\n",
      "Epoch: 090, Train_Loss: 1.5012,Test_Loss: 1.5277,Train_f1: 0.1745,Test_f1: 0.1676,Train_acc: 0.2700,Test_acc: 0.2408\n",
      "Epoch: 091, Train_Loss: 1.4993,Test_Loss: 1.5234,Train_f1: 0.1819,Test_f1: 0.1838,Train_acc: 0.2700,Test_acc: 0.2561\n",
      "Epoch: 092, Train_Loss: 1.4997,Test_Loss: 1.5237,Train_f1: 0.1795,Test_f1: 0.1853,Train_acc: 0.2700,Test_acc: 0.2573\n",
      "Epoch: 093, Train_Loss: 1.4999,Test_Loss: 1.5243,Train_f1: 0.1858,Test_f1: 0.1784,Train_acc: 0.2800,Test_acc: 0.2515\n",
      "Epoch: 094, Train_Loss: 1.4985,Test_Loss: 1.5210,Train_f1: 0.1991,Test_f1: 0.1910,Train_acc: 0.2800,Test_acc: 0.2635\n",
      "Epoch: 095, Train_Loss: 1.4995,Test_Loss: 1.5226,Train_f1: 0.1968,Test_f1: 0.1927,Train_acc: 0.2800,Test_acc: 0.2639\n",
      "Epoch: 096, Train_Loss: 1.5002,Test_Loss: 1.5251,Train_f1: 0.1858,Test_f1: 0.1740,Train_acc: 0.2800,Test_acc: 0.2482\n",
      "Epoch: 097, Train_Loss: 1.5000,Test_Loss: 1.5236,Train_f1: 0.1968,Test_f1: 0.1852,Train_acc: 0.2800,Test_acc: 0.2569\n",
      "Epoch: 098, Train_Loss: 1.4997,Test_Loss: 1.5243,Train_f1: 0.1819,Test_f1: 0.1845,Train_acc: 0.2700,Test_acc: 0.2569\n",
      "Epoch: 099, Train_Loss: 1.5003,Test_Loss: 1.5243,Train_f1: 0.1845,Test_f1: 0.1851,Train_acc: 0.2700,Test_acc: 0.2573\n",
      "Epoch 00100: reducing learning rate of group 0 to 3.9063e-06.\n",
      "Epoch: 100, Train_Loss: 1.5020,Test_Loss: 1.5265,Train_f1: 0.1845,Test_f1: 0.1855,Train_acc: 0.2700,Test_acc: 0.2561\n",
      "Epoch: 101, Train_Loss: 1.4996,Test_Loss: 1.5239,Train_f1: 0.2190,Test_f1: 0.1934,Train_acc: 0.2900,Test_acc: 0.2639\n",
      "Epoch: 102, Train_Loss: 1.5004,Test_Loss: 1.5253,Train_f1: 0.2204,Test_f1: 0.1867,Train_acc: 0.2900,Test_acc: 0.2577\n",
      "Epoch: 103, Train_Loss: 1.4984,Test_Loss: 1.5236,Train_f1: 0.1928,Test_f1: 0.1851,Train_acc: 0.2700,Test_acc: 0.2565\n",
      "Epoch: 104, Train_Loss: 1.4975,Test_Loss: 1.5208,Train_f1: 0.2086,Test_f1: 0.1909,Train_acc: 0.2800,Test_acc: 0.2619\n",
      "Epoch: 105, Train_Loss: 1.4978,Test_Loss: 1.5224,Train_f1: 0.1928,Test_f1: 0.1880,Train_acc: 0.2700,Test_acc: 0.2590\n",
      "Epoch: 106, Train_Loss: 1.4982,Test_Loss: 1.5235,Train_f1: 0.1928,Test_f1: 0.1845,Train_acc: 0.2700,Test_acc: 0.2557\n",
      "Epoch: 107, Train_Loss: 1.5004,Test_Loss: 1.5265,Train_f1: 0.1768,Test_f1: 0.1754,Train_acc: 0.2700,Test_acc: 0.2478\n",
      "Epoch: 108, Train_Loss: 1.5007,Test_Loss: 1.5261,Train_f1: 0.1845,Test_f1: 0.1761,Train_acc: 0.2700,Test_acc: 0.2487\n",
      "Epoch: 109, Train_Loss: 1.4999,Test_Loss: 1.5248,Train_f1: 0.2045,Test_f1: 0.1732,Train_acc: 0.2800,Test_acc: 0.2458\n",
      "Epoch 00110: reducing learning rate of group 0 to 1.9531e-06.\n",
      "Epoch: 110, Train_Loss: 1.5005,Test_Loss: 1.5259,Train_f1: 0.1768,Test_f1: 0.1621,Train_acc: 0.2700,Test_acc: 0.2375\n",
      "Epoch: 111, Train_Loss: 1.5004,Test_Loss: 1.5259,Train_f1: 0.1768,Test_f1: 0.1632,Train_acc: 0.2700,Test_acc: 0.2384\n",
      "Epoch: 112, Train_Loss: 1.4985,Test_Loss: 1.5233,Train_f1: 0.1973,Test_f1: 0.1679,Train_acc: 0.2800,Test_acc: 0.2421\n",
      "Epoch: 113, Train_Loss: 1.5002,Test_Loss: 1.5240,Train_f1: 0.2091,Test_f1: 0.1694,Train_acc: 0.2900,Test_acc: 0.2429\n",
      "Epoch: 114, Train_Loss: 1.4995,Test_Loss: 1.5229,Train_f1: 0.2091,Test_f1: 0.1684,Train_acc: 0.2900,Test_acc: 0.2425\n",
      "Epoch: 115, Train_Loss: 1.5000,Test_Loss: 1.5224,Train_f1: 0.1745,Test_f1: 0.1738,Train_acc: 0.2700,Test_acc: 0.2478\n",
      "Epoch: 116, Train_Loss: 1.4993,Test_Loss: 1.5220,Train_f1: 0.2091,Test_f1: 0.1743,Train_acc: 0.2900,Test_acc: 0.2482\n",
      "Epoch: 117, Train_Loss: 1.5002,Test_Loss: 1.5245,Train_f1: 0.2091,Test_f1: 0.1642,Train_acc: 0.2900,Test_acc: 0.2388\n",
      "Epoch 00118: reducing learning rate of group 0 to 9.7656e-07.\n",
      "Epoch: 118, Train_Loss: 1.5011,Test_Loss: 1.5272,Train_f1: 0.1768,Test_f1: 0.1547,Train_acc: 0.2700,Test_acc: 0.2318\n",
      "Epoch: 119, Train_Loss: 1.4998,Test_Loss: 1.5264,Train_f1: 0.1768,Test_f1: 0.1511,Train_acc: 0.2700,Test_acc: 0.2293\n",
      "Epoch: 120, Train_Loss: 1.5010,Test_Loss: 1.5280,Train_f1: 0.1768,Test_f1: 0.1501,Train_acc: 0.2700,Test_acc: 0.2289\n",
      "Epoch: 121, Train_Loss: 1.5005,Test_Loss: 1.5270,Train_f1: 0.1768,Test_f1: 0.1547,Train_acc: 0.2700,Test_acc: 0.2318\n",
      "Epoch: 122, Train_Loss: 1.4992,Test_Loss: 1.5253,Train_f1: 0.1768,Test_f1: 0.1605,Train_acc: 0.2700,Test_acc: 0.2363\n",
      "Epoch: 123, Train_Loss: 1.4981,Test_Loss: 1.5232,Train_f1: 0.1768,Test_f1: 0.1683,Train_acc: 0.2700,Test_acc: 0.2421\n",
      "Epoch: 124, Train_Loss: 1.4965,Test_Loss: 1.5205,Train_f1: 0.2045,Test_f1: 0.1856,Train_acc: 0.2800,Test_acc: 0.2569\n",
      "Epoch: 125, Train_Loss: 1.4954,Test_Loss: 1.5198,Train_f1: 0.1928,Test_f1: 0.1887,Train_acc: 0.2700,Test_acc: 0.2598\n",
      "Epoch: 126, Train_Loss: 1.4988,Test_Loss: 1.5245,Train_f1: 0.2045,Test_f1: 0.1801,Train_acc: 0.2800,Test_acc: 0.2515\n",
      "Epoch: 127, Train_Loss: 1.4974,Test_Loss: 1.5225,Train_f1: 0.1928,Test_f1: 0.1804,Train_acc: 0.2700,Test_acc: 0.2528\n",
      "Epoch: 128, Train_Loss: 1.4985,Test_Loss: 1.5239,Train_f1: 0.2091,Test_f1: 0.1775,Train_acc: 0.2900,Test_acc: 0.2499\n",
      "Epoch: 129, Train_Loss: 1.4962,Test_Loss: 1.5205,Train_f1: 0.1928,Test_f1: 0.1847,Train_acc: 0.2700,Test_acc: 0.2565\n",
      "Epoch: 130, Train_Loss: 1.4951,Test_Loss: 1.5178,Train_f1: 0.2086,Test_f1: 0.1911,Train_acc: 0.2800,Test_acc: 0.2631\n",
      "Epoch: 131, Train_Loss: 1.4979,Test_Loss: 1.5222,Train_f1: 0.2045,Test_f1: 0.1789,Train_acc: 0.2800,Test_acc: 0.2515\n",
      "Epoch: 132, Train_Loss: 1.4998,Test_Loss: 1.5247,Train_f1: 0.1845,Test_f1: 0.1807,Train_acc: 0.2700,Test_acc: 0.2520\n",
      "Epoch: 133, Train_Loss: 1.4997,Test_Loss: 1.5248,Train_f1: 0.1845,Test_f1: 0.1790,Train_acc: 0.2700,Test_acc: 0.2507\n",
      "Epoch: 134, Train_Loss: 1.4996,Test_Loss: 1.5237,Train_f1: 0.2017,Test_f1: 0.1839,Train_acc: 0.2800,Test_acc: 0.2557\n",
      "Epoch: 135, Train_Loss: 1.4990,Test_Loss: 1.5231,Train_f1: 0.2017,Test_f1: 0.1872,Train_acc: 0.2800,Test_acc: 0.2590\n",
      "Epoch 00136: reducing learning rate of group 0 to 4.8828e-07.\n",
      "Epoch: 136, Train_Loss: 1.4983,Test_Loss: 1.5230,Train_f1: 0.2045,Test_f1: 0.1818,Train_acc: 0.2800,Test_acc: 0.2536\n",
      "Epoch: 137, Train_Loss: 1.4994,Test_Loss: 1.5256,Train_f1: 0.1845,Test_f1: 0.1833,Train_acc: 0.2700,Test_acc: 0.2540\n",
      "Epoch: 138, Train_Loss: 1.5021,Test_Loss: 1.5290,Train_f1: 0.1768,Test_f1: 0.1535,Train_acc: 0.2700,Test_acc: 0.2313\n",
      "Epoch: 139, Train_Loss: 1.5004,Test_Loss: 1.5269,Train_f1: 0.1768,Test_f1: 0.1631,Train_acc: 0.2700,Test_acc: 0.2384\n",
      "Epoch: 140, Train_Loss: 1.5000,Test_Loss: 1.5265,Train_f1: 0.1768,Test_f1: 0.1681,Train_acc: 0.2700,Test_acc: 0.2425\n",
      "Epoch: 141, Train_Loss: 1.4978,Test_Loss: 1.5225,Train_f1: 0.1730,Test_f1: 0.1784,Train_acc: 0.2600,Test_acc: 0.2507\n",
      "Epoch: 142, Train_Loss: 1.4991,Test_Loss: 1.5237,Train_f1: 0.2045,Test_f1: 0.1796,Train_acc: 0.2800,Test_acc: 0.2515\n",
      "Epoch: 143, Train_Loss: 1.5010,Test_Loss: 1.5276,Train_f1: 0.1768,Test_f1: 0.1625,Train_acc: 0.2700,Test_acc: 0.2384\n",
      "Epoch 00144: reducing learning rate of group 0 to 2.4414e-07.\n",
      "Epoch: 144, Train_Loss: 1.4994,Test_Loss: 1.5256,Train_f1: 0.1768,Test_f1: 0.1687,Train_acc: 0.2700,Test_acc: 0.2433\n",
      "Epoch: 145, Train_Loss: 1.5011,Test_Loss: 1.5277,Train_f1: 0.1768,Test_f1: 0.1655,Train_acc: 0.2700,Test_acc: 0.2400\n",
      "Epoch: 146, Train_Loss: 1.4981,Test_Loss: 1.5230,Train_f1: 0.1973,Test_f1: 0.1730,Train_acc: 0.2800,Test_acc: 0.2466\n",
      "Epoch: 147, Train_Loss: 1.5001,Test_Loss: 1.5254,Train_f1: 0.1973,Test_f1: 0.1621,Train_acc: 0.2800,Test_acc: 0.2379\n",
      "Epoch: 148, Train_Loss: 1.4995,Test_Loss: 1.5235,Train_f1: 0.2091,Test_f1: 0.1662,Train_acc: 0.2900,Test_acc: 0.2404\n",
      "Epoch: 149, Train_Loss: 1.4995,Test_Loss: 1.5255,Train_f1: 0.1768,Test_f1: 0.1631,Train_acc: 0.2700,Test_acc: 0.2384\n",
      "Epoch: 150, Train_Loss: 1.5003,Test_Loss: 1.5259,Train_f1: 0.2091,Test_f1: 0.1647,Train_acc: 0.2900,Test_acc: 0.2396\n",
      "Epoch: 151, Train_Loss: 1.5001,Test_Loss: 1.5256,Train_f1: 0.2091,Test_f1: 0.1683,Train_acc: 0.2900,Test_acc: 0.2421\n",
      "Epoch 00152: reducing learning rate of group 0 to 1.2207e-07.\n",
      "Epoch: 152, Train_Loss: 1.5006,Test_Loss: 1.5256,Train_f1: 0.1883,Test_f1: 0.1671,Train_acc: 0.2800,Test_acc: 0.2408\n",
      "Epoch: 153, Train_Loss: 1.5002,Test_Loss: 1.5256,Train_f1: 0.1883,Test_f1: 0.1713,Train_acc: 0.2800,Test_acc: 0.2445\n",
      "Epoch: 154, Train_Loss: 1.5011,Test_Loss: 1.5279,Train_f1: 0.1768,Test_f1: 0.1570,Train_acc: 0.2700,Test_acc: 0.2330\n",
      "Epoch: 155, Train_Loss: 1.4980,Test_Loss: 1.5230,Train_f1: 0.1768,Test_f1: 0.1722,Train_acc: 0.2700,Test_acc: 0.2454\n",
      "Epoch: 156, Train_Loss: 1.4996,Test_Loss: 1.5246,Train_f1: 0.1883,Test_f1: 0.1701,Train_acc: 0.2800,Test_acc: 0.2437\n",
      "Epoch: 157, Train_Loss: 1.4985,Test_Loss: 1.5235,Train_f1: 0.1768,Test_f1: 0.1761,Train_acc: 0.2700,Test_acc: 0.2487\n",
      "Epoch: 158, Train_Loss: 1.4982,Test_Loss: 1.5240,Train_f1: 0.1768,Test_f1: 0.1643,Train_acc: 0.2700,Test_acc: 0.2396\n",
      "Epoch: 159, Train_Loss: 1.4974,Test_Loss: 1.5226,Train_f1: 0.1768,Test_f1: 0.1695,Train_acc: 0.2700,Test_acc: 0.2433\n",
      "Epoch 00160: reducing learning rate of group 0 to 6.1035e-08.\n",
      "Epoch: 160, Train_Loss: 1.4966,Test_Loss: 1.5214,Train_f1: 0.1768,Test_f1: 0.1774,Train_acc: 0.2700,Test_acc: 0.2503\n",
      "Epoch: 161, Train_Loss: 1.4991,Test_Loss: 1.5247,Train_f1: 0.1973,Test_f1: 0.1724,Train_acc: 0.2800,Test_acc: 0.2449\n",
      "Epoch: 162, Train_Loss: 1.4977,Test_Loss: 1.5221,Train_f1: 0.2045,Test_f1: 0.1812,Train_acc: 0.2800,Test_acc: 0.2536\n",
      "Epoch: 163, Train_Loss: 1.4973,Test_Loss: 1.5225,Train_f1: 0.2045,Test_f1: 0.1821,Train_acc: 0.2800,Test_acc: 0.2540\n",
      "Epoch: 164, Train_Loss: 1.4963,Test_Loss: 1.5214,Train_f1: 0.1973,Test_f1: 0.1812,Train_acc: 0.2800,Test_acc: 0.2536\n",
      "Epoch: 165, Train_Loss: 1.4988,Test_Loss: 1.5246,Train_f1: 0.1768,Test_f1: 0.1740,Train_acc: 0.2700,Test_acc: 0.2474\n",
      "Epoch: 166, Train_Loss: 1.4997,Test_Loss: 1.5261,Train_f1: 0.1768,Test_f1: 0.1702,Train_acc: 0.2700,Test_acc: 0.2441\n",
      "Epoch: 167, Train_Loss: 1.5002,Test_Loss: 1.5269,Train_f1: 0.1768,Test_f1: 0.1670,Train_acc: 0.2700,Test_acc: 0.2416\n",
      "Epoch 00168: reducing learning rate of group 0 to 3.0518e-08.\n",
      "Epoch: 168, Train_Loss: 1.4984,Test_Loss: 1.5238,Train_f1: 0.1973,Test_f1: 0.1697,Train_acc: 0.2800,Test_acc: 0.2433\n",
      "Epoch: 169, Train_Loss: 1.4989,Test_Loss: 1.5244,Train_f1: 0.2045,Test_f1: 0.1779,Train_acc: 0.2800,Test_acc: 0.2499\n",
      "Epoch: 170, Train_Loss: 1.4994,Test_Loss: 1.5252,Train_f1: 0.2045,Test_f1: 0.1787,Train_acc: 0.2800,Test_acc: 0.2503\n",
      "Epoch: 171, Train_Loss: 1.5011,Test_Loss: 1.5277,Train_f1: 0.1845,Test_f1: 0.1757,Train_acc: 0.2700,Test_acc: 0.2478\n",
      "Epoch: 172, Train_Loss: 1.4997,Test_Loss: 1.5263,Train_f1: 0.1768,Test_f1: 0.1715,Train_acc: 0.2700,Test_acc: 0.2445\n",
      "Epoch: 173, Train_Loss: 1.5005,Test_Loss: 1.5272,Train_f1: 0.1768,Test_f1: 0.1646,Train_acc: 0.2700,Test_acc: 0.2396\n",
      "Epoch: 174, Train_Loss: 1.4982,Test_Loss: 1.5240,Train_f1: 0.1973,Test_f1: 0.1760,Train_acc: 0.2800,Test_acc: 0.2491\n",
      "Epoch: 175, Train_Loss: 1.4989,Test_Loss: 1.5245,Train_f1: 0.2045,Test_f1: 0.1772,Train_acc: 0.2800,Test_acc: 0.2499\n",
      "Epoch 00176: reducing learning rate of group 0 to 1.5259e-08.\n",
      "Epoch: 176, Train_Loss: 1.4989,Test_Loss: 1.5244,Train_f1: 0.1973,Test_f1: 0.1696,Train_acc: 0.2800,Test_acc: 0.2437\n",
      "Epoch: 177, Train_Loss: 1.4999,Test_Loss: 1.5260,Train_f1: 0.2091,Test_f1: 0.1721,Train_acc: 0.2900,Test_acc: 0.2458\n",
      "Epoch: 178, Train_Loss: 1.5001,Test_Loss: 1.5263,Train_f1: 0.1973,Test_f1: 0.1678,Train_acc: 0.2800,Test_acc: 0.2425\n",
      "Epoch: 179, Train_Loss: 1.4992,Test_Loss: 1.5251,Train_f1: 0.1768,Test_f1: 0.1652,Train_acc: 0.2700,Test_acc: 0.2404\n",
      "Epoch: 180, Train_Loss: 1.4986,Test_Loss: 1.5238,Train_f1: 0.1768,Test_f1: 0.1718,Train_acc: 0.2700,Test_acc: 0.2454\n",
      "Epoch: 181, Train_Loss: 1.4970,Test_Loss: 1.5218,Train_f1: 0.1768,Test_f1: 0.1761,Train_acc: 0.2700,Test_acc: 0.2487\n",
      "Epoch: 182, Train_Loss: 1.4983,Test_Loss: 1.5239,Train_f1: 0.1768,Test_f1: 0.1734,Train_acc: 0.2700,Test_acc: 0.2470\n",
      "Epoch: 183, Train_Loss: 1.4997,Test_Loss: 1.5259,Train_f1: 0.1768,Test_f1: 0.1686,Train_acc: 0.2700,Test_acc: 0.2429\n",
      "Epoch: 184, Train_Loss: 1.5004,Test_Loss: 1.5264,Train_f1: 0.1973,Test_f1: 0.1668,Train_acc: 0.2800,Test_acc: 0.2416\n",
      "Epoch: 185, Train_Loss: 1.4995,Test_Loss: 1.5255,Train_f1: 0.1973,Test_f1: 0.1646,Train_acc: 0.2800,Test_acc: 0.2404\n",
      "Epoch: 186, Train_Loss: 1.5002,Test_Loss: 1.5259,Train_f1: 0.2045,Test_f1: 0.1754,Train_acc: 0.2800,Test_acc: 0.2478\n",
      "Epoch: 187, Train_Loss: 1.4999,Test_Loss: 1.5255,Train_f1: 0.2045,Test_f1: 0.1750,Train_acc: 0.2800,Test_acc: 0.2478\n",
      "Epoch: 188, Train_Loss: 1.4995,Test_Loss: 1.5245,Train_f1: 0.2045,Test_f1: 0.1777,Train_acc: 0.2800,Test_acc: 0.2503\n",
      "Epoch: 189, Train_Loss: 1.4987,Test_Loss: 1.5242,Train_f1: 0.1973,Test_f1: 0.1737,Train_acc: 0.2800,Test_acc: 0.2466\n",
      "Epoch: 190, Train_Loss: 1.4993,Test_Loss: 1.5244,Train_f1: 0.1883,Test_f1: 0.1696,Train_acc: 0.2800,Test_acc: 0.2433\n",
      "Epoch: 191, Train_Loss: 1.5012,Test_Loss: 1.5261,Train_f1: 0.1883,Test_f1: 0.1674,Train_acc: 0.2800,Test_acc: 0.2408\n",
      "Epoch: 192, Train_Loss: 1.5019,Test_Loss: 1.5287,Train_f1: 0.1768,Test_f1: 0.1538,Train_acc: 0.2700,Test_acc: 0.2309\n",
      "Epoch: 193, Train_Loss: 1.5028,Test_Loss: 1.5297,Train_f1: 0.1768,Test_f1: 0.1547,Train_acc: 0.2700,Test_acc: 0.2318\n",
      "Epoch: 194, Train_Loss: 1.5023,Test_Loss: 1.5283,Train_f1: 0.1883,Test_f1: 0.1546,Train_acc: 0.2800,Test_acc: 0.2318\n",
      "Epoch: 195, Train_Loss: 1.5014,Test_Loss: 1.5277,Train_f1: 0.1768,Test_f1: 0.1575,Train_acc: 0.2700,Test_acc: 0.2338\n",
      "Epoch: 196, Train_Loss: 1.5023,Test_Loss: 1.5287,Train_f1: 0.1883,Test_f1: 0.1651,Train_acc: 0.2800,Test_acc: 0.2392\n",
      "Epoch: 197, Train_Loss: 1.4998,Test_Loss: 1.5250,Train_f1: 0.1845,Test_f1: 0.1721,Train_acc: 0.2700,Test_acc: 0.2449\n",
      "Epoch: 198, Train_Loss: 1.4992,Test_Loss: 1.5232,Train_f1: 0.2045,Test_f1: 0.1784,Train_acc: 0.2800,Test_acc: 0.2511\n",
      "Epoch: 199, Train_Loss: 1.4976,Test_Loss: 1.5211,Train_f1: 0.2045,Test_f1: 0.1849,Train_acc: 0.2800,Test_acc: 0.2569\n",
      "Epoch: 200, Train_Loss: 1.4981,Test_Loss: 1.5221,Train_f1: 0.2045,Test_f1: 0.1836,Train_acc: 0.2800,Test_acc: 0.2557\n",
      "Epoch: 201, Train_Loss: 1.4978,Test_Loss: 1.5237,Train_f1: 0.1973,Test_f1: 0.1797,Train_acc: 0.2800,Test_acc: 0.2520\n",
      "Epoch: 202, Train_Loss: 1.4969,Test_Loss: 1.5215,Train_f1: 0.2045,Test_f1: 0.1852,Train_acc: 0.2800,Test_acc: 0.2573\n",
      "Epoch: 203, Train_Loss: 1.4973,Test_Loss: 1.5218,Train_f1: 0.2045,Test_f1: 0.1854,Train_acc: 0.2800,Test_acc: 0.2577\n",
      "Epoch: 204, Train_Loss: 1.4976,Test_Loss: 1.5223,Train_f1: 0.2045,Test_f1: 0.1841,Train_acc: 0.2800,Test_acc: 0.2561\n",
      "Epoch: 205, Train_Loss: 1.4983,Test_Loss: 1.5231,Train_f1: 0.2045,Test_f1: 0.1817,Train_acc: 0.2800,Test_acc: 0.2536\n",
      "Epoch: 206, Train_Loss: 1.4986,Test_Loss: 1.5239,Train_f1: 0.2045,Test_f1: 0.1816,Train_acc: 0.2800,Test_acc: 0.2532\n",
      "Epoch: 207, Train_Loss: 1.4996,Test_Loss: 1.5251,Train_f1: 0.2045,Test_f1: 0.1790,Train_acc: 0.2800,Test_acc: 0.2507\n",
      "Epoch: 208, Train_Loss: 1.4986,Test_Loss: 1.5238,Train_f1: 0.2045,Test_f1: 0.1832,Train_acc: 0.2800,Test_acc: 0.2548\n",
      "Epoch: 209, Train_Loss: 1.4990,Test_Loss: 1.5251,Train_f1: 0.2045,Test_f1: 0.1824,Train_acc: 0.2800,Test_acc: 0.2532\n",
      "Epoch: 210, Train_Loss: 1.4991,Test_Loss: 1.5251,Train_f1: 0.1730,Test_f1: 0.1809,Train_acc: 0.2600,Test_acc: 0.2520\n",
      "Epoch: 211, Train_Loss: 1.5006,Test_Loss: 1.5273,Train_f1: 0.1768,Test_f1: 0.1734,Train_acc: 0.2700,Test_acc: 0.2462\n",
      "Epoch: 212, Train_Loss: 1.4982,Test_Loss: 1.5242,Train_f1: 0.1845,Test_f1: 0.1831,Train_acc: 0.2700,Test_acc: 0.2540\n",
      "Epoch: 213, Train_Loss: 1.4976,Test_Loss: 1.5231,Train_f1: 0.1973,Test_f1: 0.1808,Train_acc: 0.2800,Test_acc: 0.2528\n",
      "Epoch: 214, Train_Loss: 1.4990,Test_Loss: 1.5251,Train_f1: 0.1768,Test_f1: 0.1765,Train_acc: 0.2700,Test_acc: 0.2487\n",
      "Epoch: 215, Train_Loss: 1.4991,Test_Loss: 1.5246,Train_f1: 0.1845,Test_f1: 0.1813,Train_acc: 0.2700,Test_acc: 0.2532\n",
      "Epoch: 216, Train_Loss: 1.5000,Test_Loss: 1.5253,Train_f1: 0.1845,Test_f1: 0.1794,Train_acc: 0.2700,Test_acc: 0.2511\n",
      "Epoch: 217, Train_Loss: 1.5008,Test_Loss: 1.5273,Train_f1: 0.1883,Test_f1: 0.1716,Train_acc: 0.2800,Test_acc: 0.2449\n",
      "Epoch: 218, Train_Loss: 1.5004,Test_Loss: 1.5264,Train_f1: 0.2045,Test_f1: 0.1750,Train_acc: 0.2800,Test_acc: 0.2474\n",
      "Epoch: 219, Train_Loss: 1.4994,Test_Loss: 1.5255,Train_f1: 0.2091,Test_f1: 0.1783,Train_acc: 0.2900,Test_acc: 0.2499\n",
      "Epoch: 220, Train_Loss: 1.5043,Test_Loss: 1.5319,Train_f1: 0.1768,Test_f1: 0.1369,Train_acc: 0.2700,Test_acc: 0.2194\n",
      "Epoch: 221, Train_Loss: 1.5027,Test_Loss: 1.5298,Train_f1: 0.1768,Test_f1: 0.1461,Train_acc: 0.2700,Test_acc: 0.2260\n",
      "Epoch: 222, Train_Loss: 1.5032,Test_Loss: 1.5304,Train_f1: 0.1768,Test_f1: 0.1515,Train_acc: 0.2700,Test_acc: 0.2293\n",
      "Epoch: 223, Train_Loss: 1.5036,Test_Loss: 1.5310,Train_f1: 0.1768,Test_f1: 0.1457,Train_acc: 0.2700,Test_acc: 0.2256\n",
      "Epoch: 224, Train_Loss: 1.4997,Test_Loss: 1.5259,Train_f1: 0.1768,Test_f1: 0.1672,Train_acc: 0.2700,Test_acc: 0.2416\n",
      "Epoch: 225, Train_Loss: 1.4981,Test_Loss: 1.5226,Train_f1: 0.1845,Test_f1: 0.1782,Train_acc: 0.2700,Test_acc: 0.2503\n",
      "Epoch: 226, Train_Loss: 1.4964,Test_Loss: 1.5205,Train_f1: 0.1730,Test_f1: 0.1870,Train_acc: 0.2600,Test_acc: 0.2581\n",
      "Epoch: 227, Train_Loss: 1.4985,Test_Loss: 1.5227,Train_f1: 0.2045,Test_f1: 0.1852,Train_acc: 0.2800,Test_acc: 0.2569\n",
      "Epoch: 228, Train_Loss: 1.4986,Test_Loss: 1.5221,Train_f1: 0.2045,Test_f1: 0.1855,Train_acc: 0.2800,Test_acc: 0.2573\n",
      "Epoch: 229, Train_Loss: 1.5002,Test_Loss: 1.5258,Train_f1: 0.1845,Test_f1: 0.1741,Train_acc: 0.2700,Test_acc: 0.2466\n",
      "Epoch: 230, Train_Loss: 1.4985,Test_Loss: 1.5224,Train_f1: 0.2045,Test_f1: 0.1850,Train_acc: 0.2800,Test_acc: 0.2569\n",
      "Epoch: 231, Train_Loss: 1.4966,Test_Loss: 1.5205,Train_f1: 0.2045,Test_f1: 0.1884,Train_acc: 0.2800,Test_acc: 0.2598\n",
      "Epoch: 232, Train_Loss: 1.4987,Test_Loss: 1.5224,Train_f1: 0.2045,Test_f1: 0.1865,Train_acc: 0.2800,Test_acc: 0.2577\n",
      "Epoch: 233, Train_Loss: 1.4982,Test_Loss: 1.5226,Train_f1: 0.2045,Test_f1: 0.1822,Train_acc: 0.2800,Test_acc: 0.2540\n",
      "Epoch: 234, Train_Loss: 1.4969,Test_Loss: 1.5220,Train_f1: 0.2045,Test_f1: 0.1830,Train_acc: 0.2800,Test_acc: 0.2548\n",
      "Epoch: 235, Train_Loss: 1.4990,Test_Loss: 1.5251,Train_f1: 0.1973,Test_f1: 0.1720,Train_acc: 0.2800,Test_acc: 0.2454\n",
      "Epoch: 236, Train_Loss: 1.4989,Test_Loss: 1.5252,Train_f1: 0.1768,Test_f1: 0.1654,Train_acc: 0.2700,Test_acc: 0.2404\n",
      "Epoch: 237, Train_Loss: 1.4967,Test_Loss: 1.5221,Train_f1: 0.1973,Test_f1: 0.1755,Train_acc: 0.2800,Test_acc: 0.2482\n",
      "Epoch: 238, Train_Loss: 1.5018,Test_Loss: 1.5288,Train_f1: 0.1768,Test_f1: 0.1510,Train_acc: 0.2700,Test_acc: 0.2293\n",
      "Epoch: 239, Train_Loss: 1.5022,Test_Loss: 1.5292,Train_f1: 0.1768,Test_f1: 0.1565,Train_acc: 0.2700,Test_acc: 0.2330\n",
      "Epoch: 240, Train_Loss: 1.5025,Test_Loss: 1.5294,Train_f1: 0.1768,Test_f1: 0.1639,Train_acc: 0.2700,Test_acc: 0.2384\n",
      "Epoch: 241, Train_Loss: 1.5009,Test_Loss: 1.5276,Train_f1: 0.1768,Test_f1: 0.1620,Train_acc: 0.2700,Test_acc: 0.2375\n",
      "Epoch: 242, Train_Loss: 1.4999,Test_Loss: 1.5264,Train_f1: 0.1768,Test_f1: 0.1701,Train_acc: 0.2700,Test_acc: 0.2437\n",
      "Epoch: 243, Train_Loss: 1.5013,Test_Loss: 1.5277,Train_f1: 0.1768,Test_f1: 0.1625,Train_acc: 0.2700,Test_acc: 0.2379\n",
      "Epoch: 244, Train_Loss: 1.4996,Test_Loss: 1.5256,Train_f1: 0.1973,Test_f1: 0.1730,Train_acc: 0.2800,Test_acc: 0.2462\n",
      "Epoch: 245, Train_Loss: 1.4990,Test_Loss: 1.5231,Train_f1: 0.1845,Test_f1: 0.1783,Train_acc: 0.2700,Test_acc: 0.2507\n",
      "Epoch: 246, Train_Loss: 1.4987,Test_Loss: 1.5235,Train_f1: 0.2091,Test_f1: 0.1773,Train_acc: 0.2900,Test_acc: 0.2495\n",
      "Epoch: 247, Train_Loss: 1.4999,Test_Loss: 1.5238,Train_f1: 0.2091,Test_f1: 0.1645,Train_acc: 0.2900,Test_acc: 0.2396\n",
      "Epoch: 248, Train_Loss: 1.4982,Test_Loss: 1.5215,Train_f1: 0.2045,Test_f1: 0.1844,Train_acc: 0.2800,Test_acc: 0.2569\n",
      "Epoch: 249, Train_Loss: 1.4985,Test_Loss: 1.5221,Train_f1: 0.2045,Test_f1: 0.1799,Train_acc: 0.2800,Test_acc: 0.2524\n",
      "Epoch: 250, Train_Loss: 1.4969,Test_Loss: 1.5213,Train_f1: 0.2045,Test_f1: 0.1873,Train_acc: 0.2800,Test_acc: 0.2590\n",
      "Epoch: 251, Train_Loss: 1.4990,Test_Loss: 1.5244,Train_f1: 0.2091,Test_f1: 0.1750,Train_acc: 0.2900,Test_acc: 0.2478\n",
      "Epoch: 252, Train_Loss: 1.4983,Test_Loss: 1.5238,Train_f1: 0.1973,Test_f1: 0.1771,Train_acc: 0.2800,Test_acc: 0.2499\n",
      "Epoch: 253, Train_Loss: 1.4973,Test_Loss: 1.5229,Train_f1: 0.1973,Test_f1: 0.1793,Train_acc: 0.2800,Test_acc: 0.2520\n",
      "Epoch: 254, Train_Loss: 1.4960,Test_Loss: 1.5198,Train_f1: 0.2045,Test_f1: 0.1885,Train_acc: 0.2800,Test_acc: 0.2598\n",
      "Epoch: 255, Train_Loss: 1.4961,Test_Loss: 1.5200,Train_f1: 0.1928,Test_f1: 0.1869,Train_acc: 0.2700,Test_acc: 0.2586\n",
      "Epoch: 256, Train_Loss: 1.4979,Test_Loss: 1.5231,Train_f1: 0.2045,Test_f1: 0.1803,Train_acc: 0.2800,Test_acc: 0.2524\n",
      "Epoch: 257, Train_Loss: 1.4984,Test_Loss: 1.5223,Train_f1: 0.2045,Test_f1: 0.1794,Train_acc: 0.2800,Test_acc: 0.2520\n",
      "Epoch: 258, Train_Loss: 1.4965,Test_Loss: 1.5198,Train_f1: 0.2204,Test_f1: 0.1922,Train_acc: 0.2900,Test_acc: 0.2639\n",
      "Epoch: 259, Train_Loss: 1.4974,Test_Loss: 1.5225,Train_f1: 0.1730,Test_f1: 0.1814,Train_acc: 0.2600,Test_acc: 0.2536\n",
      "Epoch: 260, Train_Loss: 1.4976,Test_Loss: 1.5233,Train_f1: 0.1730,Test_f1: 0.1812,Train_acc: 0.2600,Test_acc: 0.2528\n",
      "Epoch: 261, Train_Loss: 1.4956,Test_Loss: 1.5205,Train_f1: 0.1928,Test_f1: 0.1892,Train_acc: 0.2700,Test_acc: 0.2598\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m train()\n\u001b[1;32m      3\u001b[0m train_macro_f1,train_acc, train_loss \u001b[39m=\u001b[39m test(train_loader)\n\u001b[0;32m----> 4\u001b[0m test_macro_f1,test_acc, test_loss \u001b[39m=\u001b[39m test(test_loader, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      5\u001b[0m scheduler\u001b[39m.\u001b[39mstep(train_loss)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[1;32m      7\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train_Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m02.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m,Test_Loss: \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m:\u001b[39;00m\u001b[39m02.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m,Train_f1: \u001b[39m\u001b[39m{\u001b[39;00mtrain_macro_f1\u001b[39m:\u001b[39;00m\u001b[39m01.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m,Test_f1: \u001b[39m\u001b[39m{\u001b[39;00mtest_macro_f1\u001b[39m:\u001b[39;00m\u001b[39m01.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m,Train_acc: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m01.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m,Test_acc: \u001b[39m\u001b[39m{\u001b[39;00mtest_acc\u001b[39m:\u001b[39;00m\u001b[39m01.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(loader, return_loss)\u001b[0m\n\u001b[1;32m     19\u001b[0m y_true \u001b[39m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m loader:\n\u001b[1;32m     22\u001b[0m     y_out \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39medge_index,data\u001b[39m.\u001b[39medge_attr, data\u001b[39m.\u001b[39mbatch)\n\u001b[1;32m     23\u001b[0m     y_pred\u001b[39m.\u001b[39mappend(y_out\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch_geometric/loader/dataloader.py:20\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     18\u001b[0m elem \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m]\n\u001b[1;32m     19\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, BaseData):\n\u001b[0;32m---> 20\u001b[0m     \u001b[39mreturn\u001b[39;00m Batch\u001b[39m.\u001b[39;49mfrom_data_list(batch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfollow_batch,\n\u001b[1;32m     21\u001b[0m                                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexclude_keys)\n\u001b[1;32m     22\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch_geometric/data/batch.py:76\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_data_list\u001b[39m(\u001b[39mcls\u001b[39m, data_list: List[BaseData],\n\u001b[1;32m     66\u001b[0m                    follow_batch: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     67\u001b[0m                    exclude_keys: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     68\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39m    Python list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39m    :obj:`follow_batch`.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39m    Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     batch, slice_dict, inc_dict \u001b[39m=\u001b[39m collate(\n\u001b[1;32m     77\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[1;32m     78\u001b[0m         data_list\u001b[39m=\u001b[39;49mdata_list,\n\u001b[1;32m     79\u001b[0m         increment\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     80\u001b[0m         add_batch\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(data_list[\u001b[39m0\u001b[39;49m], Batch),\n\u001b[1;32m     81\u001b[0m         follow_batch\u001b[39m=\u001b[39;49mfollow_batch,\n\u001b[1;32m     82\u001b[0m         exclude_keys\u001b[39m=\u001b[39;49mexclude_keys,\n\u001b[1;32m     83\u001b[0m     )\n\u001b[1;32m     85\u001b[0m     batch\u001b[39m.\u001b[39m_num_graphs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data_list)\n\u001b[1;32m     86\u001b[0m     batch\u001b[39m.\u001b[39m_slice_dict \u001b[39m=\u001b[39m slice_dict\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch_geometric/data/collate.py:32\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     29\u001b[0m     data_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(data_list)\n\u001b[1;32m     31\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39m!=\u001b[39m data_list[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m:\n\u001b[0;32m---> 32\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(_base_cls\u001b[39m=\u001b[39;49mdata_list[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m)  \u001b[39m# Dynamic inheritance.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m()\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch_geometric/data/batch.py:38\u001b[0m, in \u001b[0;36mDynamicInheritance.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[39mglobals\u001b[39m()[name] \u001b[39m=\u001b[39m MetaResolver(name, (\u001b[39mcls\u001b[39m, base_cls), {})\n\u001b[1;32m     36\u001b[0m     new_cls \u001b[39m=\u001b[39m \u001b[39mglobals\u001b[39m()[name]\n\u001b[0;32m---> 38\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(inspect\u001b[39m.\u001b[39;49msignature(base_cls\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mitems())\n\u001b[1;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m i, (k, v) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(params[\u001b[39m1\u001b[39m:]):\n\u001b[1;32m     40\u001b[0m     \u001b[39mif\u001b[39;00m k \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39margs\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m k \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mkwargs\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3.9/inspect.py:3130\u001b[0m, in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped)\u001b[0m\n\u001b[1;32m   3128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msignature\u001b[39m(obj, \u001b[39m*\u001b[39m, follow_wrapped\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   3129\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3130\u001b[0m     \u001b[39mreturn\u001b[39;00m Signature\u001b[39m.\u001b[39;49mfrom_callable(obj, follow_wrapped\u001b[39m=\u001b[39;49mfollow_wrapped)\n",
      "File \u001b[0;32m/usr/lib/python3.9/inspect.py:2879\u001b[0m, in \u001b[0;36mSignature.from_callable\u001b[0;34m(cls, obj, follow_wrapped)\u001b[0m\n\u001b[1;32m   2876\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   2877\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_callable\u001b[39m(\u001b[39mcls\u001b[39m, obj, \u001b[39m*\u001b[39m, follow_wrapped\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   2878\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2879\u001b[0m     \u001b[39mreturn\u001b[39;00m _signature_from_callable(obj, sigcls\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m,\n\u001b[1;32m   2880\u001b[0m                                     follow_wrapper_chains\u001b[39m=\u001b[39;49mfollow_wrapped)\n",
      "File \u001b[0;32m/usr/lib/python3.9/inspect.py:2330\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, sigcls)\u001b[0m\n\u001b[1;32m   2325\u001b[0m             \u001b[39mreturn\u001b[39;00m sig\u001b[39m.\u001b[39mreplace(parameters\u001b[39m=\u001b[39mnew_params)\n\u001b[1;32m   2327\u001b[0m \u001b[39mif\u001b[39;00m isfunction(obj) \u001b[39mor\u001b[39;00m _signature_is_functionlike(obj):\n\u001b[1;32m   2328\u001b[0m     \u001b[39m# If it's a pure Python function, or an object that is duck type\u001b[39;00m\n\u001b[1;32m   2329\u001b[0m     \u001b[39m# of a Python function (Cython functions, for instance), then:\u001b[39;00m\n\u001b[0;32m-> 2330\u001b[0m     \u001b[39mreturn\u001b[39;00m _signature_from_function(sigcls, obj,\n\u001b[1;32m   2331\u001b[0m                                     skip_bound_arg\u001b[39m=\u001b[39;49mskip_bound_arg)\n\u001b[1;32m   2333\u001b[0m \u001b[39mif\u001b[39;00m _signature_is_builtin(obj):\n\u001b[1;32m   2334\u001b[0m     \u001b[39mreturn\u001b[39;00m _signature_from_builtin(sigcls, obj,\n\u001b[1;32m   2335\u001b[0m                                    skip_bound_arg\u001b[39m=\u001b[39mskip_bound_arg)\n",
      "File \u001b[0;32m/usr/lib/python3.9/inspect.py:2203\u001b[0m, in \u001b[0;36m_signature_from_function\u001b[0;34m(cls, func, skip_bound_arg)\u001b[0m\n\u001b[1;32m   2201\u001b[0m kind \u001b[39m=\u001b[39m _POSITIONAL_ONLY \u001b[39mif\u001b[39;00m posonly_left \u001b[39melse\u001b[39;00m _POSITIONAL_OR_KEYWORD\n\u001b[1;32m   2202\u001b[0m annotation \u001b[39m=\u001b[39m annotations\u001b[39m.\u001b[39mget(name, _empty)\n\u001b[0;32m-> 2203\u001b[0m parameters\u001b[39m.\u001b[39mappend(Parameter(name, annotation\u001b[39m=\u001b[39;49mannotation,\n\u001b[1;32m   2204\u001b[0m                             kind\u001b[39m=\u001b[39;49mkind,\n\u001b[1;32m   2205\u001b[0m                             default\u001b[39m=\u001b[39;49mdefaults[offset]))\n\u001b[1;32m   2206\u001b[0m \u001b[39mif\u001b[39;00m posonly_left:\n\u001b[1;32m   2207\u001b[0m     posonly_left \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.9/inspect.py:2550\u001b[0m, in \u001b[0;36mParameter.__init__\u001b[0;34m(self, name, kind, default, annotation)\u001b[0m\n\u001b[1;32m   2547\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_kind \u001b[39m=\u001b[39m _POSITIONAL_ONLY\n\u001b[1;32m   2548\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mimplicit\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(name[\u001b[39m1\u001b[39m:])\n\u001b[0;32m-> 2550\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m name\u001b[39m.\u001b[39;49misidentifier():\n\u001b[1;32m   2551\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m is not a valid parameter name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(name))\n\u001b[1;32m   2553\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m name\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 501):\n",
    "    train()\n",
    "    train_macro_f1, train_acc, train_loss = test(train_loader)\n",
    "    test_macro_f1, test_acc, test_loss = test(test_loader, True)\n",
    "    scheduler.step(train_loss)\n",
    "    print(\n",
    "        f\"Epoch: {epoch:03d}, Train_Loss: {train_loss:02.4f},Test_Loss: {test_loss:02.4f},Train_f1: {train_macro_f1:01.4f},Test_f1: {test_macro_f1:01.4f},Train_acc: {train_acc:01.4f},Test_acc: {test_acc:01.4f}\"\n",
    "    )\n",
    "    if test_macro_f1 > best_macro_f1:\n",
    "        best_accuracy = test_macro_f1\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), \"../data/best_model.pth\")\n",
    "    elif epoch - best_epoch > early_stop_thresh:\n",
    "        print(\n",
    "            f\"Early stopped training at epoch {epoch} best macro F1 {best_macro_f1} in {best_epoch}\"\n",
    "        )\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
