{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import utils\n",
    "from gcn_models import GIN, GATC, SAGE\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from os import path\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stop_thresh = 25\n",
    "best_macro_f1 = -1\n",
    "\n",
    "\n",
    "train_dataset = utils.GraphDataset(\n",
    "    \"../data/\", \"MixedShapesSmallTrain_TRAIN\", True, n_quantiles=100\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = utils.GraphDataset(\n",
    "    \"../data/\", \"MixedShapesSmallTrain_TEST\", True, n_quantiles=100\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "\n",
    "model = GIN(1, [8, 16, 32, 32, 32, 16, 16], pool_method=\"add\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=5, mode=\"min\", cooldown=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x.to(torch.float32), data.edge_index, data.batch)\n",
    "        loss = F.cross_entropy(out, data.y - 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader, return_loss=True):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    loss = 0\n",
    "    for data in loader:\n",
    "        y_out = model(data.x.to(torch.float32), data.edge_index, data.batch)\n",
    "        y_pred.append(y_out.argmax(dim=-1))\n",
    "        y_true.append(data.y - 1)\n",
    "        if return_loss:\n",
    "            loss += float(F.cross_entropy(y_out, data.y - 1)) * data.num_graphs\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    y_true = np.concatenate(y_true)\n",
    "    if return_loss:\n",
    "        return f1_score(y_true=y_true, y_pred=y_pred, average=\"macro\"), loss / len(\n",
    "            loader.dataset\n",
    "        )\n",
    "    return f1_score(y_true=y_true, y_pred=y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train_Loss: 20.3010, Train: 0.2076, Test_Loss: 19.4301, Test: 0.1978\n",
      "Epoch: 002, Train_Loss: 35.5605, Train: 0.1841, Test_Loss: 25.6824, Test: 0.1920\n",
      "Epoch: 003, Train_Loss: 22.1005, Train: 0.2122, Test_Loss: 15.6819, Test: 0.1865\n",
      "Epoch: 004, Train_Loss: 3.8044, Train: 0.2164, Test_Loss: 3.8729, Test: 0.2165\n",
      "Epoch: 005, Train_Loss: 4.2472, Train: 0.1828, Test_Loss: 4.2155, Test: 0.1753\n",
      "Epoch: 006, Train_Loss: 2.5106, Train: 0.1581, Test_Loss: 2.6406, Test: 0.1561\n",
      "Epoch: 007, Train_Loss: 1.9503, Train: 0.2080, Test_Loss: 1.8451, Test: 0.1683\n",
      "Epoch: 008, Train_Loss: 1.7603, Train: 0.0667, Test_Loss: 1.6410, Test: 0.0799\n",
      "Epoch: 009, Train_Loss: 1.7844, Train: 0.0667, Test_Loss: 1.6603, Test: 0.0782\n",
      "Epoch: 010, Train_Loss: 1.6448, Train: 0.1819, Test_Loss: 1.6096, Test: 0.1603\n",
      "Epoch: 011, Train_Loss: 1.6011, Train: 0.1984, Test_Loss: 1.5907, Test: 0.1620\n",
      "Epoch: 012, Train_Loss: 1.5937, Train: 0.1978, Test_Loss: 1.5828, Test: 0.1741\n",
      "Epoch: 013, Train_Loss: 1.5781, Train: 0.2124, Test_Loss: 1.5686, Test: 0.1749\n",
      "Epoch: 014, Train_Loss: 1.6874, Train: 0.1667, Test_Loss: 1.6036, Test: 0.1636\n",
      "Epoch: 015, Train_Loss: 1.7129, Train: 0.1666, Test_Loss: 1.6256, Test: 0.1601\n",
      "Epoch: 016, Train_Loss: 1.5694, Train: 0.1700, Test_Loss: 1.5839, Test: 0.1652\n",
      "Epoch: 017, Train_Loss: 1.5154, Train: 0.1697, Test_Loss: 1.5411, Test: 0.1703\n",
      "Epoch: 018, Train_Loss: 1.4486, Train: 0.2324, Test_Loss: 1.5174, Test: 0.1887\n",
      "Epoch: 019, Train_Loss: 1.5069, Train: 0.2418, Test_Loss: 1.5834, Test: 0.1933\n",
      "Epoch: 020, Train_Loss: 1.4589, Train: 0.2429, Test_Loss: 1.5545, Test: 0.1836\n",
      "Epoch: 021, Train_Loss: 1.4668, Train: 0.2174, Test_Loss: 1.5345, Test: 0.1625\n",
      "Epoch: 022, Train_Loss: 1.4304, Train: 0.1998, Test_Loss: 1.5092, Test: 0.1692\n",
      "Epoch: 023, Train_Loss: 1.6881, Train: 0.1834, Test_Loss: 1.6328, Test: 0.1740\n",
      "Epoch: 024, Train_Loss: 1.6900, Train: 0.1892, Test_Loss: 1.6226, Test: 0.1741\n",
      "Epoch: 025, Train_Loss: 1.5604, Train: 0.1671, Test_Loss: 1.5606, Test: 0.1603\n",
      "Epoch: 026, Train_Loss: 1.4309, Train: 0.2170, Test_Loss: 1.5145, Test: 0.1611\n",
      "Epoch: 027, Train_Loss: 1.4299, Train: 0.1994, Test_Loss: 1.5296, Test: 0.1491\n",
      "Epoch: 028, Train_Loss: 1.4223, Train: 0.2125, Test_Loss: 1.5300, Test: 0.1520\n",
      "Epoch: 029, Train_Loss: 1.4584, Train: 0.2114, Test_Loss: 1.5305, Test: 0.1350\n",
      "Epoch: 030, Train_Loss: 1.5333, Train: 0.1998, Test_Loss: 1.5593, Test: 0.1414\n",
      "Epoch: 031, Train_Loss: 1.5525, Train: 0.2022, Test_Loss: 1.5605, Test: 0.1447\n",
      "Epoch: 032, Train_Loss: 1.5077, Train: 0.2037, Test_Loss: 1.5252, Test: 0.1729\n",
      "Epoch: 033, Train_Loss: 1.4981, Train: 0.2084, Test_Loss: 1.4943, Test: 0.2100\n",
      "Epoch: 034, Train_Loss: 1.5034, Train: 0.2357, Test_Loss: 1.5297, Test: 0.1829\n",
      "Epoch: 035, Train_Loss: 1.4863, Train: 0.2205, Test_Loss: 1.5224, Test: 0.1774\n",
      "Epoch: 036, Train_Loss: 1.4895, Train: 0.2225, Test_Loss: 1.5256, Test: 0.1760\n",
      "Epoch: 037, Train_Loss: 1.4627, Train: 0.2406, Test_Loss: 1.5146, Test: 0.1790\n",
      "Epoch: 038, Train_Loss: 1.4353, Train: 0.2591, Test_Loss: 1.5019, Test: 0.1840\n",
      "Epoch: 039, Train_Loss: 1.4345, Train: 0.2477, Test_Loss: 1.5041, Test: 0.1864\n",
      "Epoch: 040, Train_Loss: 1.4269, Train: 0.2524, Test_Loss: 1.4933, Test: 0.1895\n",
      "Epoch: 041, Train_Loss: 1.4184, Train: 0.2596, Test_Loss: 1.4830, Test: 0.1909\n",
      "Epoch: 042, Train_Loss: 1.4310, Train: 0.2595, Test_Loss: 1.4842, Test: 0.1890\n",
      "Epoch: 043, Train_Loss: 1.4610, Train: 0.2558, Test_Loss: 1.4930, Test: 0.1924\n",
      "Epoch: 044, Train_Loss: 1.4661, Train: 0.2496, Test_Loss: 1.4876, Test: 0.1947\n",
      "Epoch: 045, Train_Loss: 1.4569, Train: 0.2350, Test_Loss: 1.4794, Test: 0.1932\n",
      "Epoch: 046, Train_Loss: 1.4110, Train: 0.2209, Test_Loss: 1.4568, Test: 0.2086\n",
      "Epoch: 047, Train_Loss: 1.4195, Train: 0.2313, Test_Loss: 1.4568, Test: 0.2077\n",
      "Epoch: 048, Train_Loss: 1.4431, Train: 0.2238, Test_Loss: 1.4682, Test: 0.2035\n",
      "Epoch: 049, Train_Loss: 1.4091, Train: 0.2200, Test_Loss: 1.4424, Test: 0.2177\n",
      "Epoch: 050, Train_Loss: 1.3481, Train: 0.2358, Test_Loss: 1.4062, Test: 0.2331\n",
      "Epoch: 051, Train_Loss: 1.3591, Train: 0.2513, Test_Loss: 1.4135, Test: 0.2278\n",
      "Epoch: 052, Train_Loss: 1.4053, Train: 0.2470, Test_Loss: 1.4418, Test: 0.2351\n",
      "Epoch: 053, Train_Loss: 1.3426, Train: 0.3258, Test_Loss: 1.4066, Test: 0.2809\n",
      "Epoch: 054, Train_Loss: 1.3161, Train: 0.3271, Test_Loss: 1.3952, Test: 0.2884\n",
      "Epoch: 055, Train_Loss: 1.3355, Train: 0.3257, Test_Loss: 1.4060, Test: 0.2867\n",
      "Epoch: 056, Train_Loss: 1.3547, Train: 0.3703, Test_Loss: 1.4159, Test: 0.3001\n",
      "Epoch: 057, Train_Loss: 1.4850, Train: 0.2421, Test_Loss: 1.5033, Test: 0.2197\n",
      "Epoch: 058, Train_Loss: 1.3830, Train: 0.3267, Test_Loss: 1.4386, Test: 0.2746\n",
      "Epoch: 059, Train_Loss: 1.2755, Train: 0.3445, Test_Loss: 1.4178, Test: 0.3041\n",
      "Epoch: 060, Train_Loss: 1.3709, Train: 0.4896, Test_Loss: 1.5514, Test: 0.3769\n",
      "Epoch: 061, Train_Loss: 1.3726, Train: 0.3728, Test_Loss: 1.5441, Test: 0.3319\n",
      "Epoch: 062, Train_Loss: 1.4167, Train: 0.3685, Test_Loss: 1.6018, Test: 0.3029\n",
      "Epoch: 063, Train_Loss: 1.5476, Train: 0.3843, Test_Loss: 1.7416, Test: 0.3172\n",
      "Epoch: 064, Train_Loss: 1.3691, Train: 0.3677, Test_Loss: 1.4943, Test: 0.3387\n",
      "Epoch: 065, Train_Loss: 1.3223, Train: 0.3142, Test_Loss: 1.4196, Test: 0.3443\n",
      "Epoch: 066, Train_Loss: 1.3202, Train: 0.3157, Test_Loss: 1.4070, Test: 0.3467\n",
      "Epoch: 067, Train_Loss: 1.3109, Train: 0.3417, Test_Loss: 1.3962, Test: 0.3638\n",
      "Epoch: 068, Train_Loss: 1.3029, Train: 0.3025, Test_Loss: 1.3879, Test: 0.3587\n",
      "Epoch: 069, Train_Loss: 1.3093, Train: 0.3154, Test_Loss: 1.3891, Test: 0.3373\n",
      "Epoch: 070, Train_Loss: 1.3141, Train: 0.3384, Test_Loss: 1.3917, Test: 0.3272\n",
      "Epoch: 071, Train_Loss: 1.3008, Train: 0.3547, Test_Loss: 1.3833, Test: 0.3336\n",
      "Epoch: 072, Train_Loss: 1.2963, Train: 0.3753, Test_Loss: 1.3739, Test: 0.3461\n",
      "Epoch: 073, Train_Loss: 1.2910, Train: 0.3842, Test_Loss: 1.3693, Test: 0.3486\n",
      "Epoch: 074, Train_Loss: 1.2775, Train: 0.4060, Test_Loss: 1.3639, Test: 0.3619\n",
      "Epoch: 075, Train_Loss: 1.2700, Train: 0.4145, Test_Loss: 1.3634, Test: 0.3646\n",
      "Epoch: 076, Train_Loss: 1.2691, Train: 0.3886, Test_Loss: 1.3652, Test: 0.3584\n",
      "Epoch: 077, Train_Loss: 1.2678, Train: 0.3822, Test_Loss: 1.3659, Test: 0.3586\n",
      "Epoch: 078, Train_Loss: 1.2739, Train: 0.3387, Test_Loss: 1.3691, Test: 0.3430\n",
      "Epoch: 079, Train_Loss: 1.2724, Train: 0.3306, Test_Loss: 1.3730, Test: 0.3359\n",
      "Epoch: 080, Train_Loss: 1.2687, Train: 0.3188, Test_Loss: 1.3713, Test: 0.3392\n",
      "Epoch: 081, Train_Loss: 1.2667, Train: 0.3343, Test_Loss: 1.3709, Test: 0.3448\n",
      "Epoch: 082, Train_Loss: 1.2670, Train: 0.3343, Test_Loss: 1.3707, Test: 0.3449\n",
      "Epoch: 083, Train_Loss: 1.2661, Train: 0.3623, Test_Loss: 1.3703, Test: 0.3568\n",
      "Epoch: 084, Train_Loss: 1.2697, Train: 0.3617, Test_Loss: 1.3699, Test: 0.3485\n",
      "Epoch: 085, Train_Loss: 1.2673, Train: 0.3792, Test_Loss: 1.3694, Test: 0.3484\n",
      "Epoch: 086, Train_Loss: 1.2634, Train: 0.3715, Test_Loss: 1.3690, Test: 0.3644\n",
      "Epoch: 087, Train_Loss: 1.2646, Train: 0.3138, Test_Loss: 1.3708, Test: 0.3548\n",
      "Epoch: 088, Train_Loss: 1.2661, Train: 0.3267, Test_Loss: 1.3694, Test: 0.3521\n",
      "Epoch: 089, Train_Loss: 1.2654, Train: 0.3894, Test_Loss: 1.3684, Test: 0.3539\n",
      "Epoch: 090, Train_Loss: 1.2678, Train: 0.3014, Test_Loss: 1.3728, Test: 0.3405\n",
      "Epoch: 091, Train_Loss: 1.2645, Train: 0.3388, Test_Loss: 1.3711, Test: 0.3484\n",
      "Epoch: 092, Train_Loss: 1.2662, Train: 0.3489, Test_Loss: 1.3711, Test: 0.3431\n",
      "Epoch: 093, Train_Loss: 1.2730, Train: 0.3468, Test_Loss: 1.3733, Test: 0.3402\n",
      "Epoch: 094, Train_Loss: 1.2686, Train: 0.3946, Test_Loss: 1.3696, Test: 0.3509\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m train()\n\u001b[1;32m      3\u001b[0m train_macro_f1, train_loss \u001b[39m=\u001b[39m test(train_loader)\n\u001b[0;32m----> 4\u001b[0m test_macro_f1, test_loss \u001b[39m=\u001b[39m test(test_loader, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      5\u001b[0m scheduler\u001b[39m.\u001b[39mstep(train_loss)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[1;32m      7\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train_Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m02.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train: \u001b[39m\u001b[39m{\u001b[39;00mtrain_macro_f1\u001b[39m:\u001b[39;00m\u001b[39m01.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Test_Loss: \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m:\u001b[39;00m\u001b[39m02.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Test: \u001b[39m\u001b[39m{\u001b[39;00mtest_macro_f1\u001b[39m:\u001b[39;00m\u001b[39m01.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[11], line 21\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(loader, return_loss)\u001b[0m\n\u001b[1;32m     19\u001b[0m y_true \u001b[39m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m loader:\n\u001b[1;32m     22\u001b[0m     y_out \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32), data\u001b[39m.\u001b[39medge_index, data\u001b[39m.\u001b[39mbatch)\n\u001b[1;32m     23\u001b[0m     y_pred\u001b[39m.\u001b[39mappend(y_out\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Projects/2023L-machine-learning-on-graphs/project_2/utils.py:147\u001b[0m, in \u001b[0;36mGraphDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m--> 147\u001b[0m     data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(path\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00midx\u001b[39m}\u001b[39;49;00m\u001b[39m.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    148\u001b[0m     data\u001b[39m.\u001b[39my \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels[idx]\n\u001b[1;32m    149\u001b[0m     data\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 809\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    810\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1174\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/serialization.py:1163\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1161\u001b[0m         \u001b[39mreturn\u001b[39;00m StorageType(name)\n\u001b[1;32m   1162\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m mod_name \u001b[39m=\u001b[39m load_module_mapping\u001b[39m.\u001b[39mget(mod_name, mod_name)\n\u001b[1;32m   1165\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mfind_class(mod_name, name)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 501):\n",
    "    train()\n",
    "    train_macro_f1, train_loss = test(train_loader)\n",
    "    test_macro_f1, test_loss = test(test_loader, True)\n",
    "    scheduler.step(train_loss)\n",
    "    print(\n",
    "        f\"Epoch: {epoch:03d}, Train_Loss: {train_loss:02.4f}, Train: {train_macro_f1:01.4f}, Test_Loss: {test_loss:02.4f}, Test: {test_macro_f1:01.4f}\"\n",
    "    )\n",
    "    if test_macro_f1 > best_macro_f1:\n",
    "        best_accuracy = test_macro_f1\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), \"../data/best_model.pth\")\n",
    "    elif epoch - best_epoch > early_stop_thresh:\n",
    "        print(\n",
    "            f\"Early stopped training at epoch {epoch} best macro F1 {best_macro_f1} in {best_epoch}\"\n",
    "        )\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
