{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import DenseGraphConv, GCNConv, dense_mincut_pool\n",
    "from torch_geometric.utils import to_dense_adj, to_dense_batch\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import utils\n",
    "import torch_geometric as tg\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "early_stop_thresh = 25\n",
    "best_macro_f1 = -1\n",
    "\n",
    "\n",
    "train_dataset = utils.GraphDataset(\n",
    "    \"../data/\", \"MixedShapesSmallTrain_TRAIN\", True, n_quantiles=100\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "test_dataset = utils.GraphDataset(\n",
    "    \"../data/\", \"MixedShapesSmallTrain_TEST\", True, n_quantiles=100\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        graph_sizes,\n",
    "        n_edge_layers=2,\n",
    "        hidden_channels=32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.edge_layers = torch.nn.ModuleList()\n",
    "        for it in range(n_edge_layers):\n",
    "            nn = tg.nn.MLP([in_channels, hidden_channels, hidden_channels])\n",
    "            self.edge_layers.append(tg.nn.GINConv(nn))\n",
    "            in_channels = hidden_channels\n",
    "        self.gcn_layers = torch.nn.ModuleList()\n",
    "        for n_nodes in graph_sizes:\n",
    "            self.gcn_layers.append(Linear(hidden_channels, n_nodes))\n",
    "            nn = tg.nn.MLP([hidden_channels, hidden_channels])\n",
    "            self.gcn_layers.append(tg.nn.DenseGINConv(nn))\n",
    "            nn = tg.nn.MLP([hidden_channels, hidden_channels])\n",
    "            self.gcn_layers.append(tg.nn.DenseGINConv(nn))\n",
    "        nn = tg.nn.MLP([hidden_channels, hidden_channels])\n",
    "        self.gcn_layers.append(tg.nn.DenseGINConv(nn))\n",
    "\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = (\n",
    "            data.x,\n",
    "            data.edge_index,\n",
    "            data.edge_attr,\n",
    "            data.batch,\n",
    "        )\n",
    "        for layer in self.edge_layers:\n",
    "            x = layer(x, edge_index).relu()\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        adj = to_dense_adj(edge_index, batch)\n",
    "\n",
    "        mincut_loss = torch.zeros(1)\n",
    "        ortho_loss = torch.zeros(1)\n",
    "        for layer in self.gcn_layers:\n",
    "            if layer._get_name() == \"Linear\":\n",
    "                s = layer(x)\n",
    "                x, adj, mc, o = dense_mincut_pool(x, adj, s, mask)\n",
    "                mincut_loss += mc\n",
    "                ortho_loss += o\n",
    "                mask = None\n",
    "            else:\n",
    "                x = layer(x, adj).relu()\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1), mincut_loss, ortho_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net(1, 5, [50, 20], hidden_channels=8).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=5, mode=\"min\", cooldown=2, factor=0.5, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train_Loss: 2.0645,Test_Loss: 2.1069,Train_acc: 0.3000,Test_acc: 0.2169\n",
      "Epoch: 002, Train_Loss: 2.0597,Test_Loss: 2.1024,Train_acc: 0.3100,Test_acc: 0.2227\n",
      "Epoch: 003, Train_Loss: 2.0552,Test_Loss: 2.0973,Train_acc: 0.3000,Test_acc: 0.2289\n",
      "Epoch: 004, Train_Loss: 2.0512,Test_Loss: 2.0920,Train_acc: 0.3100,Test_acc: 0.2326\n",
      "Epoch: 005, Train_Loss: 2.0470,Test_Loss: 2.0866,Train_acc: 0.3300,Test_acc: 0.2346\n",
      "Epoch: 006, Train_Loss: 2.0424,Test_Loss: 2.0808,Train_acc: 0.3200,Test_acc: 0.2326\n",
      "Epoch: 007, Train_Loss: 2.0375,Test_Loss: 2.0751,Train_acc: 0.3300,Test_acc: 0.2713\n",
      "Epoch: 008, Train_Loss: 2.0325,Test_Loss: 2.0697,Train_acc: 0.3300,Test_acc: 0.3419\n",
      "Epoch: 009, Train_Loss: 2.0276,Test_Loss: 2.0644,Train_acc: 0.3900,Test_acc: 0.3856\n",
      "Epoch: 010, Train_Loss: 2.0224,Test_Loss: 2.0586,Train_acc: 0.4300,Test_acc: 0.4276\n",
      "Epoch: 011, Train_Loss: 2.0168,Test_Loss: 2.0523,Train_acc: 0.4400,Test_acc: 0.4503\n",
      "Epoch: 012, Train_Loss: 2.0109,Test_Loss: 2.0464,Train_acc: 0.4700,Test_acc: 0.4586\n",
      "Epoch: 013, Train_Loss: 2.0053,Test_Loss: 2.0399,Train_acc: 0.4700,Test_acc: 0.4647\n",
      "Epoch: 014, Train_Loss: 1.9992,Test_Loss: 2.0338,Train_acc: 0.4700,Test_acc: 0.4656\n",
      "Epoch: 015, Train_Loss: 1.9927,Test_Loss: 2.0278,Train_acc: 0.4800,Test_acc: 0.4689\n",
      "Epoch: 016, Train_Loss: 1.9865,Test_Loss: 2.0209,Train_acc: 0.4700,Test_acc: 0.4693\n",
      "Epoch: 017, Train_Loss: 1.9799,Test_Loss: 2.0136,Train_acc: 0.4700,Test_acc: 0.4759\n",
      "Epoch: 018, Train_Loss: 1.9730,Test_Loss: 2.0060,Train_acc: 0.4700,Test_acc: 0.4800\n",
      "Epoch: 019, Train_Loss: 1.9662,Test_Loss: 1.9978,Train_acc: 0.4600,Test_acc: 0.4779\n",
      "Epoch: 020, Train_Loss: 1.9589,Test_Loss: 1.9894,Train_acc: 0.4700,Test_acc: 0.4734\n",
      "Epoch: 021, Train_Loss: 1.9514,Test_Loss: 1.9809,Train_acc: 0.4700,Test_acc: 0.4718\n",
      "Epoch: 022, Train_Loss: 1.9438,Test_Loss: 1.9721,Train_acc: 0.4700,Test_acc: 0.4668\n",
      "Epoch: 023, Train_Loss: 1.9359,Test_Loss: 1.9636,Train_acc: 0.4600,Test_acc: 0.4590\n",
      "Epoch: 024, Train_Loss: 1.9280,Test_Loss: 1.9555,Train_acc: 0.4300,Test_acc: 0.4445\n",
      "Epoch: 025, Train_Loss: 1.9199,Test_Loss: 1.9469,Train_acc: 0.4200,Test_acc: 0.4416\n",
      "Epoch: 026, Train_Loss: 1.9117,Test_Loss: 1.9377,Train_acc: 0.4200,Test_acc: 0.4367\n",
      "Epoch: 027, Train_Loss: 1.9034,Test_Loss: 1.9274,Train_acc: 0.4300,Test_acc: 0.4355\n",
      "Epoch: 028, Train_Loss: 1.8950,Test_Loss: 1.9160,Train_acc: 0.4300,Test_acc: 0.4416\n",
      "Epoch: 029, Train_Loss: 1.8863,Test_Loss: 1.9051,Train_acc: 0.4300,Test_acc: 0.4478\n",
      "Epoch: 030, Train_Loss: 1.8777,Test_Loss: 1.8955,Train_acc: 0.4400,Test_acc: 0.4503\n",
      "Epoch: 031, Train_Loss: 1.8693,Test_Loss: 1.8878,Train_acc: 0.4200,Test_acc: 0.4466\n",
      "Epoch: 032, Train_Loss: 1.8610,Test_Loss: 1.8828,Train_acc: 0.4000,Test_acc: 0.4416\n",
      "Epoch: 033, Train_Loss: 1.8530,Test_Loss: 1.8781,Train_acc: 0.4000,Test_acc: 0.4379\n",
      "Epoch: 034, Train_Loss: 1.8452,Test_Loss: 1.8723,Train_acc: 0.4000,Test_acc: 0.4355\n",
      "Epoch: 035, Train_Loss: 1.8375,Test_Loss: 1.8669,Train_acc: 0.4000,Test_acc: 0.4338\n",
      "Epoch: 036, Train_Loss: 1.8299,Test_Loss: 1.8642,Train_acc: 0.3900,Test_acc: 0.4334\n",
      "Epoch: 037, Train_Loss: 1.8229,Test_Loss: 1.8626,Train_acc: 0.4000,Test_acc: 0.4301\n",
      "Epoch: 038, Train_Loss: 1.8159,Test_Loss: 1.8608,Train_acc: 0.4000,Test_acc: 0.4289\n",
      "Epoch: 039, Train_Loss: 1.8089,Test_Loss: 1.8564,Train_acc: 0.4000,Test_acc: 0.4293\n",
      "Epoch: 040, Train_Loss: 1.8020,Test_Loss: 1.8524,Train_acc: 0.4000,Test_acc: 0.4268\n",
      "Epoch: 041, Train_Loss: 1.7953,Test_Loss: 1.8452,Train_acc: 0.4000,Test_acc: 0.4305\n",
      "Epoch: 042, Train_Loss: 1.7888,Test_Loss: 1.8376,Train_acc: 0.3900,Test_acc: 0.4384\n",
      "Epoch: 043, Train_Loss: 1.7822,Test_Loss: 1.8319,Train_acc: 0.3900,Test_acc: 0.4429\n",
      "Epoch: 044, Train_Loss: 1.7760,Test_Loss: 1.8312,Train_acc: 0.4000,Test_acc: 0.4412\n",
      "Epoch: 045, Train_Loss: 1.7698,Test_Loss: 1.8324,Train_acc: 0.4000,Test_acc: 0.4379\n",
      "Epoch: 046, Train_Loss: 1.7637,Test_Loss: 1.8292,Train_acc: 0.4100,Test_acc: 0.4371\n",
      "Epoch: 047, Train_Loss: 1.7579,Test_Loss: 1.8139,Train_acc: 0.4100,Test_acc: 0.4482\n",
      "Epoch: 048, Train_Loss: 1.7521,Test_Loss: 1.8117,Train_acc: 0.4200,Test_acc: 0.4544\n",
      "Epoch: 049, Train_Loss: 1.7464,Test_Loss: 1.8077,Train_acc: 0.4200,Test_acc: 0.4623\n",
      "Epoch: 050, Train_Loss: 1.7411,Test_Loss: 1.8011,Train_acc: 0.4400,Test_acc: 0.4668\n",
      "Epoch: 051, Train_Loss: 1.7356,Test_Loss: 1.7966,Train_acc: 0.4400,Test_acc: 0.4693\n",
      "Epoch: 052, Train_Loss: 1.7302,Test_Loss: 1.7952,Train_acc: 0.4300,Test_acc: 0.4705\n",
      "Epoch: 053, Train_Loss: 1.7248,Test_Loss: 1.7918,Train_acc: 0.4500,Test_acc: 0.4751\n",
      "Epoch: 054, Train_Loss: 1.7195,Test_Loss: 1.7875,Train_acc: 0.4700,Test_acc: 0.4854\n",
      "Epoch: 055, Train_Loss: 1.7142,Test_Loss: 1.7828,Train_acc: 0.4700,Test_acc: 0.4887\n",
      "Epoch: 056, Train_Loss: 1.7089,Test_Loss: 1.7800,Train_acc: 0.4700,Test_acc: 0.4866\n",
      "Epoch: 057, Train_Loss: 1.7035,Test_Loss: 1.7776,Train_acc: 0.4700,Test_acc: 0.4907\n",
      "Epoch: 058, Train_Loss: 1.6983,Test_Loss: 1.7756,Train_acc: 0.4700,Test_acc: 0.4907\n",
      "Epoch: 059, Train_Loss: 1.6930,Test_Loss: 1.7756,Train_acc: 0.4700,Test_acc: 0.4907\n",
      "Epoch: 060, Train_Loss: 1.6877,Test_Loss: 1.7759,Train_acc: 0.5000,Test_acc: 0.4944\n",
      "Epoch: 061, Train_Loss: 1.6825,Test_Loss: 1.7753,Train_acc: 0.5100,Test_acc: 0.4969\n",
      "Epoch: 062, Train_Loss: 1.6773,Test_Loss: 1.7780,Train_acc: 0.5100,Test_acc: 0.4953\n",
      "Epoch: 063, Train_Loss: 1.6723,Test_Loss: 1.7769,Train_acc: 0.5000,Test_acc: 0.4977\n",
      "Epoch: 064, Train_Loss: 1.6674,Test_Loss: 1.7747,Train_acc: 0.4900,Test_acc: 0.4953\n",
      "Epoch: 065, Train_Loss: 1.6627,Test_Loss: 1.7754,Train_acc: 0.4900,Test_acc: 0.4920\n",
      "Epoch: 066, Train_Loss: 1.6582,Test_Loss: 1.7708,Train_acc: 0.4800,Test_acc: 0.4953\n",
      "Epoch: 067, Train_Loss: 1.6536,Test_Loss: 1.7702,Train_acc: 0.4800,Test_acc: 0.4944\n",
      "Epoch: 068, Train_Loss: 1.6490,Test_Loss: 1.7684,Train_acc: 0.4900,Test_acc: 0.4928\n",
      "Epoch: 069, Train_Loss: 1.6444,Test_Loss: 1.7670,Train_acc: 0.4900,Test_acc: 0.4920\n",
      "Epoch: 070, Train_Loss: 1.6399,Test_Loss: 1.7659,Train_acc: 0.5000,Test_acc: 0.4973\n",
      "Epoch: 071, Train_Loss: 1.6354,Test_Loss: 1.7652,Train_acc: 0.5000,Test_acc: 0.4994\n",
      "Epoch: 072, Train_Loss: 1.6309,Test_Loss: 1.7607,Train_acc: 0.4800,Test_acc: 0.4998\n",
      "Epoch: 073, Train_Loss: 1.6267,Test_Loss: 1.7611,Train_acc: 0.5100,Test_acc: 0.5006\n",
      "Epoch: 074, Train_Loss: 1.6218,Test_Loss: 1.7633,Train_acc: 0.5200,Test_acc: 0.5031\n",
      "Epoch: 075, Train_Loss: 1.6176,Test_Loss: 1.7608,Train_acc: 0.5100,Test_acc: 0.4998\n",
      "Epoch: 076, Train_Loss: 1.6130,Test_Loss: 1.7637,Train_acc: 0.5500,Test_acc: 0.5109\n",
      "Epoch: 077, Train_Loss: 1.6078,Test_Loss: 1.7627,Train_acc: 0.5500,Test_acc: 0.5188\n",
      "Epoch: 078, Train_Loss: 1.6029,Test_Loss: 1.7572,Train_acc: 0.5600,Test_acc: 0.5130\n",
      "Epoch: 079, Train_Loss: 1.5976,Test_Loss: 1.7612,Train_acc: 0.5500,Test_acc: 0.5134\n",
      "Epoch: 080, Train_Loss: 1.5919,Test_Loss: 1.7665,Train_acc: 0.5600,Test_acc: 0.5151\n",
      "Epoch: 081, Train_Loss: 1.5870,Test_Loss: 1.7629,Train_acc: 0.5600,Test_acc: 0.5146\n",
      "Epoch: 082, Train_Loss: 1.5810,Test_Loss: 1.7604,Train_acc: 0.5400,Test_acc: 0.5163\n",
      "Epoch: 083, Train_Loss: 1.5754,Test_Loss: 1.7708,Train_acc: 0.5600,Test_acc: 0.5196\n",
      "Epoch: 084, Train_Loss: 1.5699,Test_Loss: 1.7770,Train_acc: 0.5500,Test_acc: 0.5200\n",
      "Epoch: 085, Train_Loss: 1.5631,Test_Loss: 1.8033,Train_acc: 0.5400,Test_acc: 0.5134\n",
      "Epoch: 086, Train_Loss: 1.5569,Test_Loss: 1.8190,Train_acc: 0.5400,Test_acc: 0.5101\n",
      "Epoch: 087, Train_Loss: 1.5516,Test_Loss: 1.8751,Train_acc: 0.5400,Test_acc: 0.4965\n",
      "Epoch: 088, Train_Loss: 1.5467,Test_Loss: 1.8941,Train_acc: 0.5300,Test_acc: 0.4874\n",
      "Epoch: 089, Train_Loss: 1.5400,Test_Loss: 1.8987,Train_acc: 0.5100,Test_acc: 0.4767\n",
      "Epoch: 090, Train_Loss: 1.5336,Test_Loss: 1.8950,Train_acc: 0.4900,Test_acc: 0.4730\n",
      "Epoch: 091, Train_Loss: 1.5280,Test_Loss: 1.9063,Train_acc: 0.4800,Test_acc: 0.4614\n",
      "Epoch: 092, Train_Loss: 1.5224,Test_Loss: 1.9459,Train_acc: 0.4300,Test_acc: 0.4363\n",
      "Epoch: 093, Train_Loss: 1.5169,Test_Loss: 2.0006,Train_acc: 0.4000,Test_acc: 0.4252\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m train_loss \u001b[39m=\u001b[39m train(epoch)\n\u001b[1;32m     69\u001b[0m _, train_acc \u001b[39m=\u001b[39m test(train_loader)\n\u001b[0;32m---> 70\u001b[0m val_loss, val_acc \u001b[39m=\u001b[39m test(test_loader)\n\u001b[1;32m     71\u001b[0m \u001b[39mif\u001b[39;00m val_loss \u001b[39m<\u001b[39m best_val_loss:\n\u001b[1;32m     72\u001b[0m     test_loss, test_acc \u001b[39m=\u001b[39m test(test_loader)\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[28], line 54\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m     51\u001b[0m correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     52\u001b[0m loss_all \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 54\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m loader:\n\u001b[1;32m     55\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     56\u001b[0m     pred, mc_loss, o_loss \u001b[39m=\u001b[39m model(data)\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Projects/2023L-machine-learning-on-graphs/project_2/utils.py:137\u001b[0m, in \u001b[0;36mGraphDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m--> 137\u001b[0m     data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(path\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00midx\u001b[39m}\u001b[39;49;00m\u001b[39m.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    138\u001b[0m     data\u001b[39m.\u001b[39my \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels[idx] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    139\u001b[0m     data\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 809\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    810\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1174\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1142\u001b[0m     typed_storage \u001b[39m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1144\u001b[0m \u001b[39mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/serialization.py:1112\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[1;32m   1110\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1112\u001b[0m     storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39;49mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39;49mUntypedStorage)\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_untyped_storage\n\u001b[1;32m   1113\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m     typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[1;32m   1116\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1117\u001b[0m         dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   1118\u001b[0m         _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# def train():\n",
    "#     model.train()\n",
    "\n",
    "#     total_loss = 0\n",
    "#     for data in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         y_out,mc_loss, o_loss  = model(data)\n",
    "#         loss = F.nll_loss(y_out, data.y)+mc_loss+o_loss\n",
    "#         # print(F.nll_loss(y_out, data.y),\"and\",mc_loss+o_loss)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += float(loss) * data.num_graphs\n",
    "#     return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def test(loader):\n",
    "#     model.eval()\n",
    "#     y_pred = []\n",
    "#     y_true = []\n",
    "#     loss = 0\n",
    "#     for data in loader:\n",
    "#         y_out,mc_loss, o_loss = model(data)\n",
    "#         y_pred.append(y_out.argmax(dim=-1))\n",
    "#         y_true.append(data.y - 1)\n",
    "#         loss += float(F.nll_loss(y_out, data.y) * data.num_graphs + mc_loss + o_loss)\n",
    "#     y_pred = np.concatenate(y_pred)\n",
    "#     y_true = np.concatenate(y_true)\n",
    "#     return f1_score(y_true=y_true, y_pred=y_pred, average=\"macro\"), accuracy_score(y_true=y_true,y_pred=y_pred), loss / len(\n",
    "#             loader.dataset\n",
    "#         )\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out, mc_loss, o_loss = model(data)\n",
    "        loss = F.nll_loss(out, data.y.view(-1)) + mc_loss + o_loss\n",
    "        loss.backward()\n",
    "        loss_all += data.y.size(0) * float(loss)\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    loss_all = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred, mc_loss, o_loss = model(data)\n",
    "        loss = F.nll_loss(pred, data.y.view(-1)) + mc_loss + o_loss\n",
    "        loss_all += data.y.size(0) * float(loss)\n",
    "        correct += int(pred.max(dim=1)[1].eq(data.y.view(-1)).sum())\n",
    "\n",
    "    return loss_all / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "best_val_acc = test_acc = 0\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = start_patience = 50\n",
    "for epoch in range(1, 2000):\n",
    "    train_loss = train(epoch)\n",
    "    _, train_acc = test(train_loader)\n",
    "    val_loss, val_acc = test(test_loader)\n",
    "    if val_loss < best_val_loss:\n",
    "        test_loss, test_acc = test(test_loader)\n",
    "        best_val_acc = val_acc\n",
    "        patience = start_patience\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            break\n",
    "    print(\n",
    "        f\"Epoch: {epoch:03d}, Train_Loss: {train_loss:02.4f},Test_Loss: {test_loss:02.4f},Train_acc: {train_acc:01.4f},Test_acc: {test_acc:01.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train_Loss: 1.6379,Test_Loss: 1.6328,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 002, Train_Loss: 1.6367,Test_Loss: 1.6324,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 003, Train_Loss: 1.6358,Test_Loss: 1.6322,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 004, Train_Loss: 1.6348,Test_Loss: 1.6317,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 005, Train_Loss: 1.6342,Test_Loss: 1.6312,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 006, Train_Loss: 1.6336,Test_Loss: 1.6308,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 007, Train_Loss: 1.6332,Test_Loss: 1.6305,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 008, Train_Loss: 1.6328,Test_Loss: 1.6301,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 009, Train_Loss: 1.6324,Test_Loss: 1.6298,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 010, Train_Loss: 1.6321,Test_Loss: 1.6295,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 011, Train_Loss: 1.6317,Test_Loss: 1.6292,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 012, Train_Loss: 1.6313,Test_Loss: 1.6289,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 013, Train_Loss: 1.6309,Test_Loss: 1.6286,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 014, Train_Loss: 1.6304,Test_Loss: 1.6282,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 015, Train_Loss: 1.6298,Test_Loss: 1.6278,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 016, Train_Loss: 1.6291,Test_Loss: 1.6274,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 017, Train_Loss: 1.6284,Test_Loss: 1.6268,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 018, Train_Loss: 1.6276,Test_Loss: 1.6263,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 019, Train_Loss: 1.6269,Test_Loss: 1.6259,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 020, Train_Loss: 1.6263,Test_Loss: 1.6257,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 021, Train_Loss: 1.6259,Test_Loss: 1.6256,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 022, Train_Loss: 1.6254,Test_Loss: 1.6255,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 023, Train_Loss: 1.6250,Test_Loss: 1.6255,Train_f1: 0.0667,Test_f1: 0.0382,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 024, Train_Loss: 1.6245,Test_Loss: 1.6254,Train_f1: 0.0667,Test_f1: 0.0382,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 025, Train_Loss: 1.6240,Test_Loss: 1.6253,Train_f1: 0.0667,Test_f1: 0.0382,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 026, Train_Loss: 1.6234,Test_Loss: 1.6252,Train_f1: 0.0667,Test_f1: 0.0383,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 027, Train_Loss: 1.6228,Test_Loss: 1.6250,Train_f1: 0.0667,Test_f1: 0.0383,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 028, Train_Loss: 1.6222,Test_Loss: 1.6249,Train_f1: 0.0667,Test_f1: 0.0383,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 029, Train_Loss: 1.6216,Test_Loss: 1.6248,Train_f1: 0.0560,Test_f1: 0.0384,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 030, Train_Loss: 1.6210,Test_Loss: 1.6247,Train_f1: 0.0560,Test_f1: 0.0384,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 031, Train_Loss: 1.6204,Test_Loss: 1.6247,Train_f1: 0.0560,Test_f1: 0.0385,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 032, Train_Loss: 1.6199,Test_Loss: 1.6248,Train_f1: 0.0560,Test_f1: 0.0386,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 033, Train_Loss: 1.6194,Test_Loss: 1.6249,Train_f1: 0.0560,Test_f1: 0.0386,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 034, Train_Loss: 1.6187,Test_Loss: 1.6249,Train_f1: 0.0560,Test_f1: 0.0388,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 035, Train_Loss: 1.6180,Test_Loss: 1.6248,Train_f1: 0.0560,Test_f1: 0.0389,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 036, Train_Loss: 1.6171,Test_Loss: 1.6246,Train_f1: 0.0560,Test_f1: 0.0389,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 037, Train_Loss: 1.6160,Test_Loss: 1.6241,Train_f1: 0.0560,Test_f1: 0.0390,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 038, Train_Loss: 1.6149,Test_Loss: 1.6235,Train_f1: 0.0560,Test_f1: 0.0390,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 039, Train_Loss: 1.6137,Test_Loss: 1.6228,Train_f1: 0.0560,Test_f1: 0.0391,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 040, Train_Loss: 1.6126,Test_Loss: 1.6221,Train_f1: 0.0560,Test_f1: 0.0391,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 041, Train_Loss: 1.6116,Test_Loss: 1.6215,Train_f1: 0.0560,Test_f1: 0.0392,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 042, Train_Loss: 1.6105,Test_Loss: 1.6209,Train_f1: 0.0560,Test_f1: 0.0392,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 043, Train_Loss: 1.6093,Test_Loss: 1.6203,Train_f1: 0.0560,Test_f1: 0.0393,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 044, Train_Loss: 1.6079,Test_Loss: 1.6195,Train_f1: 0.0560,Test_f1: 0.0393,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 045, Train_Loss: 1.6063,Test_Loss: 1.6187,Train_f1: 0.0565,Test_f1: 0.0395,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 046, Train_Loss: 1.6045,Test_Loss: 1.6180,Train_f1: 0.0565,Test_f1: 0.0398,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 047, Train_Loss: 1.6025,Test_Loss: 1.6172,Train_f1: 0.0565,Test_f1: 0.0400,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 048, Train_Loss: 1.6004,Test_Loss: 1.6163,Train_f1: 0.0565,Test_f1: 0.0401,Train_acc: 0.2000,Test_acc: 0.1282\n",
      "Epoch: 049, Train_Loss: 1.5983,Test_Loss: 1.6155,Train_f1: 0.0565,Test_f1: 0.0398,Train_acc: 0.2000,Test_acc: 0.1262\n",
      "Epoch: 050, Train_Loss: 1.5961,Test_Loss: 1.6148,Train_f1: 0.0565,Test_f1: 0.0394,Train_acc: 0.2000,Test_acc: 0.1233\n",
      "Epoch: 051, Train_Loss: 1.5941,Test_Loss: 1.6144,Train_f1: 0.0522,Test_f1: 0.0378,Train_acc: 0.1800,Test_acc: 0.1159\n",
      "Epoch: 052, Train_Loss: 1.5926,Test_Loss: 1.6142,Train_f1: 0.0511,Test_f1: 0.0361,Train_acc: 0.1700,Test_acc: 0.1085\n",
      "Epoch: 053, Train_Loss: 1.5915,Test_Loss: 1.6144,Train_f1: 0.0515,Test_f1: 0.0341,Train_acc: 0.1700,Test_acc: 0.1002\n",
      "Epoch: 054, Train_Loss: 1.5906,Test_Loss: 1.6146,Train_f1: 0.0449,Test_f1: 0.0294,Train_acc: 0.1400,Test_acc: 0.0829\n",
      "Epoch: 055, Train_Loss: 1.5895,Test_Loss: 1.6148,Train_f1: 0.0374,Test_f1: 0.0250,Train_acc: 0.1100,Test_acc: 0.0676\n",
      "Epoch: 056, Train_Loss: 1.5879,Test_Loss: 1.6143,Train_f1: 0.0319,Test_f1: 0.0211,Train_acc: 0.0900,Test_acc: 0.0553\n",
      "Epoch: 057, Train_Loss: 1.5857,Test_Loss: 1.6132,Train_f1: 0.0227,Test_f1: 0.0191,Train_acc: 0.0600,Test_acc: 0.0487\n",
      "Epoch: 058, Train_Loss: 1.5827,Test_Loss: 1.6114,Train_f1: 0.0227,Test_f1: 0.0168,Train_acc: 0.0600,Test_acc: 0.0421\n",
      "Epoch: 059, Train_Loss: 1.5799,Test_Loss: 1.6096,Train_f1: 0.0196,Test_f1: 0.0155,Train_acc: 0.0500,Test_acc: 0.0384\n",
      "Epoch: 060, Train_Loss: 1.5773,Test_Loss: 1.6081,Train_f1: 0.0161,Test_f1: 0.0144,Train_acc: 0.0400,Test_acc: 0.0351\n",
      "Epoch: 061, Train_Loss: 1.5752,Test_Loss: 1.6071,Train_f1: 0.0167,Test_f1: 0.0134,Train_acc: 0.0400,Test_acc: 0.0322\n",
      "Epoch: 062, Train_Loss: 1.5729,Test_Loss: 1.6059,Train_f1: 0.0167,Test_f1: 0.0121,Train_acc: 0.0400,Test_acc: 0.0289\n",
      "Epoch: 063, Train_Loss: 1.5701,Test_Loss: 1.6044,Train_f1: 0.0167,Test_f1: 0.0115,Train_acc: 0.0400,Test_acc: 0.0272\n",
      "Epoch: 064, Train_Loss: 1.5657,Test_Loss: 1.6016,Train_f1: 0.0175,Test_f1: 0.0133,Train_acc: 0.0400,Test_acc: 0.0285\n",
      "Epoch: 065, Train_Loss: 1.5613,Test_Loss: 1.5984,Train_f1: 0.0533,Test_f1: 0.0262,Train_acc: 0.0700,Test_acc: 0.0396\n",
      "Epoch: 066, Train_Loss: 1.5569,Test_Loss: 1.5948,Train_f1: 0.0410,Test_f1: 0.0291,Train_acc: 0.0600,Test_acc: 0.0458\n",
      "Epoch: 067, Train_Loss: 1.5527,Test_Loss: 1.5914,Train_f1: 0.0452,Test_f1: 0.0322,Train_acc: 0.0700,Test_acc: 0.0499\n",
      "Epoch: 068, Train_Loss: 1.5491,Test_Loss: 1.5882,Train_f1: 0.0545,Test_f1: 0.0345,Train_acc: 0.0800,Test_acc: 0.0524\n",
      "Epoch: 069, Train_Loss: 1.5465,Test_Loss: 1.5855,Train_f1: 0.0550,Test_f1: 0.0372,Train_acc: 0.0800,Test_acc: 0.0557\n",
      "Epoch: 070, Train_Loss: 1.5452,Test_Loss: 1.5834,Train_f1: 0.0483,Test_f1: 0.0368,Train_acc: 0.0800,Test_acc: 0.0553\n",
      "Epoch: 071, Train_Loss: 1.5436,Test_Loss: 1.5804,Train_f1: 0.0488,Test_f1: 0.0365,Train_acc: 0.0800,Test_acc: 0.0553\n",
      "Epoch: 072, Train_Loss: 1.5414,Test_Loss: 1.5768,Train_f1: 0.0528,Test_f1: 0.0367,Train_acc: 0.0900,Test_acc: 0.0548\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m501\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     train()\n\u001b[1;32m      3\u001b[0m     train_macro_f1,train_acc, train_loss \u001b[39m=\u001b[39m test(train_loader)\n\u001b[1;32m      4\u001b[0m     test_macro_f1,test_acc, test_loss \u001b[39m=\u001b[39m test(test_loader)\n",
      "Cell \u001b[0;32mIn[26], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mnll_loss(y_out, data\u001b[39m.\u001b[39my)\u001b[39m+\u001b[39mmc_loss\u001b[39m+\u001b[39mo_loss\n\u001b[1;32m      9\u001b[0m \u001b[39m# print(F.nll_loss(y_out, data.y),\"and\",mc_loss+o_loss)\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     12\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(loss) \u001b[39m*\u001b[39m data\u001b[39m.\u001b[39mnum_graphs\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 501):\n",
    "    train()\n",
    "    train_macro_f1, train_acc, train_loss = test(train_loader)\n",
    "    test_macro_f1, test_acc, test_loss = test(test_loader)\n",
    "    scheduler.step(train_loss)\n",
    "    print(\n",
    "        f\"Epoch: {epoch:03d}, Train_Loss: {train_loss:02.4f},Test_Loss: {test_loss:02.4f},Train_f1: {train_macro_f1:01.4f},Test_f1: {test_macro_f1:01.4f},Train_acc: {train_acc:01.4f},Test_acc: {test_acc:01.4f}\"\n",
    "    )\n",
    "    if test_macro_f1 > best_macro_f1:\n",
    "        best_accuracy = test_macro_f1\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), \"../data/best_model.pth\")\n",
    "    # elif epoch - best_epoch > early_stop_thresh:\n",
    "    #     print(\n",
    "    #         f\"Early stopped training at epoch {epoch} best macro F1 {best_macro_f1} in {best_epoch}\"\n",
    "    #     )\n",
    "    #     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
