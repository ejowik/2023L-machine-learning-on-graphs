{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch: 001, Train_Loss: 2.1947,Test_Loss: 2.1892,Train_acc: 0.2000,Test_acc: 0.1889\n",
      "Epoch: 002, Train_Loss: 2.1853,Test_Loss: 2.1889,Train_acc: 0.2000,Test_acc: 0.1889\n",
      "Epoch: 003, Train_Loss: 2.1854,Test_Loss: 2.1915,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 004, Train_Loss: 2.1823,Test_Loss: 2.1931,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 005, Train_Loss: 2.1827,Test_Loss: 2.1940,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 006, Train_Loss: 2.1815,Test_Loss: 2.1912,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 007, Train_Loss: 2.1800,Test_Loss: 2.1887,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 008, Train_Loss: 2.1795,Test_Loss: 2.1861,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 009, Train_Loss: 2.1795,Test_Loss: 2.1813,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 010, Train_Loss: 2.1785,Test_Loss: 2.1790,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 011, Train_Loss: 2.1776,Test_Loss: 2.1790,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 012, Train_Loss: 2.1758,Test_Loss: 2.1795,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 013, Train_Loss: 2.1752,Test_Loss: 2.1761,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 014, Train_Loss: 2.1734,Test_Loss: 2.1723,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 015, Train_Loss: 2.1730,Test_Loss: 2.1699,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 016, Train_Loss: 2.1704,Test_Loss: 2.1696,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 017, Train_Loss: 2.1690,Test_Loss: 2.1673,Train_acc: 0.2000,Test_acc: 0.1761\n",
      "Epoch: 018, Train_Loss: 2.1670,Test_Loss: 2.1646,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 019, Train_Loss: 2.1654,Test_Loss: 2.1630,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 020, Train_Loss: 2.1635,Test_Loss: 2.1625,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 021, Train_Loss: 2.1621,Test_Loss: 2.1619,Train_acc: 0.2000,Test_acc: 0.1744\n",
      "Epoch: 022, Train_Loss: 2.1612,Test_Loss: 2.1607,Train_acc: 0.2800,Test_acc: 0.2260\n",
      "Epoch: 023, Train_Loss: 2.1594,Test_Loss: 2.1625,Train_acc: 0.2100,Test_acc: 0.1868\n",
      "Epoch: 024, Train_Loss: 2.1575,Test_Loss: 2.1630,Train_acc: 0.2000,Test_acc: 0.1769\n",
      "Epoch: 025, Train_Loss: 2.1564,Test_Loss: 2.1623,Train_acc: 0.2900,Test_acc: 0.2342\n",
      "Epoch: 026, Train_Loss: 2.1544,Test_Loss: 2.1621,Train_acc: 0.3100,Test_acc: 0.2854\n",
      "Epoch: 027, Train_Loss: 2.1535,Test_Loss: 2.1613,Train_acc: 0.3100,Test_acc: 0.2614\n",
      "Epoch: 028, Train_Loss: 2.1511,Test_Loss: 2.1588,Train_acc: 0.2000,Test_acc: 0.1843\n",
      "Epoch: 029, Train_Loss: 2.1494,Test_Loss: 2.1570,Train_acc: 0.2000,Test_acc: 0.1748\n",
      "Epoch: 030, Train_Loss: 2.1480,Test_Loss: 2.1571,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 031, Train_Loss: 2.1456,Test_Loss: 2.1512,Train_acc: 0.2000,Test_acc: 0.1724\n",
      "Epoch: 032, Train_Loss: 2.1435,Test_Loss: 2.1433,Train_acc: 0.2500,Test_acc: 0.2672\n",
      "Epoch: 033, Train_Loss: 2.1428,Test_Loss: 2.1344,Train_acc: 0.2000,Test_acc: 0.2697\n",
      "Epoch: 034, Train_Loss: 2.1430,Test_Loss: 2.1277,Train_acc: 0.2000,Test_acc: 0.2697\n",
      "Epoch: 035, Train_Loss: 2.1440,Test_Loss: 2.1251,Train_acc: 0.2000,Test_acc: 0.2697\n",
      "Epoch: 036, Train_Loss: 2.1396,Test_Loss: 2.1323,Train_acc: 0.3500,Test_acc: 0.4012\n",
      "Epoch: 037, Train_Loss: 2.1371,Test_Loss: 2.1414,Train_acc: 0.2000,Test_acc: 0.1328\n",
      "Epoch: 038, Train_Loss: 2.1356,Test_Loss: 2.1379,Train_acc: 0.2100,Test_acc: 0.1526\n",
      "Epoch: 039, Train_Loss: 2.1351,Test_Loss: 2.1271,Train_acc: 0.3800,Test_acc: 0.3934\n",
      "Epoch: 040, Train_Loss: 2.1325,Test_Loss: 2.1328,Train_acc: 0.2800,Test_acc: 0.2342\n",
      "Epoch: 041, Train_Loss: 2.1289,Test_Loss: 2.1383,Train_acc: 0.2000,Test_acc: 0.1299\n",
      "Epoch: 042, Train_Loss: 2.1268,Test_Loss: 2.1301,Train_acc: 0.2400,Test_acc: 0.1765\n",
      "Epoch: 043, Train_Loss: 2.1244,Test_Loss: 2.1169,Train_acc: 0.3100,Test_acc: 0.4132\n",
      "Epoch: 044, Train_Loss: 2.1236,Test_Loss: 2.1143,Train_acc: 0.2300,Test_acc: 0.3365\n",
      "Epoch: 045, Train_Loss: 2.1202,Test_Loss: 2.1266,Train_acc: 0.3300,Test_acc: 0.2305\n",
      "Epoch: 046, Train_Loss: 2.1188,Test_Loss: 2.1343,Train_acc: 0.3100,Test_acc: 0.2082\n",
      "Epoch: 047, Train_Loss: 2.1189,Test_Loss: 2.1352,Train_acc: 0.2800,Test_acc: 0.1757\n",
      "Epoch: 048, Train_Loss: 2.1165,Test_Loss: 2.1252,Train_acc: 0.3200,Test_acc: 0.2120\n",
      "Epoch: 049, Train_Loss: 2.1094,Test_Loss: 2.1011,Train_acc: 0.2000,Test_acc: 0.3085\n",
      "Epoch: 050, Train_Loss: 2.1207,Test_Loss: 2.0984,Train_acc: 0.2400,Test_acc: 0.3184\n",
      "Epoch: 051, Train_Loss: 2.1294,Test_Loss: 2.0944,Train_acc: 0.2000,Test_acc: 0.3126\n",
      "Epoch: 052, Train_Loss: 2.1105,Test_Loss: 2.1006,Train_acc: 0.2200,Test_acc: 0.1773\n",
      "Epoch: 053, Train_Loss: 2.0989,Test_Loss: 2.1115,Train_acc: 0.3500,Test_acc: 0.2507\n",
      "Epoch: 054, Train_Loss: 2.1015,Test_Loss: 2.1205,Train_acc: 0.3000,Test_acc: 0.2305\n",
      "Epoch: 055, Train_Loss: 2.0974,Test_Loss: 2.1017,Train_acc: 0.3200,Test_acc: 0.2689\n",
      "Epoch: 056, Train_Loss: 2.0884,Test_Loss: 2.0822,Train_acc: 0.3200,Test_acc: 0.3744\n",
      "Epoch: 057, Train_Loss: 2.0805,Test_Loss: 2.0725,Train_acc: 0.3300,Test_acc: 0.4062\n",
      "Epoch: 058, Train_Loss: 2.0738,Test_Loss: 2.0636,Train_acc: 0.3300,Test_acc: 0.4095\n",
      "Epoch: 059, Train_Loss: 2.0663,Test_Loss: 2.0550,Train_acc: 0.3400,Test_acc: 0.4140\n",
      "Epoch: 060, Train_Loss: 2.0508,Test_Loss: 2.0266,Train_acc: 0.2700,Test_acc: 0.3402\n",
      "Epoch: 061, Train_Loss: 2.0533,Test_Loss: 2.0124,Train_acc: 0.2500,Test_acc: 0.3200\n",
      "Epoch: 062, Train_Loss: 2.0314,Test_Loss: 2.0308,Train_acc: 0.3100,Test_acc: 0.3320\n",
      "Epoch: 063, Train_Loss: 2.0164,Test_Loss: 1.9941,Train_acc: 0.3600,Test_acc: 0.3934\n",
      "Epoch: 064, Train_Loss: 1.9853,Test_Loss: 1.9568,Train_acc: 0.3700,Test_acc: 0.3856\n",
      "Epoch: 065, Train_Loss: 1.9811,Test_Loss: 1.9407,Train_acc: 0.2700,Test_acc: 0.3546\n",
      "Epoch: 066, Train_Loss: 1.9764,Test_Loss: 1.9139,Train_acc: 0.4900,Test_acc: 0.4849\n",
      "Epoch: 067, Train_Loss: 1.9325,Test_Loss: 1.9152,Train_acc: 0.4500,Test_acc: 0.4635\n",
      "Epoch: 068, Train_Loss: 1.9113,Test_Loss: 1.8815,Train_acc: 0.3600,Test_acc: 0.3773\n",
      "Epoch: 069, Train_Loss: 1.8822,Test_Loss: 1.9026,Train_acc: 0.3000,Test_acc: 0.2169\n",
      "Epoch: 070, Train_Loss: 1.8844,Test_Loss: 1.8602,Train_acc: 0.3500,Test_acc: 0.2907\n",
      "Epoch: 071, Train_Loss: 1.8561,Test_Loss: 1.8352,Train_acc: 0.3300,Test_acc: 0.2668\n",
      "Epoch: 072, Train_Loss: 1.8463,Test_Loss: 1.8392,Train_acc: 0.3300,Test_acc: 0.2474\n",
      "Epoch: 073, Train_Loss: 1.8975,Test_Loss: 1.8342,Train_acc: 0.4100,Test_acc: 0.3208\n",
      "Epoch: 074, Train_Loss: 1.8368,Test_Loss: 1.8196,Train_acc: 0.3400,Test_acc: 0.4008\n",
      "Epoch: 075, Train_Loss: 1.8172,Test_Loss: 1.8584,Train_acc: 0.3300,Test_acc: 0.4025\n",
      "Epoch: 076, Train_Loss: 1.8663,Test_Loss: 1.7995,Train_acc: 0.3300,Test_acc: 0.4532\n",
      "Epoch: 077, Train_Loss: 1.7922,Test_Loss: 1.8170,Train_acc: 0.4000,Test_acc: 0.4458\n",
      "Epoch: 078, Train_Loss: 1.8324,Test_Loss: 1.8158,Train_acc: 0.3200,Test_acc: 0.3167\n",
      "Epoch: 079, Train_Loss: 1.9033,Test_Loss: 1.9370,Train_acc: 0.3400,Test_acc: 0.2792\n",
      "Epoch: 080, Train_Loss: 1.8853,Test_Loss: 1.7733,Train_acc: 0.3500,Test_acc: 0.3753\n",
      "Epoch: 081, Train_Loss: 1.8070,Test_Loss: 1.7693,Train_acc: 0.4000,Test_acc: 0.4029\n",
      "Epoch: 082, Train_Loss: 1.8037,Test_Loss: 1.7731,Train_acc: 0.4000,Test_acc: 0.4157\n",
      "Epoch: 083, Train_Loss: 1.7918,Test_Loss: 1.7652,Train_acc: 0.3600,Test_acc: 0.3715\n",
      "Epoch: 084, Train_Loss: 1.7881,Test_Loss: 1.7627,Train_acc: 0.3600,Test_acc: 0.3724\n",
      "Epoch: 085, Train_Loss: 1.7721,Test_Loss: 1.7828,Train_acc: 0.3500,Test_acc: 0.3682\n",
      "Epoch: 086, Train_Loss: 1.7929,Test_Loss: 1.7603,Train_acc: 0.3700,Test_acc: 0.5031\n",
      "Epoch: 087, Train_Loss: 1.7815,Test_Loss: 1.7814,Train_acc: 0.3600,Test_acc: 0.4619\n",
      "Epoch: 088, Train_Loss: 1.8356,Test_Loss: 1.7541,Train_acc: 0.3900,Test_acc: 0.3901\n",
      "Epoch: 089, Train_Loss: 1.7811,Test_Loss: 1.8644,Train_acc: 0.3400,Test_acc: 0.3200\n",
      "Epoch: 090, Train_Loss: 1.8138,Test_Loss: 1.7792,Train_acc: 0.3700,Test_acc: 0.3546\n",
      "Epoch: 091, Train_Loss: 1.8030,Test_Loss: 1.8039,Train_acc: 0.3900,Test_acc: 0.3353\n",
      "Epoch: 092, Train_Loss: 1.7682,Test_Loss: 1.8215,Train_acc: 0.3300,Test_acc: 0.3130\n",
      "Epoch: 093, Train_Loss: 1.7959,Test_Loss: 1.7908,Train_acc: 0.3200,Test_acc: 0.3551\n",
      "Epoch: 094, Train_Loss: 1.7692,Test_Loss: 1.7586,Train_acc: 0.4000,Test_acc: 0.4219\n",
      "Epoch: 095, Train_Loss: 1.7771,Test_Loss: 1.7574,Train_acc: 0.3900,Test_acc: 0.4462\n",
      "Epoch: 096, Train_Loss: 1.7585,Test_Loss: 1.7534,Train_acc: 0.4300,Test_acc: 0.5085\n",
      "Epoch: 097, Train_Loss: 1.7795,Test_Loss: 1.7884,Train_acc: 0.3600,Test_acc: 0.4784\n",
      "Epoch: 098, Train_Loss: 1.7794,Test_Loss: 1.7303,Train_acc: 0.4400,Test_acc: 0.5225\n",
      "Epoch: 099, Train_Loss: 1.7559,Test_Loss: 1.7317,Train_acc: 0.3600,Test_acc: 0.5163\n",
      "Epoch: 100, Train_Loss: 1.7591,Test_Loss: 1.7540,Train_acc: 0.5500,Test_acc: 0.5633\n",
      "Epoch: 101, Train_Loss: 1.7566,Test_Loss: 1.7571,Train_acc: 0.5200,Test_acc: 0.4557\n",
      "Epoch: 102, Train_Loss: 1.7464,Test_Loss: 1.7452,Train_acc: 0.5000,Test_acc: 0.4697\n",
      "Epoch: 103, Train_Loss: 1.7405,Test_Loss: 1.7565,Train_acc: 0.4600,Test_acc: 0.4421\n",
      "Epoch: 104, Train_Loss: 1.7430,Test_Loss: 1.7540,Train_acc: 0.4300,Test_acc: 0.4301\n",
      "Epoch: 105, Train_Loss: 1.7420,Test_Loss: 1.7685,Train_acc: 0.3900,Test_acc: 0.4701\n",
      "Epoch: 106, Train_Loss: 1.7602,Test_Loss: 1.7974,Train_acc: 0.3800,Test_acc: 0.4705\n",
      "Epoch: 107, Train_Loss: 1.8050,Test_Loss: 1.7812,Train_acc: 0.3700,Test_acc: 0.4672\n",
      "Epoch: 108, Train_Loss: 1.7518,Test_Loss: 1.7598,Train_acc: 0.4000,Test_acc: 0.4808\n",
      "Epoch: 109, Train_Loss: 1.8008,Test_Loss: 1.7428,Train_acc: 0.4400,Test_acc: 0.4606\n",
      "Epoch: 110, Train_Loss: 1.7401,Test_Loss: 1.7782,Train_acc: 0.4000,Test_acc: 0.4656\n",
      "Epoch: 111, Train_Loss: 1.7795,Test_Loss: 1.7729,Train_acc: 0.3700,Test_acc: 0.4388\n",
      "Epoch: 112, Train_Loss: 1.7483,Test_Loss: 1.7529,Train_acc: 0.4100,Test_acc: 0.3757\n",
      "Epoch: 113, Train_Loss: 1.7412,Test_Loss: 1.7849,Train_acc: 0.4000,Test_acc: 0.3336\n",
      "Epoch: 114, Train_Loss: 1.7814,Test_Loss: 1.8255,Train_acc: 0.3500,Test_acc: 0.2779\n",
      "Epoch: 115, Train_Loss: 1.7636,Test_Loss: 1.7853,Train_acc: 0.3500,Test_acc: 0.3126\n",
      "Epoch: 116, Train_Loss: 1.7505,Test_Loss: 1.7728,Train_acc: 0.4000,Test_acc: 0.3493\n",
      "Epoch: 117, Train_Loss: 1.7384,Test_Loss: 1.7674,Train_acc: 0.5300,Test_acc: 0.4355\n",
      "Epoch: 118, Train_Loss: 1.7579,Test_Loss: 1.7775,Train_acc: 0.4500,Test_acc: 0.4214\n",
      "Epoch: 119, Train_Loss: 1.7764,Test_Loss: 1.7635,Train_acc: 0.4100,Test_acc: 0.4639\n",
      "Epoch: 120, Train_Loss: 1.7538,Test_Loss: 1.7359,Train_acc: 0.4400,Test_acc: 0.4396\n",
      "Epoch: 121, Train_Loss: 1.7397,Test_Loss: 1.7447,Train_acc: 0.4000,Test_acc: 0.4181\n",
      "Epoch: 122, Train_Loss: 1.7346,Test_Loss: 1.7505,Train_acc: 0.4000,Test_acc: 0.4070\n",
      "Epoch: 123, Train_Loss: 1.7402,Test_Loss: 1.7620,Train_acc: 0.4100,Test_acc: 0.3893\n",
      "Epoch: 124, Train_Loss: 1.7453,Test_Loss: 1.7474,Train_acc: 0.4300,Test_acc: 0.4334\n",
      "Epoch: 125, Train_Loss: 1.7350,Test_Loss: 1.7367,Train_acc: 0.4900,Test_acc: 0.4544\n",
      "Epoch: 126, Train_Loss: 1.7254,Test_Loss: 1.7552,Train_acc: 0.4400,Test_acc: 0.4375\n",
      "Epoch: 127, Train_Loss: 1.7582,Test_Loss: 1.7313,Train_acc: 0.5200,Test_acc: 0.4788\n",
      "Epoch: 128, Train_Loss: 1.7283,Test_Loss: 1.7365,Train_acc: 0.5200,Test_acc: 0.5118\n",
      "Epoch: 129, Train_Loss: 1.7264,Test_Loss: 1.7284,Train_acc: 0.4800,Test_acc: 0.4953\n",
      "Epoch: 130, Train_Loss: 1.7329,Test_Loss: 1.7331,Train_acc: 0.5400,Test_acc: 0.5336\n",
      "Epoch: 131, Train_Loss: 1.7639,Test_Loss: 1.8145,Train_acc: 0.4900,Test_acc: 0.4738\n",
      "Epoch: 132, Train_Loss: 1.7678,Test_Loss: 1.7387,Train_acc: 0.4500,Test_acc: 0.4388\n",
      "Epoch: 133, Train_Loss: 1.7393,Test_Loss: 1.7494,Train_acc: 0.4700,Test_acc: 0.4247\n",
      "Epoch: 134, Train_Loss: 1.7162,Test_Loss: 1.7573,Train_acc: 0.4600,Test_acc: 0.4214\n",
      "Epoch: 135, Train_Loss: 1.7530,Test_Loss: 1.7499,Train_acc: 0.4900,Test_acc: 0.4416\n",
      "Epoch: 136, Train_Loss: 1.7275,Test_Loss: 1.7431,Train_acc: 0.4600,Test_acc: 0.4392\n",
      "Epoch: 137, Train_Loss: 1.7441,Test_Loss: 1.7223,Train_acc: 0.4900,Test_acc: 0.4796\n",
      "Epoch: 138, Train_Loss: 1.7195,Test_Loss: 1.7223,Train_acc: 0.5100,Test_acc: 0.4973\n",
      "Epoch: 139, Train_Loss: 1.7176,Test_Loss: 1.7299,Train_acc: 0.5300,Test_acc: 0.4742\n",
      "Epoch: 140, Train_Loss: 1.7215,Test_Loss: 1.7364,Train_acc: 0.5200,Test_acc: 0.4577\n",
      "Epoch: 141, Train_Loss: 1.7220,Test_Loss: 1.7363,Train_acc: 0.4900,Test_acc: 0.4441\n",
      "Epoch: 142, Train_Loss: 1.7185,Test_Loss: 1.7392,Train_acc: 0.5200,Test_acc: 0.4569\n",
      "Epoch: 143, Train_Loss: 1.7126,Test_Loss: 1.7383,Train_acc: 0.4800,Test_acc: 0.4404\n",
      "Epoch: 144, Train_Loss: 1.7313,Test_Loss: 1.7286,Train_acc: 0.5100,Test_acc: 0.4544\n",
      "Epoch: 145, Train_Loss: 1.7277,Test_Loss: 1.7269,Train_acc: 0.5100,Test_acc: 0.4544\n",
      "Epoch: 146, Train_Loss: 1.7179,Test_Loss: 1.7301,Train_acc: 0.5100,Test_acc: 0.4524\n",
      "Epoch: 147, Train_Loss: 1.7030,Test_Loss: 1.7364,Train_acc: 0.4700,Test_acc: 0.4812\n",
      "Epoch: 148, Train_Loss: 1.7388,Test_Loss: 1.7378,Train_acc: 0.5000,Test_acc: 0.5134\n",
      "Epoch: 149, Train_Loss: 1.7228,Test_Loss: 1.7071,Train_acc: 0.5500,Test_acc: 0.5538\n",
      "Epoch: 150, Train_Loss: 1.7109,Test_Loss: 1.7034,Train_acc: 0.5400,Test_acc: 0.5571\n",
      "Epoch: 151, Train_Loss: 1.7145,Test_Loss: 1.7048,Train_acc: 0.5100,Test_acc: 0.5551\n",
      "Epoch: 152, Train_Loss: 1.7166,Test_Loss: 1.7070,Train_acc: 0.5200,Test_acc: 0.5530\n",
      "Epoch: 153, Train_Loss: 1.7064,Test_Loss: 1.7068,Train_acc: 0.4700,Test_acc: 0.5365\n",
      "Epoch: 154, Train_Loss: 1.7039,Test_Loss: 1.7327,Train_acc: 0.4800,Test_acc: 0.5221\n",
      "Epoch: 155, Train_Loss: 1.8083,Test_Loss: 1.8399,Train_acc: 0.3600,Test_acc: 0.4309\n",
      "Epoch: 156, Train_Loss: 1.7737,Test_Loss: 1.7244,Train_acc: 0.4400,Test_acc: 0.4548\n",
      "Epoch: 157, Train_Loss: 1.8224,Test_Loss: 1.7550,Train_acc: 0.4100,Test_acc: 0.4111\n",
      "Epoch: 158, Train_Loss: 1.7688,Test_Loss: 1.8338,Train_acc: 0.3800,Test_acc: 0.4124\n",
      "Epoch: 159, Train_Loss: 1.8450,Test_Loss: 1.8249,Train_acc: 0.4000,Test_acc: 0.4194\n",
      "Epoch: 160, Train_Loss: 1.7918,Test_Loss: 1.7400,Train_acc: 0.4400,Test_acc: 0.4239\n",
      "Epoch: 161, Train_Loss: 1.7469,Test_Loss: 1.7734,Train_acc: 0.4000,Test_acc: 0.3431\n",
      "Epoch: 162, Train_Loss: 1.7639,Test_Loss: 1.7555,Train_acc: 0.4200,Test_acc: 0.3509\n",
      "Epoch: 163, Train_Loss: 1.7299,Test_Loss: 1.7422,Train_acc: 0.4400,Test_acc: 0.4474\n",
      "Epoch: 164, Train_Loss: 1.7484,Test_Loss: 1.7733,Train_acc: 0.4200,Test_acc: 0.4643\n",
      "Epoch: 165, Train_Loss: 1.7740,Test_Loss: 1.7570,Train_acc: 0.5100,Test_acc: 0.5002\n",
      "Epoch: 166, Train_Loss: 1.7442,Test_Loss: 1.7214,Train_acc: 0.4600,Test_acc: 0.4734\n",
      "Epoch: 167, Train_Loss: 1.7084,Test_Loss: 1.7401,Train_acc: 0.4400,Test_acc: 0.4000\n",
      "Epoch: 168, Train_Loss: 1.7406,Test_Loss: 1.7312,Train_acc: 0.4500,Test_acc: 0.4136\n",
      "Epoch: 169, Train_Loss: 1.7225,Test_Loss: 1.7167,Train_acc: 0.5100,Test_acc: 0.5014\n",
      "Epoch: 170, Train_Loss: 1.7055,Test_Loss: 1.7137,Train_acc: 0.4600,Test_acc: 0.4676\n",
      "Epoch: 171, Train_Loss: 1.7075,Test_Loss: 1.7239,Train_acc: 0.4200,Test_acc: 0.4553\n",
      "Epoch: 172, Train_Loss: 1.7263,Test_Loss: 1.7046,Train_acc: 0.4700,Test_acc: 0.5229\n",
      "Epoch: 173, Train_Loss: 1.7148,Test_Loss: 1.7130,Train_acc: 0.5400,Test_acc: 0.5406\n",
      "Epoch: 174, Train_Loss: 1.7239,Test_Loss: 1.7021,Train_acc: 0.5200,Test_acc: 0.5480\n",
      "Epoch: 175, Train_Loss: 1.7116,Test_Loss: 1.6952,Train_acc: 0.5000,Test_acc: 0.5472\n",
      "Epoch: 176, Train_Loss: 1.7048,Test_Loss: 1.7002,Train_acc: 0.5300,Test_acc: 0.5538\n",
      "Epoch: 177, Train_Loss: 1.7111,Test_Loss: 1.6978,Train_acc: 0.5500,Test_acc: 0.5534\n",
      "Epoch: 178, Train_Loss: 1.6924,Test_Loss: 1.7153,Train_acc: 0.4000,Test_acc: 0.4986\n",
      "Epoch: 179, Train_Loss: 1.7334,Test_Loss: 1.7047,Train_acc: 0.4500,Test_acc: 0.5031\n",
      "Epoch: 180, Train_Loss: 1.6946,Test_Loss: 1.7043,Train_acc: 0.5200,Test_acc: 0.5200\n",
      "Epoch: 181, Train_Loss: 1.7120,Test_Loss: 1.7172,Train_acc: 0.5000,Test_acc: 0.4965\n",
      "Epoch: 182, Train_Loss: 1.7024,Test_Loss: 1.7099,Train_acc: 0.5200,Test_acc: 0.4676\n",
      "Epoch: 183, Train_Loss: 1.6905,Test_Loss: 1.7195,Train_acc: 0.4800,Test_acc: 0.4561\n",
      "Epoch: 184, Train_Loss: 1.7128,Test_Loss: 1.6987,Train_acc: 0.5100,Test_acc: 0.5146\n",
      "Epoch: 185, Train_Loss: 1.6996,Test_Loss: 1.7251,Train_acc: 0.4100,Test_acc: 0.5027\n",
      "Epoch: 186, Train_Loss: 1.7477,Test_Loss: 1.7178,Train_acc: 0.4400,Test_acc: 0.5014\n",
      "Epoch: 187, Train_Loss: 1.7190,Test_Loss: 1.6905,Train_acc: 0.5100,Test_acc: 0.5452\n",
      "Epoch: 188, Train_Loss: 1.7343,Test_Loss: 1.7174,Train_acc: 0.4300,Test_acc: 0.4924\n",
      "Epoch: 189, Train_Loss: 1.6932,Test_Loss: 1.7177,Train_acc: 0.5000,Test_acc: 0.5019\n",
      "Epoch: 190, Train_Loss: 1.7164,Test_Loss: 1.7379,Train_acc: 0.4900,Test_acc: 0.4474\n",
      "Epoch: 191, Train_Loss: 1.7040,Test_Loss: 1.7406,Train_acc: 0.4500,Test_acc: 0.4132\n",
      "Epoch: 192, Train_Loss: 1.6928,Test_Loss: 1.7320,Train_acc: 0.4700,Test_acc: 0.4309\n",
      "Epoch: 193, Train_Loss: 1.6871,Test_Loss: 1.7243,Train_acc: 0.5000,Test_acc: 0.4606\n",
      "Epoch: 194, Train_Loss: 1.7041,Test_Loss: 1.7302,Train_acc: 0.5300,Test_acc: 0.4623\n",
      "Epoch: 195, Train_Loss: 1.6909,Test_Loss: 1.7192,Train_acc: 0.4800,Test_acc: 0.4577\n",
      "Epoch: 196, Train_Loss: 1.7021,Test_Loss: 1.7062,Train_acc: 0.5200,Test_acc: 0.4829\n",
      "Epoch: 197, Train_Loss: 1.6933,Test_Loss: 1.7094,Train_acc: 0.5400,Test_acc: 0.5386\n",
      "Epoch: 198, Train_Loss: 1.6903,Test_Loss: 1.6913,Train_acc: 0.4800,Test_acc: 0.5406\n",
      "Epoch: 199, Train_Loss: 1.6957,Test_Loss: 1.6856,Train_acc: 0.5400,Test_acc: 0.5588\n",
      "Epoch: 200, Train_Loss: 1.7027,Test_Loss: 1.7072,Train_acc: 0.5300,Test_acc: 0.5216\n",
      "Epoch: 201, Train_Loss: 1.6889,Test_Loss: 1.7213,Train_acc: 0.5300,Test_acc: 0.4449\n",
      "Epoch: 202, Train_Loss: 1.6808,Test_Loss: 1.7274,Train_acc: 0.5100,Test_acc: 0.4392\n",
      "Epoch: 203, Train_Loss: 1.7007,Test_Loss: 1.7397,Train_acc: 0.4800,Test_acc: 0.4363\n",
      "Epoch: 204, Train_Loss: 1.7151,Test_Loss: 1.7380,Train_acc: 0.4400,Test_acc: 0.4421\n",
      "Epoch: 205, Train_Loss: 1.6984,Test_Loss: 1.7209,Train_acc: 0.4300,Test_acc: 0.4049\n",
      "Epoch: 206, Train_Loss: 1.7432,Test_Loss: 1.7363,Train_acc: 0.4100,Test_acc: 0.3847\n",
      "Epoch: 207, Train_Loss: 1.6979,Test_Loss: 1.7393,Train_acc: 0.5200,Test_acc: 0.5163\n",
      "Epoch: 208, Train_Loss: 1.7398,Test_Loss: 1.7572,Train_acc: 0.5000,Test_acc: 0.4969\n",
      "Epoch: 209, Train_Loss: 1.7156,Test_Loss: 1.7096,Train_acc: 0.4900,Test_acc: 0.4656\n",
      "Epoch: 210, Train_Loss: 1.7032,Test_Loss: 1.7170,Train_acc: 0.4600,Test_acc: 0.4338\n",
      "Epoch: 211, Train_Loss: 1.6821,Test_Loss: 1.7582,Train_acc: 0.4700,Test_acc: 0.4610\n",
      "Epoch: 212, Train_Loss: 1.7393,Test_Loss: 1.7181,Train_acc: 0.5000,Test_acc: 0.4957\n",
      "Epoch: 213, Train_Loss: 1.6829,Test_Loss: 1.7345,Train_acc: 0.3800,Test_acc: 0.4235\n",
      "Epoch: 214, Train_Loss: 1.7494,Test_Loss: 1.7087,Train_acc: 0.4300,Test_acc: 0.4701\n",
      "Epoch: 215, Train_Loss: 1.6817,Test_Loss: 1.6938,Train_acc: 0.5400,Test_acc: 0.5505\n",
      "Epoch: 216, Train_Loss: 1.6806,Test_Loss: 1.6926,Train_acc: 0.5300,Test_acc: 0.5357\n",
      "Epoch: 217, Train_Loss: 1.6811,Test_Loss: 1.7034,Train_acc: 0.5100,Test_acc: 0.4804\n",
      "Epoch: 218, Train_Loss: 1.6716,Test_Loss: 1.7023,Train_acc: 0.5300,Test_acc: 0.4738\n",
      "Epoch: 219, Train_Loss: 1.6715,Test_Loss: 1.7051,Train_acc: 0.5100,Test_acc: 0.4647\n",
      "Epoch: 220, Train_Loss: 1.6675,Test_Loss: 1.6989,Train_acc: 0.5000,Test_acc: 0.4977\n",
      "Epoch: 221, Train_Loss: 1.6699,Test_Loss: 1.7012,Train_acc: 0.5100,Test_acc: 0.5184\n",
      "Epoch: 222, Train_Loss: 1.6731,Test_Loss: 1.6948,Train_acc: 0.5200,Test_acc: 0.5163\n",
      "Epoch: 223, Train_Loss: 1.6667,Test_Loss: 1.6944,Train_acc: 0.5400,Test_acc: 0.5542\n",
      "Epoch: 224, Train_Loss: 1.6844,Test_Loss: 1.7036,Train_acc: 0.5500,Test_acc: 0.5414\n",
      "Epoch: 225, Train_Loss: 1.6641,Test_Loss: 1.7397,Train_acc: 0.4200,Test_acc: 0.4515\n",
      "Epoch: 226, Train_Loss: 1.7908,Test_Loss: 1.7002,Train_acc: 0.5200,Test_acc: 0.5394\n",
      "Epoch: 227, Train_Loss: 1.6836,Test_Loss: 1.7088,Train_acc: 0.5000,Test_acc: 0.5258\n",
      "Epoch: 228, Train_Loss: 1.6984,Test_Loss: 1.6910,Train_acc: 0.5200,Test_acc: 0.5505\n",
      "Epoch: 229, Train_Loss: 1.6840,Test_Loss: 1.7033,Train_acc: 0.4800,Test_acc: 0.5241\n",
      "Epoch: 230, Train_Loss: 1.6795,Test_Loss: 1.6924,Train_acc: 0.5400,Test_acc: 0.5579\n",
      "Epoch: 231, Train_Loss: 1.6884,Test_Loss: 1.7276,Train_acc: 0.4900,Test_acc: 0.5303\n",
      "Epoch: 232, Train_Loss: 1.7126,Test_Loss: 1.7015,Train_acc: 0.5200,Test_acc: 0.5485\n",
      "Epoch: 233, Train_Loss: 1.6731,Test_Loss: 1.7186,Train_acc: 0.3900,Test_acc: 0.4598\n",
      "Epoch: 234, Train_Loss: 1.7879,Test_Loss: 1.7356,Train_acc: 0.3800,Test_acc: 0.4012\n",
      "Epoch: 235, Train_Loss: 1.6900,Test_Loss: 1.7103,Train_acc: 0.4900,Test_acc: 0.4862\n",
      "Epoch: 236, Train_Loss: 1.6777,Test_Loss: 1.7207,Train_acc: 0.5100,Test_acc: 0.4491\n",
      "Epoch: 237, Train_Loss: 1.6749,Test_Loss: 1.7168,Train_acc: 0.5300,Test_acc: 0.4482\n",
      "Epoch: 238, Train_Loss: 1.6646,Test_Loss: 1.7167,Train_acc: 0.5200,Test_acc: 0.4458\n",
      "Epoch: 239, Train_Loss: 1.6668,Test_Loss: 1.6999,Train_acc: 0.5000,Test_acc: 0.4705\n",
      "Epoch: 240, Train_Loss: 1.6565,Test_Loss: 1.6880,Train_acc: 0.5100,Test_acc: 0.5142\n",
      "Epoch: 241, Train_Loss: 1.6620,Test_Loss: 1.6850,Train_acc: 0.5200,Test_acc: 0.5249\n",
      "Epoch: 242, Train_Loss: 1.6570,Test_Loss: 1.6811,Train_acc: 0.5600,Test_acc: 0.5555\n",
      "Epoch: 243, Train_Loss: 1.6710,Test_Loss: 1.6984,Train_acc: 0.5100,Test_acc: 0.5328\n",
      "Epoch: 244, Train_Loss: 1.6993,Test_Loss: 1.6875,Train_acc: 0.5300,Test_acc: 0.5439\n",
      "Epoch: 245, Train_Loss: 1.6843,Test_Loss: 1.6919,Train_acc: 0.5000,Test_acc: 0.5410\n",
      "Epoch: 246, Train_Loss: 1.6806,Test_Loss: 1.6842,Train_acc: 0.5200,Test_acc: 0.5200\n",
      "Epoch: 247, Train_Loss: 1.6578,Test_Loss: 1.6950,Train_acc: 0.5200,Test_acc: 0.4808\n",
      "Epoch: 248, Train_Loss: 1.6565,Test_Loss: 1.6981,Train_acc: 0.5100,Test_acc: 0.4697\n",
      "Epoch: 249, Train_Loss: 1.6544,Test_Loss: 1.6989,Train_acc: 0.5000,Test_acc: 0.4680\n",
      "Epoch: 250, Train_Loss: 1.6566,Test_Loss: 1.7108,Train_acc: 0.4300,Test_acc: 0.4363\n",
      "Epoch: 251, Train_Loss: 1.6870,Test_Loss: 1.7102,Train_acc: 0.4500,Test_acc: 0.4581\n",
      "Epoch: 252, Train_Loss: 1.6883,Test_Loss: 1.7377,Train_acc: 0.5200,Test_acc: 0.5212\n",
      "Epoch: 253, Train_Loss: 1.7116,Test_Loss: 1.6831,Train_acc: 0.5200,Test_acc: 0.5386\n",
      "Epoch: 254, Train_Loss: 1.6350,Test_Loss: 1.7661,Train_acc: 0.3800,Test_acc: 0.4173\n",
      "Epoch: 255, Train_Loss: 1.7716,Test_Loss: 1.6797,Train_acc: 0.5400,Test_acc: 0.5472\n",
      "Epoch: 256, Train_Loss: 1.6697,Test_Loss: 1.6877,Train_acc: 0.5100,Test_acc: 0.5336\n",
      "Epoch: 257, Train_Loss: 1.6756,Test_Loss: 1.6789,Train_acc: 0.5400,Test_acc: 0.5472\n",
      "Epoch: 258, Train_Loss: 1.6487,Test_Loss: 1.7017,Train_acc: 0.4800,Test_acc: 0.4849\n",
      "Epoch: 259, Train_Loss: 1.6852,Test_Loss: 1.7178,Train_acc: 0.4500,Test_acc: 0.4495\n",
      "Epoch: 260, Train_Loss: 1.6825,Test_Loss: 1.6831,Train_acc: 0.5200,Test_acc: 0.5361\n",
      "Epoch: 261, Train_Loss: 1.6440,Test_Loss: 1.7031,Train_acc: 0.5500,Test_acc: 0.5373\n",
      "Epoch: 262, Train_Loss: 1.6886,Test_Loss: 1.6769,Train_acc: 0.5500,Test_acc: 0.5621\n",
      "Epoch: 263, Train_Loss: 1.6475,Test_Loss: 1.7402,Train_acc: 0.4000,Test_acc: 0.4256\n",
      "Epoch: 264, Train_Loss: 1.7289,Test_Loss: 1.6846,Train_acc: 0.5100,Test_acc: 0.5274\n",
      "Epoch: 265, Train_Loss: 1.7010,Test_Loss: 1.8013,Train_acc: 0.4600,Test_acc: 0.4482\n",
      "Epoch: 266, Train_Loss: 1.7772,Test_Loss: 1.7301,Train_acc: 0.4800,Test_acc: 0.4544\n",
      "Epoch: 267, Train_Loss: 1.6799,Test_Loss: 1.7814,Train_acc: 0.4100,Test_acc: 0.4029\n",
      "Epoch: 268, Train_Loss: 1.7666,Test_Loss: 1.7607,Train_acc: 0.4300,Test_acc: 0.4157\n",
      "Epoch: 269, Train_Loss: 1.6767,Test_Loss: 1.6992,Train_acc: 0.5200,Test_acc: 0.4680\n",
      "Epoch: 270, Train_Loss: 1.6634,Test_Loss: 1.6927,Train_acc: 0.5400,Test_acc: 0.4726\n",
      "Epoch: 271, Train_Loss: 1.6438,Test_Loss: 1.7040,Train_acc: 0.5200,Test_acc: 0.4623\n",
      "Epoch: 272, Train_Loss: 1.6463,Test_Loss: 1.7011,Train_acc: 0.5100,Test_acc: 0.4643\n",
      "Epoch: 273, Train_Loss: 1.6429,Test_Loss: 1.6934,Train_acc: 0.5000,Test_acc: 0.4763\n",
      "Epoch: 274, Train_Loss: 1.6330,Test_Loss: 1.6818,Train_acc: 0.5200,Test_acc: 0.5052\n",
      "Epoch: 275, Train_Loss: 1.6355,Test_Loss: 1.6740,Train_acc: 0.5000,Test_acc: 0.5377\n",
      "Epoch: 276, Train_Loss: 1.6358,Test_Loss: 1.6711,Train_acc: 0.5100,Test_acc: 0.5480\n",
      "Epoch: 277, Train_Loss: 1.6295,Test_Loss: 1.6721,Train_acc: 0.5300,Test_acc: 0.5596\n",
      "Epoch: 278, Train_Loss: 1.6304,Test_Loss: 1.7020,Train_acc: 0.4600,Test_acc: 0.4784\n",
      "Epoch: 279, Train_Loss: 1.6828,Test_Loss: 1.7193,Train_acc: 0.4500,Test_acc: 0.4334\n",
      "Epoch: 280, Train_Loss: 1.6451,Test_Loss: 1.7071,Train_acc: 0.5100,Test_acc: 0.4602\n",
      "Epoch: 281, Train_Loss: 1.6407,Test_Loss: 1.7078,Train_acc: 0.5100,Test_acc: 0.4627\n",
      "Epoch: 282, Train_Loss: 1.6379,Test_Loss: 1.7042,Train_acc: 0.5100,Test_acc: 0.4660\n",
      "Epoch: 283, Train_Loss: 1.6409,Test_Loss: 1.6854,Train_acc: 0.5100,Test_acc: 0.4953\n",
      "Epoch: 284, Train_Loss: 1.6241,Test_Loss: 1.6808,Train_acc: 0.5400,Test_acc: 0.4957\n",
      "Epoch: 285, Train_Loss: 1.6171,Test_Loss: 1.6991,Train_acc: 0.4700,Test_acc: 0.4627\n",
      "Epoch: 286, Train_Loss: 1.6349,Test_Loss: 1.7126,Train_acc: 0.4600,Test_acc: 0.4392\n",
      "Epoch: 287, Train_Loss: 1.6374,Test_Loss: 1.7128,Train_acc: 0.4900,Test_acc: 0.4499\n",
      "Epoch: 288, Train_Loss: 1.6276,Test_Loss: 1.7022,Train_acc: 0.5200,Test_acc: 0.4639\n",
      "Epoch: 289, Train_Loss: 1.6211,Test_Loss: 1.6964,Train_acc: 0.5100,Test_acc: 0.4734\n",
      "Epoch: 290, Train_Loss: 1.6200,Test_Loss: 1.6955,Train_acc: 0.5100,Test_acc: 0.4788\n",
      "Epoch: 291, Train_Loss: 1.6185,Test_Loss: 1.7016,Train_acc: 0.5200,Test_acc: 0.4635\n",
      "Epoch: 292, Train_Loss: 1.6217,Test_Loss: 1.6864,Train_acc: 0.5100,Test_acc: 0.4804\n",
      "Epoch: 293, Train_Loss: 1.6140,Test_Loss: 1.6726,Train_acc: 0.5000,Test_acc: 0.5052\n",
      "Epoch: 294, Train_Loss: 1.6171,Test_Loss: 1.6653,Train_acc: 0.5200,Test_acc: 0.5328\n",
      "Epoch: 295, Train_Loss: 1.6219,Test_Loss: 1.6682,Train_acc: 0.5300,Test_acc: 0.5196\n",
      "Epoch: 296, Train_Loss: 1.6140,Test_Loss: 1.6830,Train_acc: 0.4900,Test_acc: 0.4784\n",
      "Epoch: 297, Train_Loss: 1.6156,Test_Loss: 1.6998,Train_acc: 0.5000,Test_acc: 0.4577\n",
      "Epoch: 298, Train_Loss: 1.6231,Test_Loss: 1.7104,Train_acc: 0.5100,Test_acc: 0.4532\n",
      "Epoch: 299, Train_Loss: 1.6386,Test_Loss: 1.7031,Train_acc: 0.5600,Test_acc: 0.4701\n",
      "Epoch: 300, Train_Loss: 1.6194,Test_Loss: 1.6735,Train_acc: 0.5400,Test_acc: 0.5052\n",
      "Epoch: 301, Train_Loss: 1.6087,Test_Loss: 1.6620,Train_acc: 0.5500,Test_acc: 0.5530\n",
      "Epoch: 302, Train_Loss: 1.6137,Test_Loss: 1.6495,Train_acc: 0.5500,Test_acc: 0.5728\n",
      "Epoch: 303, Train_Loss: 1.6178,Test_Loss: 1.6694,Train_acc: 0.4800,Test_acc: 0.5299\n",
      "Epoch: 304, Train_Loss: 1.6319,Test_Loss: 1.6569,Train_acc: 0.5400,Test_acc: 0.5724\n",
      "Epoch: 305, Train_Loss: 1.6136,Test_Loss: 1.6614,Train_acc: 0.5300,Test_acc: 0.5575\n",
      "Epoch: 306, Train_Loss: 1.5970,Test_Loss: 1.6997,Train_acc: 0.5000,Test_acc: 0.4812\n",
      "Epoch: 307, Train_Loss: 1.6533,Test_Loss: 1.7074,Train_acc: 0.5300,Test_acc: 0.4511\n",
      "Epoch: 308, Train_Loss: 1.6034,Test_Loss: 1.7037,Train_acc: 0.4900,Test_acc: 0.4305\n",
      "Epoch: 309, Train_Loss: 1.6205,Test_Loss: 1.6713,Train_acc: 0.5300,Test_acc: 0.4899\n",
      "Epoch: 310, Train_Loss: 1.5996,Test_Loss: 1.6645,Train_acc: 0.5200,Test_acc: 0.5093\n",
      "Epoch: 311, Train_Loss: 1.6094,Test_Loss: 1.6367,Train_acc: 0.5600,Test_acc: 0.5823\n",
      "Epoch: 312, Train_Loss: 1.6054,Test_Loss: 1.6480,Train_acc: 0.5600,Test_acc: 0.5682\n",
      "Epoch: 313, Train_Loss: 1.6058,Test_Loss: 1.6453,Train_acc: 0.5500,Test_acc: 0.5720\n",
      "Epoch: 314, Train_Loss: 1.5833,Test_Loss: 1.6488,Train_acc: 0.5600,Test_acc: 0.5530\n",
      "Epoch: 315, Train_Loss: 1.5789,Test_Loss: 1.6758,Train_acc: 0.4800,Test_acc: 0.4256\n",
      "Epoch: 316, Train_Loss: 1.6091,Test_Loss: 1.6739,Train_acc: 0.5100,Test_acc: 0.4305\n",
      "Epoch: 317, Train_Loss: 1.5844,Test_Loss: 1.6695,Train_acc: 0.5100,Test_acc: 0.4746\n",
      "Epoch: 318, Train_Loss: 1.5826,Test_Loss: 1.6440,Train_acc: 0.5500,Test_acc: 0.5534\n",
      "Epoch: 319, Train_Loss: 1.5768,Test_Loss: 1.6659,Train_acc: 0.5000,Test_acc: 0.5357\n",
      "Epoch: 320, Train_Loss: 1.6171,Test_Loss: 1.6389,Train_acc: 0.5700,Test_acc: 0.5810\n",
      "Epoch: 321, Train_Loss: 1.6282,Test_Loss: 1.6721,Train_acc: 0.5300,Test_acc: 0.5456\n",
      "Epoch: 322, Train_Loss: 1.6019,Test_Loss: 1.6525,Train_acc: 0.5400,Test_acc: 0.5373\n",
      "Epoch: 323, Train_Loss: 1.5991,Test_Loss: 1.6480,Train_acc: 0.5400,Test_acc: 0.5460\n",
      "Epoch: 324, Train_Loss: 1.5658,Test_Loss: 1.6550,Train_acc: 0.5400,Test_acc: 0.5744\n",
      "Epoch: 325, Train_Loss: 1.6261,Test_Loss: 1.6827,Train_acc: 0.5400,Test_acc: 0.5336\n",
      "Epoch: 326, Train_Loss: 1.6251,Test_Loss: 1.6165,Train_acc: 0.5600,Test_acc: 0.5918\n",
      "Epoch: 327, Train_Loss: 1.5980,Test_Loss: 1.6197,Train_acc: 0.5100,Test_acc: 0.5885\n",
      "Epoch: 328, Train_Loss: 1.5668,Test_Loss: 1.6546,Train_acc: 0.5300,Test_acc: 0.5732\n",
      "Epoch: 329, Train_Loss: 1.6100,Test_Loss: 1.6566,Train_acc: 0.5600,Test_acc: 0.5241\n",
      "Epoch: 330, Train_Loss: 1.5550,Test_Loss: 1.6492,Train_acc: 0.5600,Test_acc: 0.5254\n",
      "Epoch: 331, Train_Loss: 1.5752,Test_Loss: 1.6454,Train_acc: 0.5700,Test_acc: 0.5522\n",
      "Epoch: 332, Train_Loss: 1.5902,Test_Loss: 1.6897,Train_acc: 0.5500,Test_acc: 0.4680\n",
      "Epoch: 333, Train_Loss: 1.5977,Test_Loss: 1.6607,Train_acc: 0.5600,Test_acc: 0.4664\n",
      "Epoch: 334, Train_Loss: 1.5626,Test_Loss: 1.6376,Train_acc: 0.5900,Test_acc: 0.5295\n",
      "Epoch: 335, Train_Loss: 1.5475,Test_Loss: 1.6422,Train_acc: 0.5500,Test_acc: 0.5608\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import DenseGraphConv, GCNConv, dense_mincut_pool\n",
    "from torch_geometric.utils import to_dense_adj, to_dense_batch\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import utils\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as tg\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "early_stop_thresh = 25\n",
    "best_macro_f1 = -1\n",
    "\n",
    "\n",
    "train_dataset = utils.GraphDataset(\n",
    "    \"../data/\", \"MixedShapesSmallTrain_TRAIN\", True, n_quantiles=100\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = utils.GraphDataset(\n",
    "    \"../data/\", \"MixedShapesSmallTrain_TEST\", True, n_quantiles=100\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels=32):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        num_nodes = ceil(0.5 * 100)\n",
    "        self.pool1 = Linear(hidden_channels, num_nodes)\n",
    "\n",
    "        self.conv2 = DenseGraphConv(hidden_channels, hidden_channels)\n",
    "        num_nodes = ceil(0.5 * num_nodes)\n",
    "        self.pool2 = Linear(hidden_channels, num_nodes)\n",
    "\n",
    "        self.conv3 = DenseGraphConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "\n",
    "        x, mask = to_dense_batch(x, batch)\n",
    "        adj = to_dense_adj(edge_index, batch)\n",
    "        s = self.pool1(x)\n",
    "        x, adj, mc1, o1 = dense_mincut_pool(x, adj, s, mask)\n",
    "\n",
    "        x = self.conv2(x, adj).relu()\n",
    "        s = self.pool2(x)\n",
    "        x, adj, mc2, o2 = dense_mincut_pool(x, adj, s)\n",
    "\n",
    "        x = self.conv3(x, adj)\n",
    "\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.lin1(x).relu()\n",
    "        x = self.lin2(x)\n",
    "        return F.log_softmax(x, dim=-1), mc1 + mc2, o1 + o2\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net(1, 5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=5, mode=\"min\", cooldown=2, factor=0.5, verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out, mc_loss, o_loss = model(data.x, data.edge_index, data.batch)\n",
    "        loss = F.nll_loss(out, data.y.view(-1)) + mc_loss + o_loss\n",
    "        loss.backward()\n",
    "        loss_all += data.y.size(0) * float(loss)\n",
    "        optimizer.step()\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    loss_all = 0\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        pred, mc_loss, o_loss = model(data.x, data.edge_index, data.batch)\n",
    "        loss = F.nll_loss(pred, data.y.view(-1)) + mc_loss + o_loss\n",
    "        loss_all += data.y.size(0) * float(loss)\n",
    "        correct += int(pred.max(dim=1)[1].eq(data.y.view(-1)).sum())\n",
    "\n",
    "    return loss_all / len(loader.dataset), correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "best_val_acc = test_acc = 0\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = start_patience = 50\n",
    "for epoch in range(1, 2000):\n",
    "    train_loss = train(epoch)\n",
    "    _, train_acc = test(train_loader)\n",
    "    val_loss, val_acc = test(test_loader)\n",
    "    if val_loss < best_val_loss:\n",
    "        test_loss, test_acc = test(test_loader)\n",
    "        best_val_acc = val_acc\n",
    "        patience = start_patience\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience == 0:\n",
    "            break\n",
    "    print(\n",
    "        f\"Epoch: {epoch:03d}, Train_Loss: {train_loss:02.4f},Test_Loss: {test_loss:02.4f},Train_acc: {train_acc:01.4f},Test_acc: {test_acc:01.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
