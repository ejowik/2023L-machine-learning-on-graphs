{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0c9d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import utils\n",
    "\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from gcn_models import GIN, GATC, SAGEe\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47ce55c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_thresh = 15\n",
    "best_macro_f1 = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b7967a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = utils.GraphDataset(\n",
    "    \"../data/\", \"MixedShapesSmallTrain_TRAIN\", True, n_quantiles=100\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = utils.GraphDataset(\n",
    "    \"../data/\", \"MixedShapesSmallTrain_TEST\", True, n_quantiles=100\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "\n",
    "model = GATC(\n",
    "    1, [2, 4, 8, 16, 32, 64, 64, 64, 32, 16, 16, 8], pool_method=\"max\", heads=1\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=5, mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f00a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x.to(torch.float32), data.edge_index, data.batch)\n",
    "        loss = F.cross_entropy(out, data.y - 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader, return_loss=True):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    loss = 0\n",
    "    for data in loader:\n",
    "        y_out = model(data.x.to(torch.float32), data.edge_index, data.batch)\n",
    "        y_pred.append(y_out.argmax(dim=-1))\n",
    "        y_true.append(data.y - 1)\n",
    "        if return_loss:\n",
    "            loss += float(F.cross_entropy(y_out, data.y - 1)) * data.num_graphs\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    y_true = np.concatenate(y_true)\n",
    "    if return_loss:\n",
    "        return f1_score(y_true=y_true, y_pred=y_pred, average=\"macro\"), loss / len(\n",
    "            loader.dataset\n",
    "        )\n",
    "    return f1_score(y_true=y_true, y_pred=y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5502a6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train_Loss: 1.6233, Train: 0.0667, Test_Loss: 1.6172, Test: 0.0850\n",
      "Epoch: 002, Train_Loss: 1.6189, Train: 0.0667, Test_Loss: 1.6149, Test: 0.0850\n",
      "Epoch: 003, Train_Loss: 1.6152, Train: 0.0667, Test_Loss: 1.6103, Test: 0.0850\n",
      "Epoch: 004, Train_Loss: 1.6152, Train: 0.0667, Test_Loss: 1.6113, Test: 0.0850\n",
      "Epoch: 005, Train_Loss: 1.6135, Train: 0.0667, Test_Loss: 1.6090, Test: 0.0850\n",
      "Epoch: 006, Train_Loss: 1.6115, Train: 0.0667, Test_Loss: 1.6062, Test: 0.0850\n",
      "Epoch: 007, Train_Loss: 1.6106, Train: 0.0667, Test_Loss: 1.6040, Test: 0.0850\n",
      "Epoch: 008, Train_Loss: 1.6109, Train: 0.0667, Test_Loss: 1.6020, Test: 0.0850\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m train()\n\u001b[1;32m      3\u001b[0m train_macro_f1, train_loss \u001b[39m=\u001b[39m test(train_loader)\n\u001b[0;32m----> 4\u001b[0m test_macro_f1, test_loss \u001b[39m=\u001b[39m test(test_loader, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      5\u001b[0m scheduler\u001b[39m.\u001b[39mstep(train_loss)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[1;32m      7\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train_Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m02.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train: \u001b[39m\u001b[39m{\u001b[39;00mtrain_macro_f1\u001b[39m:\u001b[39;00m\u001b[39m01.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Test_Loss: \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m:\u001b[39;00m\u001b[39m02.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Test: \u001b[39m\u001b[39m{\u001b[39;00mtest_macro_f1\u001b[39m:\u001b[39;00m\u001b[39m01.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(loader, return_loss)\u001b[0m\n\u001b[1;32m     20\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m loader:\n\u001b[0;32m---> 22\u001b[0m     y_out \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39;49mx\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39;49mfloat32), data\u001b[39m.\u001b[39;49medge_index, data\u001b[39m.\u001b[39;49mbatch)\n\u001b[1;32m     23\u001b[0m     y_pred\u001b[39m.\u001b[39mappend(y_out\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     24\u001b[0m     y_true\u001b[39m.\u001b[39mappend(data\u001b[39m.\u001b[39my \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/2023L-machine-learning-on-graphs/project_2/gcn_models.py:94\u001b[0m, in \u001b[0;36mGATC.forward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, edge_index, batch):\n\u001b[1;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m conv \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvs:\n\u001b[0;32m---> 94\u001b[0m         x \u001b[39m=\u001b[39m conv(x, edge_index)\u001b[39m.\u001b[39mrelu()\n\u001b[1;32m     95\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool_method(x, batch)\n\u001b[1;32m     96\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x, p\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch_geometric/nn/conv/gat_conv.py:226\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39m# Next, we compute node-level attention coefficients, both for source\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[39m# and target nodes (if present):\u001b[39;00m\n\u001b[1;32m    225\u001b[0m alpha_src \u001b[39m=\u001b[39m (x_src \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39matt_src)\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 226\u001b[0m alpha_dst \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x_dst \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m (x_dst \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49matt_dst)\u001b[39m.\u001b[39;49msum(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    227\u001b[0m alpha \u001b[39m=\u001b[39m (alpha_src, alpha_dst)\n\u001b[1;32m    229\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_self_loops:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 501):\n",
    "    train()\n",
    "    train_macro_f1, train_loss = test(train_loader)\n",
    "    test_macro_f1, test_loss = test(test_loader, True)\n",
    "    scheduler.step(train_loss)\n",
    "    print(\n",
    "        f\"Epoch: {epoch:03d}, Train_Loss: {train_loss:02.4f}, Train: {train_macro_f1:01.4f}, Test_Loss: {test_loss:02.4f}, Test: {test_macro_f1:01.4f}\"\n",
    "    )\n",
    "    if test_macro_f1 > best_macro_f1:\n",
    "        best_accuracy = test_macro_f1\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), \"../data/best_model.pth\")\n",
    "    elif epoch - best_epoch > early_stop_thresh:\n",
    "        print(\n",
    "            f\"Early stopped training at epoch {epoch} best macro F1 {best_macro_f1} in {best_epoch}\"\n",
    "        )\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b22a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAGE(1, [2, 4, 8, 16, 32, 64, 64, 64, 32, 16, 16, 8], aggr_method=\"max\")\n",
    "# self,\n",
    "# in_channels,\n",
    "# sizes,\n",
    "# out_channels=5,\n",
    "# pool_method=\"max\",\n",
    "# aggr_method=\"mean\",\n",
    "# normalize=True,\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=5, mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b9a82f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x.to(torch.float32), data.edge_index, data.batch)\n",
    "        loss = F.cross_entropy(out, data.y - 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader, return_loss=True):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    loss = 0\n",
    "    for data in loader:\n",
    "        y_out = model(data.x.to(torch.float32), data.edge_index, data.batch)\n",
    "        y_pred.append(y_out.argmax(dim=-1))\n",
    "        y_true.append(data.y - 1)\n",
    "        if return_loss:\n",
    "            loss += float(F.cross_entropy(y_out, data.y - 1)) * data.num_graphs\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    y_true = np.concatenate(y_true)\n",
    "    if return_loss:\n",
    "        return f1_score(y_true=y_true, y_pred=y_pred, average=\"macro\"), loss / len(\n",
    "            loader.dataset\n",
    "        )\n",
    "    return f1_score(y_true=y_true, y_pred=y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e73f032a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train_Loss: 1.6137, Train: 0.0667, Test_Loss: 1.6037, Test: 0.0588\n",
      "Epoch: 002, Train_Loss: 1.6112, Train: 0.0667, Test_Loss: 1.6067, Test: 0.0588\n",
      "Epoch: 003, Train_Loss: 1.6110, Train: 0.0667, Test_Loss: 1.6085, Test: 0.0588\n",
      "Epoch: 004, Train_Loss: 1.6111, Train: 0.0667, Test_Loss: 1.6118, Test: 0.0588\n",
      "Epoch: 005, Train_Loss: 1.6114, Train: 0.0667, Test_Loss: 1.6144, Test: 0.0459\n",
      "Epoch: 006, Train_Loss: 1.6120, Train: 0.0667, Test_Loss: 1.6148, Test: 0.0459\n",
      "Epoch: 007, Train_Loss: 1.6124, Train: 0.0667, Test_Loss: 1.6133, Test: 0.0459\n",
      "Epoch: 008, Train_Loss: 1.6128, Train: 0.0667, Test_Loss: 1.6120, Test: 0.0459\n",
      "Epoch: 009, Train_Loss: 1.6136, Train: 0.0667, Test_Loss: 1.6115, Test: 0.0459\n",
      "Epoch: 010, Train_Loss: 1.6135, Train: 0.0667, Test_Loss: 1.6114, Test: 0.0459\n",
      "Epoch: 011, Train_Loss: 1.6135, Train: 0.0667, Test_Loss: 1.6113, Test: 0.0459\n",
      "Epoch: 012, Train_Loss: 1.6135, Train: 0.0667, Test_Loss: 1.6112, Test: 0.0459\n",
      "Epoch: 013, Train_Loss: 1.6134, Train: 0.0667, Test_Loss: 1.6110, Test: 0.0459\n",
      "Epoch: 014, Train_Loss: 1.6132, Train: 0.0667, Test_Loss: 1.6109, Test: 0.0459\n",
      "Epoch: 015, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6106, Test: 0.0459\n",
      "Epoch: 016, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6106, Test: 0.0459\n",
      "Epoch: 017, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6106, Test: 0.0459\n",
      "Epoch: 018, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 019, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 020, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 021, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 022, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 023, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 024, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 025, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 026, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 027, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 028, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 029, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 030, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 031, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 032, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 033, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 034, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 035, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 036, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 037, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 038, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 039, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 040, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 041, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 042, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 043, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 044, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 045, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 046, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 047, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 048, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 049, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 050, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 051, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 052, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 053, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 054, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 055, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n",
      "Epoch: 056, Train_Loss: 1.6131, Train: 0.0667, Test_Loss: 1.6105, Test: 0.0459\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m501\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     train()\n\u001b[1;32m      3\u001b[0m     train_macro_f1, train_loss \u001b[39m=\u001b[39m test(train_loader)\n\u001b[1;32m      4\u001b[0m     test_macro_f1, test_loss \u001b[39m=\u001b[39m test(test_loader, \u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m out \u001b[39m=\u001b[39m model(data\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32), data\u001b[39m.\u001b[39medge_index, data\u001b[39m.\u001b[39mbatch)\n\u001b[1;32m      8\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(out, data\u001b[39m.\u001b[39my \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     10\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     11\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(loss) \u001b[39m*\u001b[39m data\u001b[39m.\u001b[39mnum_graphs\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 501):\n",
    "    train()\n",
    "    train_macro_f1, train_loss = test(train_loader)\n",
    "    test_macro_f1, test_loss = test(test_loader, True)\n",
    "    scheduler.step(train_loss)\n",
    "    print(\n",
    "        f\"Epoch: {epoch:03d}, Train_Loss: {train_loss:02.4f}, Train: {train_macro_f1:01.4f}, Test_Loss: {test_loss:02.4f}, Test: {test_macro_f1:01.4f}\"\n",
    "    )\n",
    "    if test_macro_f1 > best_macro_f1:\n",
    "        best_accuracy = test_macro_f1\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), \"../data/best_model.pth\")\n",
    "    elif epoch - best_epoch > early_stop_thresh:\n",
    "        print(\n",
    "            f\"Early stopped training at epoch {epoch} best macro F1 {best_macro_f1} in {best_epoch}\"\n",
    "        )\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
