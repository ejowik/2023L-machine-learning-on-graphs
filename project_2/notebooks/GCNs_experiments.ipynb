{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c9d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import utils\n",
    "\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from gcn_models import GIN, GATC, SAGE, Net\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ce55c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_thresh = 15\n",
    "best_macro_f1 = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b7967a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = utils.GraphDataset(\n",
    "    \"../data/\", \"MixedShapesSmallTrain_TRAIN\", True, n_quantiles=100\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = utils.GraphDataset(\n",
    "    \"../data/\", \"MixedShapesSmallTrain_TEST\", True, n_quantiles=100\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  # , weight_decay=1e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=3,T_mult=5,verbose=True)#,\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=5, mode=\"min\", cooldown=2, factor=0.8, verbose=True\n",
    ")\n",
    "# scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer,lr_lambda=lambda x:)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f00a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        # out = model(data.x.to(torch.float32), data.edge_index, data.batch)[0]\n",
    "        data.x = data.x.to(torch.float32)\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y - 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader, return_loss=True):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    loss = 0\n",
    "    for data in loader:\n",
    "        # y_out = model(data.x.to(torch.float32), data.edge_index, data.batch)[0]\n",
    "        y_out = model(data)\n",
    "        y_pred.append(y_out.argmax(dim=-1))\n",
    "        y_true.append(data.y - 1)\n",
    "        if return_loss:\n",
    "            loss += float(F.cross_entropy(y_out, data.y - 1)) * data.num_graphs\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    y_true = np.concatenate(y_true)\n",
    "    if return_loss:\n",
    "        return (\n",
    "            f1_score(y_true=y_true, y_pred=y_pred, average=\"macro\"),\n",
    "            accuracy_score(y_true=y_true, y_pred=y_pred),\n",
    "            loss / len(loader.dataset),\n",
    "        )\n",
    "    return f1_score(y_true=y_true, y_pred=y_pred, average=\"macro\"), accuracy_score(\n",
    "        y_true=y_true, y_pred=y_pred\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5502a6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train_Loss: 6910.4812,Test_Loss: 7312.6342,Train_f1: 0.0667,Test_f1: 0.0775,Train_acc: 0.2000,Test_acc: 0.2392\n",
      "Epoch: 002, Train_Loss: 6647.9703,Test_Loss: 6984.2227,Train_f1: 0.0639,Test_f1: 0.0777,Train_acc: 0.1900,Test_acc: 0.2392\n",
      "Epoch: 003, Train_Loss: 6293.4893,Test_Loss: 6633.8046,Train_f1: 0.0639,Test_f1: 0.0779,Train_acc: 0.1900,Test_acc: 0.2392\n",
      "Epoch: 004, Train_Loss: 5938.8445,Test_Loss: 6290.2569,Train_f1: 0.0639,Test_f1: 0.0789,Train_acc: 0.1900,Test_acc: 0.2392\n",
      "Epoch: 005, Train_Loss: 5692.9267,Test_Loss: 6006.1441,Train_f1: 0.0644,Test_f1: 0.0788,Train_acc: 0.1900,Test_acc: 0.2388\n",
      "Epoch: 006, Train_Loss: 5377.6299,Test_Loss: 5718.1523,Train_f1: 0.0650,Test_f1: 0.0789,Train_acc: 0.1900,Test_acc: 0.2384\n",
      "Epoch: 007, Train_Loss: 5108.4401,Test_Loss: 5428.8909,Train_f1: 0.0650,Test_f1: 0.0787,Train_acc: 0.1900,Test_acc: 0.2379\n",
      "Epoch: 008, Train_Loss: 4874.1461,Test_Loss: 5155.5032,Train_f1: 0.0644,Test_f1: 0.0799,Train_acc: 0.1900,Test_acc: 0.2388\n",
      "Epoch: 009, Train_Loss: 4657.1687,Test_Loss: 4871.6518,Train_f1: 0.0644,Test_f1: 0.0800,Train_acc: 0.1900,Test_acc: 0.2379\n",
      "Epoch: 010, Train_Loss: 4430.3754,Test_Loss: 4639.8181,Train_f1: 0.0615,Test_f1: 0.0805,Train_acc: 0.1800,Test_acc: 0.2384\n",
      "Epoch: 011, Train_Loss: 4195.5265,Test_Loss: 4433.1090,Train_f1: 0.0615,Test_f1: 0.0829,Train_acc: 0.1800,Test_acc: 0.2388\n",
      "Epoch: 012, Train_Loss: 3993.0071,Test_Loss: 4226.0844,Train_f1: 0.0615,Test_f1: 0.0815,Train_acc: 0.1800,Test_acc: 0.2384\n",
      "Epoch: 013, Train_Loss: 3801.9542,Test_Loss: 4006.8368,Train_f1: 0.0650,Test_f1: 0.0852,Train_acc: 0.1900,Test_acc: 0.2396\n",
      "Epoch: 014, Train_Loss: 3595.6217,Test_Loss: 3819.3654,Train_f1: 0.0650,Test_f1: 0.0864,Train_acc: 0.1900,Test_acc: 0.2408\n",
      "Epoch: 015, Train_Loss: 3393.4479,Test_Loss: 3652.8375,Train_f1: 0.0650,Test_f1: 0.0851,Train_acc: 0.1900,Test_acc: 0.2404\n",
      "Epoch: 016, Train_Loss: 3234.4645,Test_Loss: 3498.3380,Train_f1: 0.0678,Test_f1: 0.0827,Train_acc: 0.2000,Test_acc: 0.2392\n",
      "Epoch: 017, Train_Loss: 3143.5488,Test_Loss: 3361.7310,Train_f1: 0.0621,Test_f1: 0.0825,Train_acc: 0.1800,Test_acc: 0.2384\n",
      "Epoch: 018, Train_Loss: 3028.9628,Test_Loss: 3214.2572,Train_f1: 0.0596,Test_f1: 0.0837,Train_acc: 0.1700,Test_acc: 0.2384\n",
      "Epoch: 019, Train_Loss: 2886.5078,Test_Loss: 3069.6687,Train_f1: 0.0607,Test_f1: 0.0836,Train_acc: 0.1700,Test_acc: 0.2367\n",
      "Epoch: 020, Train_Loss: 2755.8691,Test_Loss: 2923.6957,Train_f1: 0.0607,Test_f1: 0.0855,Train_acc: 0.1700,Test_acc: 0.2363\n",
      "Epoch: 021, Train_Loss: 2666.9741,Test_Loss: 2793.3981,Train_f1: 0.0643,Test_f1: 0.0885,Train_acc: 0.1800,Test_acc: 0.2351\n",
      "Epoch: 022, Train_Loss: 2556.7501,Test_Loss: 2683.0375,Train_f1: 0.0630,Test_f1: 0.0893,Train_acc: 0.1700,Test_acc: 0.2330\n",
      "Epoch: 023, Train_Loss: 2446.7498,Test_Loss: 2576.0994,Train_f1: 0.0654,Test_f1: 0.0918,Train_acc: 0.1700,Test_acc: 0.2326\n",
      "Epoch: 024, Train_Loss: 2368.9562,Test_Loss: 2459.8652,Train_f1: 0.0627,Test_f1: 0.0933,Train_acc: 0.1600,Test_acc: 0.2305\n",
      "Epoch: 025, Train_Loss: 2259.6616,Test_Loss: 2351.9458,Train_f1: 0.0660,Test_f1: 0.0952,Train_acc: 0.1600,Test_acc: 0.2247\n",
      "Epoch: 026, Train_Loss: 2145.6555,Test_Loss: 2237.2514,Train_f1: 0.0681,Test_f1: 0.0930,Train_acc: 0.1600,Test_acc: 0.2082\n",
      "Epoch: 027, Train_Loss: 2031.2081,Test_Loss: 2138.0477,Train_f1: 0.0719,Test_f1: 0.0909,Train_acc: 0.1500,Test_acc: 0.1868\n",
      "Epoch: 028, Train_Loss: 1937.4397,Test_Loss: 2030.9165,Train_f1: 0.0833,Test_f1: 0.0885,Train_acc: 0.1600,Test_acc: 0.1641\n",
      "Epoch: 029, Train_Loss: 1824.7634,Test_Loss: 1927.3082,Train_f1: 0.1138,Test_f1: 0.0838,Train_acc: 0.2000,Test_acc: 0.1423\n",
      "Epoch: 030, Train_Loss: 1712.6833,Test_Loss: 1825.3281,Train_f1: 0.1050,Test_f1: 0.0817,Train_acc: 0.1800,Test_acc: 0.1328\n",
      "Epoch: 031, Train_Loss: 1614.6453,Test_Loss: 1721.8498,Train_f1: 0.1062,Test_f1: 0.0832,Train_acc: 0.1800,Test_acc: 0.1348\n",
      "Epoch: 032, Train_Loss: 1506.0102,Test_Loss: 1615.7561,Train_f1: 0.1062,Test_f1: 0.0832,Train_acc: 0.1800,Test_acc: 0.1344\n",
      "Epoch: 033, Train_Loss: 1409.5869,Test_Loss: 1519.9305,Train_f1: 0.1250,Test_f1: 0.0832,Train_acc: 0.2100,Test_acc: 0.1328\n",
      "Epoch: 034, Train_Loss: 1291.2696,Test_Loss: 1421.2033,Train_f1: 0.1350,Test_f1: 0.0763,Train_acc: 0.2300,Test_acc: 0.1221\n",
      "Epoch: 035, Train_Loss: 1212.3932,Test_Loss: 1330.2113,Train_f1: 0.1177,Test_f1: 0.0748,Train_acc: 0.2100,Test_acc: 0.1270\n",
      "Epoch: 036, Train_Loss: 1127.1384,Test_Loss: 1256.0542,Train_f1: 0.1204,Test_f1: 0.0676,Train_acc: 0.2300,Test_acc: 0.1262\n",
      "Epoch: 037, Train_Loss: 1054.0669,Test_Loss: 1179.6835,Train_f1: 0.1023,Test_f1: 0.0590,Train_acc: 0.2200,Test_acc: 0.1233\n",
      "Epoch: 038, Train_Loss: 987.7848,Test_Loss: 1105.4392,Train_f1: 0.0860,Test_f1: 0.0550,Train_acc: 0.2100,Test_acc: 0.1282\n",
      "Epoch: 039, Train_Loss: 926.0075,Test_Loss: 1037.2786,Train_f1: 0.0667,Test_f1: 0.0470,Train_acc: 0.2000,Test_acc: 0.1291\n",
      "Epoch: 040, Train_Loss: 856.3658,Test_Loss: 966.2009,Train_f1: 0.0667,Test_f1: 0.0466,Train_acc: 0.2000,Test_acc: 0.1299\n",
      "Epoch: 041, Train_Loss: 792.9809,Test_Loss: 896.0041,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 042, Train_Loss: 722.4651,Test_Loss: 817.9042,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 043, Train_Loss: 653.8440,Test_Loss: 735.5632,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 044, Train_Loss: 592.4261,Test_Loss: 664.1833,Train_f1: 0.0667,Test_f1: 0.0459,Train_acc: 0.2000,Test_acc: 0.1295\n",
      "Epoch: 045, Train_Loss: 532.7209,Test_Loss: 611.1929,Train_f1: 0.0667,Test_f1: 0.0472,Train_acc: 0.2000,Test_acc: 0.1299\n",
      "Epoch: 046, Train_Loss: 500.7679,Test_Loss: 568.7307,Train_f1: 0.0858,Test_f1: 0.0628,Train_acc: 0.2100,Test_acc: 0.1381\n",
      "Epoch: 047, Train_Loss: 478.7634,Test_Loss: 542.1837,Train_f1: 0.0990,Test_f1: 0.0824,Train_acc: 0.2200,Test_acc: 0.1534\n",
      "Epoch: 048, Train_Loss: 456.2271,Test_Loss: 518.0099,Train_f1: 0.1202,Test_f1: 0.0895,Train_acc: 0.2400,Test_acc: 0.1608\n",
      "Epoch: 049, Train_Loss: 436.3483,Test_Loss: 498.0634,Train_f1: 0.1202,Test_f1: 0.0974,Train_acc: 0.2400,Test_acc: 0.1699\n",
      "Epoch: 050, Train_Loss: 419.4115,Test_Loss: 474.2574,Train_f1: 0.1305,Test_f1: 0.1020,Train_acc: 0.2500,Test_acc: 0.1757\n",
      "Epoch: 051, Train_Loss: 402.3403,Test_Loss: 452.9632,Train_f1: 0.1393,Test_f1: 0.1001,Train_acc: 0.2600,Test_acc: 0.1736\n",
      "Epoch: 052, Train_Loss: 382.8332,Test_Loss: 431.7461,Train_f1: 0.1300,Test_f1: 0.1024,Train_acc: 0.2500,Test_acc: 0.1761\n",
      "Epoch: 053, Train_Loss: 365.1677,Test_Loss: 411.5963,Train_f1: 0.1296,Test_f1: 0.1024,Train_acc: 0.2500,Test_acc: 0.1748\n",
      "Epoch: 054, Train_Loss: 347.8968,Test_Loss: 387.8910,Train_f1: 0.1200,Test_f1: 0.1006,Train_acc: 0.2400,Test_acc: 0.1720\n",
      "Epoch: 055, Train_Loss: 332.0319,Test_Loss: 368.9399,Train_f1: 0.1108,Test_f1: 0.1037,Train_acc: 0.2300,Test_acc: 0.1724\n",
      "Epoch: 056, Train_Loss: 318.8281,Test_Loss: 354.4884,Train_f1: 0.1638,Test_f1: 0.1212,Train_acc: 0.2700,Test_acc: 0.1860\n",
      "Epoch: 057, Train_Loss: 305.5646,Test_Loss: 338.5867,Train_f1: 0.1695,Test_f1: 0.1254,Train_acc: 0.2700,Test_acc: 0.1880\n",
      "Epoch: 058, Train_Loss: 291.3293,Test_Loss: 324.0306,Train_f1: 0.1469,Test_f1: 0.1262,Train_acc: 0.2400,Test_acc: 0.1872\n",
      "Epoch: 059, Train_Loss: 274.5667,Test_Loss: 308.1169,Train_f1: 0.1414,Test_f1: 0.1214,Train_acc: 0.2400,Test_acc: 0.1814\n",
      "Epoch: 060, Train_Loss: 267.4906,Test_Loss: 294.4685,Train_f1: 0.1428,Test_f1: 0.1141,Train_acc: 0.2400,Test_acc: 0.1740\n",
      "Epoch: 061, Train_Loss: 257.0075,Test_Loss: 282.0020,Train_f1: 0.1567,Test_f1: 0.1057,Train_acc: 0.2500,Test_acc: 0.1654\n",
      "Epoch: 062, Train_Loss: 244.8986,Test_Loss: 271.6224,Train_f1: 0.1291,Test_f1: 0.0867,Train_acc: 0.2300,Test_acc: 0.1509\n",
      "Epoch: 063, Train_Loss: 237.5527,Test_Loss: 263.1065,Train_f1: 0.1020,Test_f1: 0.0761,Train_acc: 0.2100,Test_acc: 0.1402\n",
      "Epoch: 064, Train_Loss: 225.2638,Test_Loss: 257.8107,Train_f1: 0.1193,Test_f1: 0.0680,Train_acc: 0.2200,Test_acc: 0.1344\n",
      "Epoch: 065, Train_Loss: 217.4790,Test_Loss: 252.4112,Train_f1: 0.1169,Test_f1: 0.0678,Train_acc: 0.2100,Test_acc: 0.1291\n",
      "Epoch: 066, Train_Loss: 213.6814,Test_Loss: 248.4657,Train_f1: 0.0996,Test_f1: 0.0694,Train_acc: 0.2000,Test_acc: 0.1299\n",
      "Epoch: 067, Train_Loss: 211.6930,Test_Loss: 242.0586,Train_f1: 0.0598,Test_f1: 0.0701,Train_acc: 0.1600,Test_acc: 0.1287\n",
      "Epoch: 068, Train_Loss: 207.1556,Test_Loss: 237.2047,Train_f1: 0.0748,Test_f1: 0.0638,Train_acc: 0.1600,Test_acc: 0.1221\n",
      "Epoch: 069, Train_Loss: 201.4294,Test_Loss: 229.1355,Train_f1: 0.0882,Test_f1: 0.0619,Train_acc: 0.1700,Test_acc: 0.1184\n",
      "Epoch: 070, Train_Loss: 197.1272,Test_Loss: 222.2456,Train_f1: 0.0853,Test_f1: 0.0595,Train_acc: 0.1600,Test_acc: 0.1105\n",
      "Epoch: 071, Train_Loss: 191.6604,Test_Loss: 214.8596,Train_f1: 0.0821,Test_f1: 0.0596,Train_acc: 0.1500,Test_acc: 0.1043\n",
      "Epoch: 072, Train_Loss: 186.0509,Test_Loss: 208.6426,Train_f1: 0.0998,Test_f1: 0.0605,Train_acc: 0.1600,Test_acc: 0.1014\n",
      "Epoch: 073, Train_Loss: 181.3253,Test_Loss: 203.1664,Train_f1: 0.1194,Test_f1: 0.0673,Train_acc: 0.1700,Test_acc: 0.1023\n",
      "Epoch: 074, Train_Loss: 180.1097,Test_Loss: 198.0929,Train_f1: 0.0768,Test_f1: 0.0646,Train_acc: 0.1400,Test_acc: 0.0986\n",
      "Epoch: 075, Train_Loss: 177.1740,Test_Loss: 192.5302,Train_f1: 0.0725,Test_f1: 0.0707,Train_acc: 0.1300,Test_acc: 0.1002\n",
      "Epoch: 076, Train_Loss: 172.1551,Test_Loss: 186.7296,Train_f1: 0.0920,Test_f1: 0.0649,Train_acc: 0.1400,Test_acc: 0.0973\n",
      "Epoch: 077, Train_Loss: 165.6739,Test_Loss: 180.5037,Train_f1: 0.0966,Test_f1: 0.0656,Train_acc: 0.1500,Test_acc: 0.0994\n",
      "Epoch: 078, Train_Loss: 161.0969,Test_Loss: 174.4961,Train_f1: 0.0727,Test_f1: 0.0714,Train_acc: 0.1400,Test_acc: 0.1043\n",
      "Epoch: 079, Train_Loss: 157.1526,Test_Loss: 168.8087,Train_f1: 0.0716,Test_f1: 0.0705,Train_acc: 0.1400,Test_acc: 0.1039\n",
      "Epoch: 080, Train_Loss: 151.9200,Test_Loss: 160.4701,Train_f1: 0.0716,Test_f1: 0.0789,Train_acc: 0.1400,Test_acc: 0.1085\n",
      "Epoch: 081, Train_Loss: 150.7893,Test_Loss: 152.9460,Train_f1: 0.0677,Test_f1: 0.0836,Train_acc: 0.1300,Test_acc: 0.1118\n",
      "Epoch: 082, Train_Loss: 144.3376,Test_Loss: 145.1017,Train_f1: 0.0837,Test_f1: 0.0898,Train_acc: 0.1400,Test_acc: 0.1155\n",
      "Epoch: 083, Train_Loss: 137.2441,Test_Loss: 137.9071,Train_f1: 0.0832,Test_f1: 0.0974,Train_acc: 0.1400,Test_acc: 0.1204\n",
      "Epoch: 084, Train_Loss: 129.4926,Test_Loss: 130.6420,Train_f1: 0.1272,Test_f1: 0.1068,Train_acc: 0.1600,Test_acc: 0.1262\n",
      "Epoch: 085, Train_Loss: 120.4085,Test_Loss: 123.5113,Train_f1: 0.1399,Test_f1: 0.1088,Train_acc: 0.1700,Test_acc: 0.1266\n",
      "Epoch: 086, Train_Loss: 110.6371,Test_Loss: 116.3927,Train_f1: 0.1324,Test_f1: 0.1188,Train_acc: 0.1600,Test_acc: 0.1332\n",
      "Epoch: 087, Train_Loss: 103.6957,Test_Loss: 110.3373,Train_f1: 0.1444,Test_f1: 0.1258,Train_acc: 0.1700,Test_acc: 0.1398\n",
      "Epoch: 088, Train_Loss: 96.8307,Test_Loss: 104.1287,Train_f1: 0.1250,Test_f1: 0.1282,Train_acc: 0.1600,Test_acc: 0.1419\n",
      "Epoch: 089, Train_Loss: 90.9603,Test_Loss: 99.3333,Train_f1: 0.1543,Test_f1: 0.1052,Train_acc: 0.2200,Test_acc: 0.1406\n",
      "Epoch: 090, Train_Loss: 82.4407,Test_Loss: 94.0958,Train_f1: 0.1536,Test_f1: 0.1073,Train_acc: 0.2200,Test_acc: 0.1443\n",
      "Epoch: 091, Train_Loss: 76.2321,Test_Loss: 88.5359,Train_f1: 0.1655,Test_f1: 0.1420,Train_acc: 0.1800,Test_acc: 0.1530\n",
      "Epoch: 092, Train_Loss: 70.9689,Test_Loss: 83.5393,Train_f1: 0.1756,Test_f1: 0.1430,Train_acc: 0.1900,Test_acc: 0.1551\n",
      "Epoch: 093, Train_Loss: 67.3933,Test_Loss: 79.6200,Train_f1: 0.2017,Test_f1: 0.1504,Train_acc: 0.2100,Test_acc: 0.1625\n",
      "Epoch: 094, Train_Loss: 63.6790,Test_Loss: 75.3155,Train_f1: 0.2007,Test_f1: 0.1611,Train_acc: 0.2100,Test_acc: 0.1769\n",
      "Epoch: 095, Train_Loss: 61.4689,Test_Loss: 71.4815,Train_f1: 0.1986,Test_f1: 0.1465,Train_acc: 0.2600,Test_acc: 0.1814\n",
      "Epoch: 096, Train_Loss: 58.0735,Test_Loss: 67.2641,Train_f1: 0.1790,Test_f1: 0.1772,Train_acc: 0.1900,Test_acc: 0.2016\n",
      "Epoch: 097, Train_Loss: 53.2526,Test_Loss: 63.2827,Train_f1: 0.1856,Test_f1: 0.1882,Train_acc: 0.2000,Test_acc: 0.2173\n",
      "Epoch: 098, Train_Loss: 49.9575,Test_Loss: 58.6458,Train_f1: 0.1847,Test_f1: 0.1897,Train_acc: 0.2000,Test_acc: 0.2206\n",
      "Epoch: 099, Train_Loss: 47.2557,Test_Loss: 55.3107,Train_f1: 0.2675,Test_f1: 0.1919,Train_acc: 0.3100,Test_acc: 0.2264\n",
      "Epoch: 100, Train_Loss: 43.7976,Test_Loss: 52.1528,Train_f1: 0.2171,Test_f1: 0.2013,Train_acc: 0.2500,Test_acc: 0.2425\n",
      "Epoch: 101, Train_Loss: 41.9522,Test_Loss: 49.6608,Train_f1: 0.2677,Test_f1: 0.1803,Train_acc: 0.3100,Test_acc: 0.2392\n",
      "Epoch: 102, Train_Loss: 40.4884,Test_Loss: 46.8068,Train_f1: 0.2081,Test_f1: 0.2060,Train_acc: 0.2700,Test_acc: 0.2643\n",
      "Epoch: 103, Train_Loss: 38.9801,Test_Loss: 44.4825,Train_f1: 0.2343,Test_f1: 0.2027,Train_acc: 0.3000,Test_acc: 0.2664\n",
      "Epoch: 104, Train_Loss: 36.9320,Test_Loss: 42.4887,Train_f1: 0.2616,Test_f1: 0.1861,Train_acc: 0.3200,Test_acc: 0.2520\n",
      "Epoch: 105, Train_Loss: 34.3945,Test_Loss: 39.4255,Train_f1: 0.1836,Test_f1: 0.2056,Train_acc: 0.2500,Test_acc: 0.2701\n",
      "Epoch: 106, Train_Loss: 31.6946,Test_Loss: 36.7727,Train_f1: 0.2039,Test_f1: 0.2014,Train_acc: 0.2600,Test_acc: 0.2709\n",
      "Epoch: 107, Train_Loss: 29.4779,Test_Loss: 33.7509,Train_f1: 0.2597,Test_f1: 0.1943,Train_acc: 0.3100,Test_acc: 0.2532\n",
      "Epoch: 108, Train_Loss: 26.8004,Test_Loss: 30.8287,Train_f1: 0.1682,Test_f1: 0.1952,Train_acc: 0.2400,Test_acc: 0.2693\n",
      "Epoch: 109, Train_Loss: 25.0360,Test_Loss: 28.3632,Train_f1: 0.2201,Test_f1: 0.2255,Train_acc: 0.2600,Test_acc: 0.3138\n",
      "Epoch: 110, Train_Loss: 22.7162,Test_Loss: 26.5576,Train_f1: 0.2210,Test_f1: 0.2345,Train_acc: 0.2600,Test_acc: 0.3192\n",
      "Epoch: 111, Train_Loss: 20.5743,Test_Loss: 24.9069,Train_f1: 0.2360,Test_f1: 0.2394,Train_acc: 0.2700,Test_acc: 0.3175\n",
      "Epoch: 112, Train_Loss: 17.9880,Test_Loss: 22.8451,Train_f1: 0.2213,Test_f1: 0.2038,Train_acc: 0.2900,Test_acc: 0.2738\n",
      "Epoch: 113, Train_Loss: 16.3357,Test_Loss: 21.9176,Train_f1: 0.2944,Test_f1: 0.2065,Train_acc: 0.3300,Test_acc: 0.2437\n",
      "Epoch: 114, Train_Loss: 15.7399,Test_Loss: 20.7637,Train_f1: 0.2325,Test_f1: 0.1727,Train_acc: 0.2900,Test_acc: 0.2775\n",
      "Epoch: 115, Train_Loss: 14.9866,Test_Loss: 19.6312,Train_f1: 0.2708,Test_f1: 0.1991,Train_acc: 0.3300,Test_acc: 0.2660\n",
      "Epoch: 116, Train_Loss: 14.2461,Test_Loss: 18.4075,Train_f1: 0.3696,Test_f1: 0.2693,Train_acc: 0.3800,Test_acc: 0.2841\n",
      "Epoch: 117, Train_Loss: 13.8474,Test_Loss: 17.4035,Train_f1: 0.3058,Test_f1: 0.1997,Train_acc: 0.3400,Test_acc: 0.2334\n",
      "Epoch: 118, Train_Loss: 13.1606,Test_Loss: 15.7275,Train_f1: 0.2548,Test_f1: 0.1917,Train_acc: 0.3200,Test_acc: 0.2569\n",
      "Epoch: 119, Train_Loss: 12.5189,Test_Loss: 14.8104,Train_f1: 0.2548,Test_f1: 0.1874,Train_acc: 0.3200,Test_acc: 0.2532\n",
      "Epoch: 120, Train_Loss: 13.1002,Test_Loss: 14.6828,Train_f1: 0.2122,Test_f1: 0.1720,Train_acc: 0.2900,Test_acc: 0.2416\n",
      "Epoch: 121, Train_Loss: 12.7503,Test_Loss: 14.5413,Train_f1: 0.1553,Test_f1: 0.1167,Train_acc: 0.2100,Test_acc: 0.1967\n",
      "Epoch: 122, Train_Loss: 12.3641,Test_Loss: 14.0895,Train_f1: 0.2449,Test_f1: 0.2052,Train_acc: 0.3100,Test_acc: 0.2924\n",
      "Epoch: 123, Train_Loss: 11.9517,Test_Loss: 13.5830,Train_f1: 0.2958,Test_f1: 0.2060,Train_acc: 0.3400,Test_acc: 0.2936\n",
      "Epoch: 124, Train_Loss: 12.5082,Test_Loss: 14.2185,Train_f1: 0.2327,Test_f1: 0.1634,Train_acc: 0.3000,Test_acc: 0.2338\n",
      "Epoch: 125, Train_Loss: 12.6600,Test_Loss: 13.4383,Train_f1: 0.2318,Test_f1: 0.2032,Train_acc: 0.3000,Test_acc: 0.2924\n",
      "Epoch: 126, Train_Loss: 12.3468,Test_Loss: 13.1048,Train_f1: 0.1548,Test_f1: 0.1166,Train_acc: 0.2400,Test_acc: 0.1946\n",
      "Epoch: 127, Train_Loss: 11.5371,Test_Loss: 12.5758,Train_f1: 0.2332,Test_f1: 0.1595,Train_acc: 0.3000,Test_acc: 0.2285\n",
      "Epoch: 128, Train_Loss: 10.8346,Test_Loss: 12.0069,Train_f1: 0.2448,Test_f1: 0.1865,Train_acc: 0.3000,Test_acc: 0.2858\n",
      "Epoch: 129, Train_Loss: 10.4609,Test_Loss: 11.7553,Train_f1: 0.2607,Test_f1: 0.1625,Train_acc: 0.3100,Test_acc: 0.1955\n",
      "Epoch: 130, Train_Loss: 10.5739,Test_Loss: 12.0299,Train_f1: 0.1999,Test_f1: 0.1557,Train_acc: 0.2800,Test_acc: 0.2243\n",
      "Epoch: 131, Train_Loss: 10.1508,Test_Loss: 11.5156,Train_f1: 0.2090,Test_f1: 0.1934,Train_acc: 0.2800,Test_acc: 0.2854\n",
      "Epoch: 132, Train_Loss: 9.8151,Test_Loss: 11.2285,Train_f1: 0.2352,Test_f1: 0.1579,Train_acc: 0.2900,Test_acc: 0.1926\n",
      "Epoch: 133, Train_Loss: 9.4688,Test_Loss: 10.8866,Train_f1: 0.2352,Test_f1: 0.1592,Train_acc: 0.2900,Test_acc: 0.1922\n",
      "Epoch: 134, Train_Loss: 9.4814,Test_Loss: 10.7932,Train_f1: 0.2160,Test_f1: 0.1882,Train_acc: 0.2800,Test_acc: 0.2936\n",
      "Epoch: 135, Train_Loss: 9.4646,Test_Loss: 10.6817,Train_f1: 0.2049,Test_f1: 0.1748,Train_acc: 0.2700,Test_acc: 0.2887\n",
      "Epoch: 136, Train_Loss: 9.5958,Test_Loss: 11.0448,Train_f1: 0.2019,Test_f1: 0.1432,Train_acc: 0.2800,Test_acc: 0.2173\n",
      "Epoch: 137, Train_Loss: 9.4183,Test_Loss: 11.3906,Train_f1: 0.2142,Test_f1: 0.1706,Train_acc: 0.2900,Test_acc: 0.2718\n",
      "Epoch: 138, Train_Loss: 9.3119,Test_Loss: 11.4646,Train_f1: 0.2161,Test_f1: 0.1721,Train_acc: 0.2900,Test_acc: 0.2734\n",
      "Epoch: 139, Train_Loss: 9.4965,Test_Loss: 11.7134,Train_f1: 0.1831,Test_f1: 0.1335,Train_acc: 0.2700,Test_acc: 0.2103\n",
      "Epoch: 140, Train_Loss: 8.7723,Test_Loss: 11.0303,Train_f1: 0.2264,Test_f1: 0.1421,Train_acc: 0.2900,Test_acc: 0.1761\n",
      "Epoch: 141, Train_Loss: 8.6460,Test_Loss: 10.2772,Train_f1: 0.1931,Test_f1: 0.1877,Train_acc: 0.2600,Test_acc: 0.2829\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 501):\n",
    "    train()\n",
    "    train_macro_f1, train_acc, train_loss = test(train_loader)\n",
    "    test_macro_f1, test_acc, test_loss = test(test_loader, True)\n",
    "    scheduler.step(train_loss)\n",
    "    print(\n",
    "        f\"Epoch: {epoch:03d}, Train_Loss: {train_loss:02.4f},Test_Loss: {test_loss:02.4f},Train_f1: {train_macro_f1:01.4f},Test_f1: {test_macro_f1:01.4f},Train_acc: {train_acc:01.4f},Test_acc: {test_acc:01.4f}\"\n",
    "    )\n",
    "    if test_macro_f1 > best_macro_f1:\n",
    "        best_accuracy = test_macro_f1\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), \"../data/best_model.pth\")\n",
    "    elif epoch - best_epoch > early_stop_thresh:\n",
    "        print(\n",
    "            f\"Early stopped training at epoch {epoch} best macro F1 {best_macro_f1} in {best_epoch}\"\n",
    "        )\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b22a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAGE(1, [2, 4, 8, 16, 32, 64, 64, 64, 32, 16, 16, 8], aggr_method=\"max\")\n",
    "# self,\n",
    "# in_channels,\n",
    "# sizes,\n",
    "# out_channels=5,\n",
    "# pool_method=\"max\",\n",
    "# aggr_method=\"mean\",\n",
    "# normalize=True,\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=5, mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9a82f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x.to(torch.float32), data.edge_index, data.batch)\n",
    "        loss = F.cross_entropy(out, data.y - 1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader, return_loss=True):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    loss = 0\n",
    "    for data in loader:\n",
    "        y_out = model(\n",
    "            data.x.to(torch.float32), data.edge_index, data.edge_weight, data.batch\n",
    "        )\n",
    "        y_pred.append(y_out.argmax(dim=-1))\n",
    "        y_true.append(data.y - 1)\n",
    "        if return_loss:\n",
    "            loss += float(F.cross_entropy(y_out, data.y - 1)) * data.num_graphs\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    y_true = np.concatenate(y_true)\n",
    "    if return_loss:\n",
    "        return f1_score(y_true=y_true, y_pred=y_pred, average=\"macro\"), loss / len(\n",
    "            loader.dataset\n",
    "        )\n",
    "    return f1_score(y_true=y_true, y_pred=y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73f032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 501):\n",
    "    train()\n",
    "    train_macro_f1, train_loss = test(train_loader)\n",
    "    test_macro_f1, test_loss = test(test_loader, True)\n",
    "    scheduler.step(train_loss)\n",
    "    print(\n",
    "        f\"Epoch: {epoch:03d}, Train_Loss: {train_loss:02.4f}, Train: {train_macro_f1:01.4f}, Test_Loss: {test_loss:02.4f}, Test: {test_macro_f1:01.4f}\"\n",
    "    )\n",
    "    if test_macro_f1 > best_macro_f1:\n",
    "        best_accuracy = test_macro_f1\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), \"../data/best_model.pth\")\n",
    "    elif epoch - best_epoch > early_stop_thresh:\n",
    "        print(\n",
    "            f\"Early stopped training at epoch {epoch} best macro F1 {best_macro_f1} in {best_epoch}\"\n",
    "        )\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82211af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef6d773",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd775d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
