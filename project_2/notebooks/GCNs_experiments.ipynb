{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0c9d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import utils\n",
    "\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from gcn_models import GIN, GATC, SAGE, Net\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47ce55c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_thresh = 15\n",
    "best_macro_f1 = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b7967a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = utils.GraphDataset(\"../data/\", \"GunPoint\", True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "\n",
    "test_dataset = utils.GraphDataset(\"../data/\", \"GunPoint\", False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=150)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)  # , weight_decay=1e-4)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=3,T_mult=5,verbose=True)#,\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=5, mode=\"min\", cooldown=2, factor=0.8, verbose=True\n",
    ")\n",
    "# scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer,lr_lambda=lambda x:)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f00a7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.nll_loss(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    loss = 0\n",
    "    for data in loader:\n",
    "        y_out = model(data)\n",
    "        y_pred.append(y_out.argmax(dim=-1))\n",
    "        y_true.append(data.y)\n",
    "    loss += float(F.cross_entropy(y_out, data.y)) * data.num_graphs\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    y_true = np.concatenate(y_true)\n",
    "    return (\n",
    "        f1_score(y_true=y_true, y_pred=y_pred, average=\"macro\"),\n",
    "        accuracy_score(y_true=y_true, y_pred=y_pred),\n",
    "        loss / len(loader.dataset),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5502a6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 501):\n",
    "    train()\n",
    "    train_macro_f1, train_acc, train_loss = test(train_loader)\n",
    "    test_macro_f1, test_acc, test_loss = test(test_loader)\n",
    "    scheduler.step(train_loss)\n",
    "    print(\n",
    "        f\"Epoch: {epoch:03d}, Train_Loss: {train_loss:02.4f},Test_Loss: {test_loss:02.4f},Train_f1: {train_macro_f1:01.4f},Test_f1: {test_macro_f1:01.4f},Train_acc: {train_acc:01.4f},Test_acc: {test_acc:01.4f}\"\n",
    "    )\n",
    "    if test_macro_f1 > best_macro_f1:\n",
    "        best_accuracy = test_macro_f1\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), \"../data/best_model.pth\")\n",
    "    elif epoch - best_epoch > early_stop_thresh:\n",
    "        print(\n",
    "            f\"Early stopped training at epoch {epoch} best macro F1 {best_macro_f1} in {best_epoch}\"\n",
    "        )\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b22a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAGE(1, [2, 4, 8, 16, 16, 16, 8], aggr_method=\"max\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, patience=5, mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b9a82f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = F.cross_entropy(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    loss = 0\n",
    "    for data in loader:\n",
    "        y_out = model(data)\n",
    "        y_pred.append(y_out.argmax(dim=-1))\n",
    "        y_true.append(data.y)\n",
    "    loss += float(F.cross_entropy(y_out, data.y)) * data.num_graphs\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    y_true = np.concatenate(y_true)\n",
    "    return (\n",
    "        f1_score(y_true=y_true, y_pred=y_pred, average=\"macro\"),\n",
    "        accuracy_score(y_true=y_true, y_pred=y_pred),\n",
    "        loss / len(loader.dataset),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e73f032a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train_Loss: 0.7177,Test_Loss: 1.4359,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 002, Train_Loss: 0.7184,Test_Loss: 1.4379,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 003, Train_Loss: 0.7193,Test_Loss: 1.4380,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 004, Train_Loss: 0.7202,Test_Loss: 1.4379,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 005, Train_Loss: 0.7191,Test_Loss: 1.4374,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 006, Train_Loss: 0.7179,Test_Loss: 1.4370,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 007, Train_Loss: 0.7166,Test_Loss: 1.4365,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 008, Train_Loss: 0.7177,Test_Loss: 1.4357,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 009, Train_Loss: 0.7171,Test_Loss: 1.4353,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 010, Train_Loss: 0.7173,Test_Loss: 1.4349,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 011, Train_Loss: 0.7177,Test_Loss: 1.4345,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 012, Train_Loss: 0.7157,Test_Loss: 1.4339,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 013, Train_Loss: 0.7161,Test_Loss: 1.4334,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 014, Train_Loss: 0.7162,Test_Loss: 1.4329,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 015, Train_Loss: 0.7170,Test_Loss: 1.4323,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 016, Train_Loss: 0.7165,Test_Loss: 1.4321,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 017, Train_Loss: 0.7163,Test_Loss: 1.4316,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 018, Train_Loss: 0.7153,Test_Loss: 1.4311,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 019, Train_Loss: 0.7153,Test_Loss: 1.4306,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 020, Train_Loss: 0.7147,Test_Loss: 1.4302,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 021, Train_Loss: 0.7147,Test_Loss: 1.4297,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 022, Train_Loss: 0.7154,Test_Loss: 1.4293,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 023, Train_Loss: 0.7157,Test_Loss: 1.4288,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 024, Train_Loss: 0.7145,Test_Loss: 1.4283,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 025, Train_Loss: 0.7147,Test_Loss: 1.4279,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 026, Train_Loss: 0.7148,Test_Loss: 1.4274,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 027, Train_Loss: 0.7139,Test_Loss: 1.4270,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 028, Train_Loss: 0.7135,Test_Loss: 1.4265,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 029, Train_Loss: 0.7130,Test_Loss: 1.4261,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 030, Train_Loss: 0.7133,Test_Loss: 1.4258,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 031, Train_Loss: 0.7132,Test_Loss: 1.4255,Train_f1: 0.3243,Test_f1: 0.3363,Train_acc: 0.4800,Test_acc: 0.5067\n",
      "Epoch: 032, Train_Loss: 0.7127,Test_Loss: 1.4252,Train_f1: 0.3243,Test_f1: 0.3511,Train_acc: 0.4800,Test_acc: 0.5133\n",
      "Epoch: 033, Train_Loss: 0.7122,Test_Loss: 1.4249,Train_f1: 0.3243,Test_f1: 0.3511,Train_acc: 0.4800,Test_acc: 0.5133\n",
      "Epoch: 034, Train_Loss: 0.7132,Test_Loss: 1.4245,Train_f1: 0.3243,Test_f1: 0.3511,Train_acc: 0.4800,Test_acc: 0.5133\n",
      "Epoch: 035, Train_Loss: 0.7125,Test_Loss: 1.4241,Train_f1: 0.3658,Test_f1: 0.3511,Train_acc: 0.5000,Test_acc: 0.5133\n",
      "Epoch: 036, Train_Loss: 0.7120,Test_Loss: 1.4237,Train_f1: 0.3658,Test_f1: 0.3511,Train_acc: 0.5000,Test_acc: 0.5133\n",
      "Epoch: 037, Train_Loss: 0.7120,Test_Loss: 1.4234,Train_f1: 0.3658,Test_f1: 0.3511,Train_acc: 0.5000,Test_acc: 0.5133\n",
      "Epoch: 038, Train_Loss: 0.7117,Test_Loss: 1.4230,Train_f1: 0.3658,Test_f1: 0.3511,Train_acc: 0.5000,Test_acc: 0.5133\n",
      "Epoch: 039, Train_Loss: 0.7115,Test_Loss: 1.4227,Train_f1: 0.3658,Test_f1: 0.3511,Train_acc: 0.5000,Test_acc: 0.5133\n",
      "Epoch: 040, Train_Loss: 0.7110,Test_Loss: 1.4225,Train_f1: 0.3658,Test_f1: 0.3656,Train_acc: 0.5000,Test_acc: 0.5200\n",
      "Epoch: 041, Train_Loss: 0.7117,Test_Loss: 1.4223,Train_f1: 0.3658,Test_f1: 0.3656,Train_acc: 0.5000,Test_acc: 0.5200\n",
      "Epoch: 042, Train_Loss: 0.7119,Test_Loss: 1.4218,Train_f1: 0.3658,Test_f1: 0.3798,Train_acc: 0.5000,Test_acc: 0.5267\n",
      "Epoch: 043, Train_Loss: 0.7107,Test_Loss: 1.4214,Train_f1: 0.3658,Test_f1: 0.3798,Train_acc: 0.5000,Test_acc: 0.5267\n",
      "Epoch: 044, Train_Loss: 0.7109,Test_Loss: 1.4210,Train_f1: 0.3658,Test_f1: 0.3936,Train_acc: 0.5000,Test_acc: 0.5333\n",
      "Epoch: 045, Train_Loss: 0.7104,Test_Loss: 1.4205,Train_f1: 0.4048,Test_f1: 0.4072,Train_acc: 0.5200,Test_acc: 0.5400\n",
      "Epoch: 046, Train_Loss: 0.7103,Test_Loss: 1.4199,Train_f1: 0.4415,Test_f1: 0.4335,Train_acc: 0.5400,Test_acc: 0.5533\n",
      "Epoch: 047, Train_Loss: 0.7097,Test_Loss: 1.4192,Train_f1: 0.4762,Test_f1: 0.4709,Train_acc: 0.5600,Test_acc: 0.5733\n",
      "Epoch: 048, Train_Loss: 0.7089,Test_Loss: 1.4185,Train_f1: 0.4762,Test_f1: 0.4709,Train_acc: 0.5600,Test_acc: 0.5733\n",
      "Epoch: 049, Train_Loss: 0.7085,Test_Loss: 1.4181,Train_f1: 0.4624,Test_f1: 0.4829,Train_acc: 0.5400,Test_acc: 0.5800\n",
      "Epoch: 050, Train_Loss: 0.7087,Test_Loss: 1.4182,Train_f1: 0.4799,Test_f1: 0.4462,Train_acc: 0.5400,Test_acc: 0.5600\n",
      "Epoch: 051, Train_Loss: 0.7088,Test_Loss: 1.4176,Train_f1: 0.5098,Test_f1: 0.4462,Train_acc: 0.5600,Test_acc: 0.5600\n",
      "Epoch: 052, Train_Loss: 0.7082,Test_Loss: 1.4170,Train_f1: 0.5660,Test_f1: 0.4648,Train_acc: 0.6000,Test_acc: 0.5267\n",
      "Epoch: 053, Train_Loss: 0.7072,Test_Loss: 1.4163,Train_f1: 0.5383,Test_f1: 0.5027,Train_acc: 0.5400,Test_acc: 0.5200\n",
      "Epoch: 054, Train_Loss: 0.7077,Test_Loss: 1.4168,Train_f1: 0.5169,Test_f1: 0.4658,Train_acc: 0.5200,Test_acc: 0.4667\n",
      "Epoch: 055, Train_Loss: 0.7093,Test_Loss: 1.4181,Train_f1: 0.4799,Test_f1: 0.5062,Train_acc: 0.5400,Test_acc: 0.5933\n",
      "Epoch: 056, Train_Loss: 0.7081,Test_Loss: 1.4170,Train_f1: 0.4945,Test_f1: 0.5048,Train_acc: 0.5400,Test_acc: 0.5667\n",
      "Epoch: 057, Train_Loss: 0.7071,Test_Loss: 1.4156,Train_f1: 0.4058,Test_f1: 0.4589,Train_acc: 0.4400,Test_acc: 0.4667\n",
      "Epoch: 058, Train_Loss: 0.7067,Test_Loss: 1.4142,Train_f1: 0.3658,Test_f1: 0.3480,Train_acc: 0.5000,Test_acc: 0.4667\n",
      "Epoch: 059, Train_Loss: 0.7046,Test_Loss: 1.4109,Train_f1: 0.3421,Test_f1: 0.3274,Train_acc: 0.5200,Test_acc: 0.4867\n",
      "Epoch: 060, Train_Loss: 0.7040,Test_Loss: 1.4085,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 061, Train_Loss: 0.7023,Test_Loss: 1.4076,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 062, Train_Loss: 0.7029,Test_Loss: 1.4068,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 063, Train_Loss: 0.7036,Test_Loss: 1.4062,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 064, Train_Loss: 0.7026,Test_Loss: 1.4056,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 065, Train_Loss: 0.7020,Test_Loss: 1.4050,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 066, Train_Loss: 0.7015,Test_Loss: 1.4044,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 067, Train_Loss: 0.7011,Test_Loss: 1.4039,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 068, Train_Loss: 0.7010,Test_Loss: 1.4034,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 069, Train_Loss: 0.6991,Test_Loss: 1.4029,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 070, Train_Loss: 0.7014,Test_Loss: 1.4022,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 071, Train_Loss: 0.6989,Test_Loss: 1.4015,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 072, Train_Loss: 0.7001,Test_Loss: 1.4009,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 073, Train_Loss: 0.6992,Test_Loss: 1.4003,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 074, Train_Loss: 0.7003,Test_Loss: 1.3991,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 075, Train_Loss: 0.6980,Test_Loss: 1.3990,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 076, Train_Loss: 0.6996,Test_Loss: 1.4003,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 077, Train_Loss: 0.6985,Test_Loss: 1.4001,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 078, Train_Loss: 0.6984,Test_Loss: 1.3995,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 079, Train_Loss: 0.6991,Test_Loss: 1.3988,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 080, Train_Loss: 0.6976,Test_Loss: 1.3985,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 081, Train_Loss: 0.6972,Test_Loss: 1.3980,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 082, Train_Loss: 0.6979,Test_Loss: 1.3973,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 083, Train_Loss: 0.6985,Test_Loss: 1.3969,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 084, Train_Loss: 0.6959,Test_Loss: 1.3962,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 085, Train_Loss: 0.6988,Test_Loss: 1.3955,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 086, Train_Loss: 0.6964,Test_Loss: 1.3946,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 087, Train_Loss: 0.6938,Test_Loss: 1.3936,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 088, Train_Loss: 0.6924,Test_Loss: 1.3922,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 089, Train_Loss: 0.6956,Test_Loss: 1.3902,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 090, Train_Loss: 0.6940,Test_Loss: 1.3866,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 091, Train_Loss: 0.6939,Test_Loss: 1.3858,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 092, Train_Loss: 0.6930,Test_Loss: 1.3857,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 093, Train_Loss: 0.6889,Test_Loss: 1.3872,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 094, Train_Loss: 0.6909,Test_Loss: 1.3843,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 095, Train_Loss: 0.6914,Test_Loss: 1.3827,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 096, Train_Loss: 0.6906,Test_Loss: 1.3814,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 097, Train_Loss: 0.6893,Test_Loss: 1.3770,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 098, Train_Loss: 0.6861,Test_Loss: 1.3749,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 099, Train_Loss: 0.6819,Test_Loss: 1.3719,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 100, Train_Loss: 0.6841,Test_Loss: 1.3705,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 101, Train_Loss: 0.6827,Test_Loss: 1.3713,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 102, Train_Loss: 0.6878,Test_Loss: 1.3716,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 103, Train_Loss: 0.6914,Test_Loss: 1.3700,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 104, Train_Loss: 0.6746,Test_Loss: 1.3699,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 105, Train_Loss: 0.6782,Test_Loss: 1.3644,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 106, Train_Loss: 0.6761,Test_Loss: 1.3595,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 107, Train_Loss: 0.6747,Test_Loss: 1.3556,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 108, Train_Loss: 0.6716,Test_Loss: 1.3521,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 109, Train_Loss: 0.6784,Test_Loss: 1.3495,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 110, Train_Loss: 0.6696,Test_Loss: 1.3492,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 111, Train_Loss: 0.6734,Test_Loss: 1.3479,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 112, Train_Loss: 0.6700,Test_Loss: 1.3449,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 113, Train_Loss: 0.6698,Test_Loss: 1.3399,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 114, Train_Loss: 0.6655,Test_Loss: 1.3353,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 115, Train_Loss: 0.6607,Test_Loss: 1.3303,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 116, Train_Loss: 0.6607,Test_Loss: 1.3266,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 117, Train_Loss: 0.6652,Test_Loss: 1.3234,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 118, Train_Loss: 0.6606,Test_Loss: 1.3204,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 119, Train_Loss: 0.6574,Test_Loss: 1.3183,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 120, Train_Loss: 0.6548,Test_Loss: 1.3150,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 121, Train_Loss: 0.6491,Test_Loss: 1.3114,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 122, Train_Loss: 0.6454,Test_Loss: 1.3063,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 123, Train_Loss: 0.6510,Test_Loss: 1.3011,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 124, Train_Loss: 0.6420,Test_Loss: 1.2956,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 125, Train_Loss: 0.6357,Test_Loss: 1.2937,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 126, Train_Loss: 0.6429,Test_Loss: 1.2902,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 127, Train_Loss: 0.6320,Test_Loss: 1.2875,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 128, Train_Loss: 0.6367,Test_Loss: 1.2835,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 129, Train_Loss: 0.6425,Test_Loss: 1.2785,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 130, Train_Loss: 0.6341,Test_Loss: 1.2729,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 131, Train_Loss: 0.6254,Test_Loss: 1.2692,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 132, Train_Loss: 0.6303,Test_Loss: 1.2653,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 133, Train_Loss: 0.6176,Test_Loss: 1.2590,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 134, Train_Loss: 0.6183,Test_Loss: 1.2550,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 135, Train_Loss: 0.6238,Test_Loss: 1.2525,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 136, Train_Loss: 0.6160,Test_Loss: 1.2504,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 137, Train_Loss: 0.6283,Test_Loss: 1.2454,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 138, Train_Loss: 0.6169,Test_Loss: 1.2422,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 139, Train_Loss: 0.6175,Test_Loss: 1.2385,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 140, Train_Loss: 0.6210,Test_Loss: 1.2338,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 141, Train_Loss: 0.6098,Test_Loss: 1.2286,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 142, Train_Loss: 0.6067,Test_Loss: 1.2223,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 143, Train_Loss: 0.6084,Test_Loss: 1.2138,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 144, Train_Loss: 0.6043,Test_Loss: 1.2076,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 145, Train_Loss: 0.5908,Test_Loss: 1.2033,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 146, Train_Loss: 0.5817,Test_Loss: 1.2004,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 147, Train_Loss: 0.5928,Test_Loss: 1.1967,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 148, Train_Loss: 0.5976,Test_Loss: 1.1932,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 149, Train_Loss: 0.6056,Test_Loss: 1.1889,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 150, Train_Loss: 0.5963,Test_Loss: 1.1876,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 151, Train_Loss: 0.5939,Test_Loss: 1.1856,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch 00152: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch: 152, Train_Loss: 0.5837,Test_Loss: 1.1805,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 153, Train_Loss: 0.5887,Test_Loss: 1.1756,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 154, Train_Loss: 0.6039,Test_Loss: 1.1740,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 155, Train_Loss: 0.5723,Test_Loss: 1.1711,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 156, Train_Loss: 0.5890,Test_Loss: 1.1661,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 157, Train_Loss: 0.5659,Test_Loss: 1.1604,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 158, Train_Loss: 0.5841,Test_Loss: 1.1537,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 159, Train_Loss: 0.5999,Test_Loss: 1.1486,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 160, Train_Loss: 0.5577,Test_Loss: 1.1451,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 161, Train_Loss: 0.5832,Test_Loss: 1.1415,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 162, Train_Loss: 0.5454,Test_Loss: 1.1377,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 163, Train_Loss: 0.5528,Test_Loss: 1.1346,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 164, Train_Loss: 0.5554,Test_Loss: 1.1318,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 165, Train_Loss: 0.5657,Test_Loss: 1.1285,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 166, Train_Loss: 0.5701,Test_Loss: 1.1238,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 167, Train_Loss: 0.5538,Test_Loss: 1.1177,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch 00168: reducing learning rate of group 0 to 6.4000e-05.\n",
      "Epoch: 168, Train_Loss: 0.5527,Test_Loss: 1.1131,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 169, Train_Loss: 0.5609,Test_Loss: 1.1101,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 170, Train_Loss: 0.5585,Test_Loss: 1.1071,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 171, Train_Loss: 0.5534,Test_Loss: 1.1045,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 172, Train_Loss: 0.5613,Test_Loss: 1.1023,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 173, Train_Loss: 0.5448,Test_Loss: 1.0995,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 174, Train_Loss: 0.5110,Test_Loss: 1.0971,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 175, Train_Loss: 0.5452,Test_Loss: 1.0944,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 176, Train_Loss: 0.5409,Test_Loss: 1.0910,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 177, Train_Loss: 0.5326,Test_Loss: 1.0889,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 178, Train_Loss: 0.5314,Test_Loss: 1.0867,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 179, Train_Loss: 0.5234,Test_Loss: 1.0841,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch 00180: reducing learning rate of group 0 to 5.1200e-05.\n",
      "Epoch: 180, Train_Loss: 0.5224,Test_Loss: 1.0811,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 181, Train_Loss: 0.5327,Test_Loss: 1.0789,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 182, Train_Loss: 0.5376,Test_Loss: 1.0764,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 183, Train_Loss: 0.5302,Test_Loss: 1.0738,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 184, Train_Loss: 0.5301,Test_Loss: 1.0697,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 185, Train_Loss: 0.5469,Test_Loss: 1.0672,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 186, Train_Loss: 0.5186,Test_Loss: 1.0643,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 187, Train_Loss: 0.4980,Test_Loss: 1.0623,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 188, Train_Loss: 0.5255,Test_Loss: 1.0606,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 189, Train_Loss: 0.5386,Test_Loss: 1.0586,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 190, Train_Loss: 0.5474,Test_Loss: 1.0563,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 191, Train_Loss: 0.5397,Test_Loss: 1.0541,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 192, Train_Loss: 0.5146,Test_Loss: 1.0528,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch 00193: reducing learning rate of group 0 to 4.0960e-05.\n",
      "Epoch: 193, Train_Loss: 0.5210,Test_Loss: 1.0510,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 194, Train_Loss: 0.5262,Test_Loss: 1.0486,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 195, Train_Loss: 0.5104,Test_Loss: 1.0473,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 196, Train_Loss: 0.5162,Test_Loss: 1.0459,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 197, Train_Loss: 0.5156,Test_Loss: 1.0440,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 198, Train_Loss: 0.5228,Test_Loss: 1.0420,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 199, Train_Loss: 0.5069,Test_Loss: 1.0394,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 200, Train_Loss: 0.5246,Test_Loss: 1.0369,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch 00201: reducing learning rate of group 0 to 3.2768e-05.\n",
      "Epoch: 201, Train_Loss: 0.5060,Test_Loss: 1.0346,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 202, Train_Loss: 0.5162,Test_Loss: 1.0329,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 203, Train_Loss: 0.5076,Test_Loss: 1.0312,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 204, Train_Loss: 0.5168,Test_Loss: 1.0298,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 205, Train_Loss: 0.5055,Test_Loss: 1.0283,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 206, Train_Loss: 0.5148,Test_Loss: 1.0262,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 207, Train_Loss: 0.4911,Test_Loss: 1.0245,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 208, Train_Loss: 0.5146,Test_Loss: 1.0230,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 209, Train_Loss: 0.4915,Test_Loss: 1.0216,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 210, Train_Loss: 0.5082,Test_Loss: 1.0203,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 211, Train_Loss: 0.4770,Test_Loss: 1.0193,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 212, Train_Loss: 0.4745,Test_Loss: 1.0185,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 213, Train_Loss: 0.5022,Test_Loss: 1.0184,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 214, Train_Loss: 0.4895,Test_Loss: 1.0177,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 215, Train_Loss: 0.4778,Test_Loss: 1.0168,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 216, Train_Loss: 0.4924,Test_Loss: 1.0159,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 217, Train_Loss: 0.5071,Test_Loss: 1.0148,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch 00218: reducing learning rate of group 0 to 2.6214e-05.\n",
      "Epoch: 218, Train_Loss: 0.5006,Test_Loss: 1.0136,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 219, Train_Loss: 0.4933,Test_Loss: 1.0122,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 220, Train_Loss: 0.5061,Test_Loss: 1.0109,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 221, Train_Loss: 0.5184,Test_Loss: 1.0094,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 222, Train_Loss: 0.5125,Test_Loss: 1.0080,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 223, Train_Loss: 0.5280,Test_Loss: 1.0068,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 224, Train_Loss: 0.4992,Test_Loss: 1.0053,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 225, Train_Loss: 0.4840,Test_Loss: 1.0040,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch 00226: reducing learning rate of group 0 to 2.0972e-05.\n",
      "Epoch: 226, Train_Loss: 0.4884,Test_Loss: 1.0029,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 227, Train_Loss: 0.4998,Test_Loss: 1.0020,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 228, Train_Loss: 0.4901,Test_Loss: 1.0014,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 229, Train_Loss: 0.4804,Test_Loss: 1.0005,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 230, Train_Loss: 0.4943,Test_Loss: 0.9996,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 231, Train_Loss: 0.4926,Test_Loss: 0.9989,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 232, Train_Loss: 0.4967,Test_Loss: 0.9984,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 233, Train_Loss: 0.4874,Test_Loss: 0.9984,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch 00234: reducing learning rate of group 0 to 1.6777e-05.\n",
      "Epoch: 234, Train_Loss: 0.4869,Test_Loss: 0.9981,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 235, Train_Loss: 0.5120,Test_Loss: 0.9974,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 236, Train_Loss: 0.4919,Test_Loss: 0.9968,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 237, Train_Loss: 0.4825,Test_Loss: 0.9961,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 238, Train_Loss: 0.4866,Test_Loss: 0.9954,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 239, Train_Loss: 0.4858,Test_Loss: 0.9947,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 240, Train_Loss: 0.5162,Test_Loss: 0.9939,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 241, Train_Loss: 0.4903,Test_Loss: 0.9931,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch 00242: reducing learning rate of group 0 to 1.3422e-05.\n",
      "Epoch: 242, Train_Loss: 0.4889,Test_Loss: 0.9922,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 243, Train_Loss: 0.4774,Test_Loss: 0.9917,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 244, Train_Loss: 0.4910,Test_Loss: 0.9910,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 245, Train_Loss: 0.4696,Test_Loss: 0.9906,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 246, Train_Loss: 0.4896,Test_Loss: 0.9901,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 247, Train_Loss: 0.4942,Test_Loss: 0.9897,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 248, Train_Loss: 0.4978,Test_Loss: 0.9892,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 249, Train_Loss: 0.5122,Test_Loss: 0.9889,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 250, Train_Loss: 0.4988,Test_Loss: 0.9885,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch 00251: reducing learning rate of group 0 to 1.0737e-05.\n",
      "Epoch: 251, Train_Loss: 0.4818,Test_Loss: 0.9878,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 252, Train_Loss: 0.4954,Test_Loss: 0.9874,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 253, Train_Loss: 0.4888,Test_Loss: 0.9869,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 254, Train_Loss: 0.4750,Test_Loss: 0.9866,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 255, Train_Loss: 0.5192,Test_Loss: 0.9862,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 256, Train_Loss: 0.4808,Test_Loss: 0.9856,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 257, Train_Loss: 0.4934,Test_Loss: 0.9849,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 258, Train_Loss: 0.4738,Test_Loss: 0.9844,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch 00259: reducing learning rate of group 0 to 8.5899e-06.\n",
      "Epoch: 259, Train_Loss: 0.4917,Test_Loss: 0.9838,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 260, Train_Loss: 0.4844,Test_Loss: 0.9836,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 261, Train_Loss: 0.5066,Test_Loss: 0.9835,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 262, Train_Loss: 0.4602,Test_Loss: 0.9832,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 263, Train_Loss: 0.4866,Test_Loss: 0.9831,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 264, Train_Loss: 0.4854,Test_Loss: 0.9828,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 265, Train_Loss: 0.4789,Test_Loss: 0.9826,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 266, Train_Loss: 0.4873,Test_Loss: 0.9824,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 267, Train_Loss: 0.4655,Test_Loss: 0.9821,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch 00268: reducing learning rate of group 0 to 6.8719e-06.\n",
      "Epoch: 268, Train_Loss: 0.4837,Test_Loss: 0.9819,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 269, Train_Loss: 0.4906,Test_Loss: 0.9817,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 270, Train_Loss: 0.4847,Test_Loss: 0.9815,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 271, Train_Loss: 0.4830,Test_Loss: 0.9812,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 272, Train_Loss: 0.4733,Test_Loss: 0.9809,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 273, Train_Loss: 0.4773,Test_Loss: 0.9807,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 274, Train_Loss: 0.4783,Test_Loss: 0.9804,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 275, Train_Loss: 0.4782,Test_Loss: 0.9801,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 276, Train_Loss: 0.4535,Test_Loss: 0.9798,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 277, Train_Loss: 0.4903,Test_Loss: 0.9796,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 278, Train_Loss: 0.4825,Test_Loss: 0.9793,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 279, Train_Loss: 0.4890,Test_Loss: 0.9791,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 280, Train_Loss: 0.4966,Test_Loss: 0.9789,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 281, Train_Loss: 0.4820,Test_Loss: 0.9786,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch 00282: reducing learning rate of group 0 to 5.4976e-06.\n",
      "Epoch: 282, Train_Loss: 0.4969,Test_Loss: 0.9783,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 283, Train_Loss: 0.4882,Test_Loss: 0.9781,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 284, Train_Loss: 0.4768,Test_Loss: 0.9778,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 285, Train_Loss: 0.4716,Test_Loss: 0.9776,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 286, Train_Loss: 0.5005,Test_Loss: 0.9774,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 287, Train_Loss: 0.4836,Test_Loss: 0.9771,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 288, Train_Loss: 0.4956,Test_Loss: 0.9769,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 289, Train_Loss: 0.4822,Test_Loss: 0.9767,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch 00290: reducing learning rate of group 0 to 4.3980e-06.\n",
      "Epoch: 290, Train_Loss: 0.4838,Test_Loss: 0.9764,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 291, Train_Loss: 0.4886,Test_Loss: 0.9762,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 292, Train_Loss: 0.4763,Test_Loss: 0.9760,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 293, Train_Loss: 0.4811,Test_Loss: 0.9759,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 294, Train_Loss: 0.5015,Test_Loss: 0.9758,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 295, Train_Loss: 0.4895,Test_Loss: 0.9758,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 296, Train_Loss: 0.4752,Test_Loss: 0.9756,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 297, Train_Loss: 0.4719,Test_Loss: 0.9756,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch 00298: reducing learning rate of group 0 to 3.5184e-06.\n",
      "Epoch: 298, Train_Loss: 0.4616,Test_Loss: 0.9755,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 299, Train_Loss: 0.4942,Test_Loss: 0.9754,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 300, Train_Loss: 0.4587,Test_Loss: 0.9754,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 301, Train_Loss: 0.4958,Test_Loss: 0.9753,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 302, Train_Loss: 0.4768,Test_Loss: 0.9752,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 303, Train_Loss: 0.4739,Test_Loss: 0.9751,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 304, Train_Loss: 0.4707,Test_Loss: 0.9750,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 305, Train_Loss: 0.4786,Test_Loss: 0.9749,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch 00306: reducing learning rate of group 0 to 2.8147e-06.\n",
      "Epoch: 306, Train_Loss: 0.4644,Test_Loss: 0.9748,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 307, Train_Loss: 0.4707,Test_Loss: 0.9747,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 308, Train_Loss: 0.4892,Test_Loss: 0.9746,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 309, Train_Loss: 0.4827,Test_Loss: 0.9745,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 310, Train_Loss: 0.4637,Test_Loss: 0.9744,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 311, Train_Loss: 0.4766,Test_Loss: 0.9743,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 312, Train_Loss: 0.4764,Test_Loss: 0.9742,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 313, Train_Loss: 0.4835,Test_Loss: 0.9740,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch 00314: reducing learning rate of group 0 to 2.2518e-06.\n",
      "Epoch: 314, Train_Loss: 0.4782,Test_Loss: 0.9739,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 315, Train_Loss: 0.4803,Test_Loss: 0.9739,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 316, Train_Loss: 0.4759,Test_Loss: 0.9738,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 317, Train_Loss: 0.5007,Test_Loss: 0.9738,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 318, Train_Loss: 0.4985,Test_Loss: 0.9738,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 319, Train_Loss: 0.4809,Test_Loss: 0.9737,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 320, Train_Loss: 0.4856,Test_Loss: 0.9736,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 321, Train_Loss: 0.4701,Test_Loss: 0.9736,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch 00322: reducing learning rate of group 0 to 1.8014e-06.\n",
      "Epoch: 322, Train_Loss: 0.4842,Test_Loss: 0.9736,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 323, Train_Loss: 0.4766,Test_Loss: 0.9736,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 324, Train_Loss: 0.4806,Test_Loss: 0.9736,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 325, Train_Loss: 0.4682,Test_Loss: 0.9735,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 326, Train_Loss: 0.4880,Test_Loss: 0.9734,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 327, Train_Loss: 0.4933,Test_Loss: 0.9734,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 328, Train_Loss: 0.4683,Test_Loss: 0.9733,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 329, Train_Loss: 0.4882,Test_Loss: 0.9733,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch 00330: reducing learning rate of group 0 to 1.4412e-06.\n",
      "Epoch: 330, Train_Loss: 0.4921,Test_Loss: 0.9732,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 331, Train_Loss: 0.4886,Test_Loss: 0.9731,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 332, Train_Loss: 0.4629,Test_Loss: 0.9731,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 333, Train_Loss: 0.4814,Test_Loss: 0.9731,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 334, Train_Loss: 0.4686,Test_Loss: 0.9730,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 335, Train_Loss: 0.4817,Test_Loss: 0.9730,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 336, Train_Loss: 0.4693,Test_Loss: 0.9730,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 337, Train_Loss: 0.4822,Test_Loss: 0.9729,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch 00338: reducing learning rate of group 0 to 1.1529e-06.\n",
      "Epoch: 338, Train_Loss: 0.4881,Test_Loss: 0.9728,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 339, Train_Loss: 0.4815,Test_Loss: 0.9727,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 340, Train_Loss: 0.4814,Test_Loss: 0.9727,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 341, Train_Loss: 0.4815,Test_Loss: 0.9726,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n",
      "Epoch: 342, Train_Loss: 0.4621,Test_Loss: 0.9726,Train_f1: 0.3421,Test_f1: 0.3304,Train_acc: 0.5200,Test_acc: 0.4933\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m train()\n\u001b[1;32m      3\u001b[0m train_macro_f1, train_acc, train_loss \u001b[39m=\u001b[39m test(train_loader)\n\u001b[0;32m----> 4\u001b[0m test_macro_f1, test_acc, test_loss \u001b[39m=\u001b[39m test(test_loader)\n\u001b[1;32m      5\u001b[0m scheduler\u001b[39m.\u001b[39mstep(train_loss)\n\u001b[1;32m      6\u001b[0m \u001b[39mprint\u001b[39m(\n\u001b[1;32m      7\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train_Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m02.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m,Test_Loss: \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m:\u001b[39;00m\u001b[39m02.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m,Train_f1: \u001b[39m\u001b[39m{\u001b[39;00mtrain_macro_f1\u001b[39m:\u001b[39;00m\u001b[39m01.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m,Test_f1: \u001b[39m\u001b[39m{\u001b[39;00mtest_macro_f1\u001b[39m:\u001b[39;00m\u001b[39m01.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m,Train_acc: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m01.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m,Test_acc: \u001b[39m\u001b[39m{\u001b[39;00mtest_acc\u001b[39m:\u001b[39;00m\u001b[39m01.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[0;32mIn[22], line 21\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m     19\u001b[0m y_true \u001b[39m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m loader:\n\u001b[1;32m     22\u001b[0m     y_out \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     23\u001b[0m     y_pred\u001b[39m.\u001b[39mappend(y_out\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Projects/2023L-machine-learning-on-graphs/project_2/utils.py:142\u001b[0m, in \u001b[0;36mGraphDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m--> 142\u001b[0m     data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(path\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpath, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00midx\u001b[39m}\u001b[39;49;00m\u001b[39m.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    143\u001b[0m     data\u001b[39m.\u001b[39my \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels[idx] \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    144\u001b[0m     data\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 809\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    810\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1174\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/venvs/MLG/lib/python3.9/site-packages/torch/_utils.py:166\u001b[0m, in \u001b[0;36m_rebuild_tensor_v2\u001b[0;34m(storage, storage_offset, size, stride, requires_grad, backward_hooks, metadata)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, torch\u001b[39m.\u001b[39mTensor)\n\u001b[1;32m    163\u001b[0m     torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_set_tensor_metadata(tensor, metadata)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_rebuild_tensor_v2\u001b[39m(\n\u001b[1;32m    167\u001b[0m     storage, storage_offset, size, stride, requires_grad, backward_hooks, metadata\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m    168\u001b[0m ):\n\u001b[1;32m    169\u001b[0m     tensor \u001b[39m=\u001b[39m _rebuild_tensor(storage, storage_offset, size, stride)\n\u001b[1;32m    170\u001b[0m     tensor\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m requires_grad\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 501):\n",
    "    train()\n",
    "    train_macro_f1, train_acc, train_loss = test(train_loader)\n",
    "    test_macro_f1, test_acc, test_loss = test(test_loader)\n",
    "    scheduler.step(train_loss)\n",
    "    print(\n",
    "        f\"Epoch: {epoch:03d}, Train_Loss: {train_loss:02.4f},Test_Loss: {test_loss:02.4f},Train_f1: {train_macro_f1:01.4f},Test_f1: {test_macro_f1:01.4f},Train_acc: {train_acc:01.4f},Test_acc: {test_acc:01.4f}\"\n",
    "    )\n",
    "    if test_macro_f1 > best_macro_f1:\n",
    "        best_accuracy = test_macro_f1\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), \"../data/best_model.pth\")\n",
    "    elif epoch - best_epoch > early_stop_thresh:\n",
    "        print(\n",
    "            f\"Early stopped training at epoch {epoch} best macro F1 {best_macro_f1} in {best_epoch}\"\n",
    "        )\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
