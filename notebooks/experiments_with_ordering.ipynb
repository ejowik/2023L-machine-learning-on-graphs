{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tak jak rozmawialiśmy to jest jedynie bardzo początkowe przejrzenie tych metod (dalej tej Doc2Vec wygląda dość dziwnie), ale przynajmniej wstępne wyniki wyglądają jakby miało to jakiekolwiek szanse zadziałać :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sekwencja czynności przy framework'u **WL**:\n",
    "1. wczytanie grafu przy założeniu, że feature'ami będą stopnie wierzchołków;\n",
    "2. na podstawie stopni wierzchołków i ich sąsiadów, generujemy nowe charakterystyki $\\rightarrow$ \"podobny label\" mają wierzchołki, które są podobnego stopnia i ich sąsiedzi są podobnych stopni;\n",
    "3. na podstawie listy nowoutworzonych cech tworzona jest instancja klasy `TaggedDocument`, gdzie tagiem jest nazwa grafu;\n",
    "4. utworzony w kroku $3.$ dokument podawany jest na wejściu `Doc2Vec'a`.\n",
    "\n",
    "References:\n",
    "- https://docs.cogdl.ai/en/latest/_modules/cogdl/models/emb/graph2vec.html\n",
    "- https://towardsdatascience.com/beyond-weisfeiler-lehman-approximate-isomorphisms-and-metric-embeddings-f7b816b75751"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.append('../')\n",
    "from src.graph2vec import OurGraph2Vec\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_stargazers_dataset(path: str) -> Tuple[List, pd.Series]:\n",
    "    \"\"\"\n",
    "    Function to read the graphs and their labels from github Stargazers dataset.\n",
    "    Args:\n",
    "        path (str): path to the dataset\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List,pd.Series]: A tuple consisting of a list of NetworkX graphs and a pd.Series of their respective labels\n",
    "    \"\"\"\n",
    "    data_path = os.path.join(path, \"git_edges.json\")\n",
    "    target_path = os.path.join(path, \"git_target.csv\")\n",
    "    with open(data_path) as f:\n",
    "        json_content = json.load(f)\n",
    "    target = pd.read_csv(target_path, index_col=\"id\").squeeze(\"columns\")\n",
    "    return [nx.Graph(elem) for key, elem in json_content.items()], target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logreg_model(embedding, labels, get_pred=False):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        embedding, labels, test_size=0.3\n",
    "    )\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    if get_pred:\n",
    "        return model.predict_proba(X_test), y_test\n",
    "    return 1 - (np.abs(model.predict(X_test) - y_test)).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_measure(func, G, kwargs={}):\n",
    "    return func(G, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs, labels = read_stargazers_dataset(\"../datasets/github_stargazers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_card = 1659  # number of graphs\n",
    "expt_iter_num = 1000  # number of models to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEASURES = [\n",
    "    nx.pagerank,\n",
    "    nx.closeness_centrality,\n",
    "    nx.degree_centrality,\n",
    "    nx.current_flow_closeness_centrality,\n",
    "    # nx.information_centrality,  # identical to: current_flow_closeness_centrality\n",
    "    nx.betweenness_centrality,\n",
    "    nx.current_flow_betweenness_centrality,\n",
    "    nx.approximate_current_flow_betweenness_centrality,\n",
    "]\n",
    "\n",
    "KWARGS_DICT = {\n",
    "    nx.pagerank: {},\n",
    "    nx.closeness_centrality: {},\n",
    "    nx.degree_centrality: {},\n",
    "    nx.current_flow_closeness_centrality: {},\n",
    "    # nx.information_centrality: {},\n",
    "    nx.betweenness_centrality: {},\n",
    "    nx.current_flow_betweenness_centrality: {},\n",
    "    nx.approximate_current_flow_betweenness_centrality: {},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_subset = graphs[:subset_card]\n",
    "\n",
    "order_dict = {}\n",
    "without = OurGraph2Vec(window_size=0)  # without\n",
    "with_dm = OurGraph2Vec(cbowlike=True)  # with_dm (experiment)\n",
    "\n",
    "alpha = 1  # weight corresponding to with_dm embedding (component)\n",
    "beta = 1  # weight corresponding to with_dm embedding (component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f056fa32b1b4ab3b6fbc0e3eb82172a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating pagerank...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63de89ce0c4b4395bb033a3c12f1c4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1659 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordering wrt. pagerank...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d131ba443174bc3bd0e32c15561f444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1659 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating closeness_centrality...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "766664e71acc44de9209d7351acae723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1659 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# in this cell, we create a dictionary of orders with the following structure:\n",
    "# order_dict = {\n",
    "#    measure: nested list of nodes sorted wrt. measure\n",
    "#}\n",
    "\n",
    "for measure in tqdm(MEASURES):\n",
    "    # centralities calc.\n",
    "    print(f\"Calculating {measure.__name__}...\")\n",
    "    \n",
    "    centralities = [None] * subset_card\n",
    "    for idx, G in tqdm(enumerate(graphs_subset), total=subset_card):\n",
    "        centralities[idx] = calculate_measure(func=measure, G=G, kwargs=KWARGS_DICT[measure])\n",
    "        \n",
    "    # ordering wrt. centralities\n",
    "    print(f\"Ordering wrt. {measure.__name__}...\")\n",
    "    ams = [None for i in range(subset_card)]\n",
    "\n",
    "    for it, central_dict in tqdm(enumerate(centralities), total=subset_card):\n",
    "        ams[it] = np.array(sorted(central_dict, key=central_dict.get), dtype=np.int64)\n",
    "        \n",
    "    order_dict[measure] = ams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this cell we run experiments for all measures\n",
    "\n",
    "for measure, order in order_dict.items():\n",
    "    display(measure.__name__)\n",
    "    \n",
    "    # benchmark\n",
    "    without.fit(graphs_subset, orderings=order)\n",
    "    with_dm.fit(graphs_subset, orderings=order)\n",
    "    \n",
    "    # experiment\n",
    "    no_dm_res = [0] * expt_iter_num\n",
    "    dm_res = [0] * expt_iter_num\n",
    "    ens_res = [0] * expt_iter_num\n",
    "\n",
    "    for it in tqdm(range(expt_iter_num)):\n",
    "        no_dm_res[it] = create_logreg_model(\n",
    "            without.get_embedding(), labels=labels[:subset_card]\n",
    "        )\n",
    "        dm_res[it] = create_logreg_model(\n",
    "            with_dm.get_embedding(), labels=labels[:subset_card]\n",
    "        )\n",
    "\n",
    "        # TODO: weighted ensemble of embeddings\n",
    "        ens_res[it] = create_logreg_model(\n",
    "            alpha*with_dm.get_embedding()+beta*without.get_embedding(),\n",
    "            labels=labels[:subset_card]\n",
    "        )\n",
    "    \n",
    "    # results\n",
    "    res_nested_list = [\n",
    "        no_dm_res,\n",
    "        dm_res,\n",
    "        ens_res\n",
    "    \n",
    "    ]\n",
    "    axis_labels = [\n",
    "        'Skipgram',\n",
    "        'CBOW',\n",
    "        'Ensemble'\n",
    "    ]\n",
    "\n",
    "    n_items = len(res_nested_list)\n",
    "    plt.boxplot(res_nested_list, positions=range(n_items))\n",
    "    plt.xticks(range(n_items), labels=axis_labels[:n_items])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
